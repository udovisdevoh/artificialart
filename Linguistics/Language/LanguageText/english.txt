The first residents of what is now the United States emigrated from Asia over 15,000 years ago by crossing Beringia into Alaska. Archaeological evidence of these peoples, the ancestors of the Native Americans, dates to 14,000 years ago.[1]

Christopher Columbus was the first European to land in the territory of what is now the United States when he arrived in Puerto Rico in 1493. The subsequent arrival of settlers from Europe began the colonial history of the United States. The Thirteen English colonies that would become the original US states were founded along the east coast beginning in 1607. Spain, France and Russia also founded small settlements in what would become US territory. The population of the Thirteen Colonies grew rapidly, reaching 50,000 by 1650, 250,000 by 1700, and 2.5 million by 1775. High birth rates and low death rates were augmented by steady flows of immigrants from Europe and slaves from the West Indies. Occasional small-scale wars involved the French and Indians to the north, and the Spanish and Indians to the south. Religion was a powerful influence on many immigrants, especially the Puritans in New England and the German sects in Pennsylvania, with boosts from the revivals of the First Great Awakening. The colonies by the 1750s had achieved a standard of living about as high as Britain, with far more self-government than anywhere else. Most free men owned their own farms and could vote in elections for the colonial legislatures, while local courts dispensed justice. Royal soldiers were rarely seen.[2]

The colonists did not have representation in the ruling British government and believed they were being denied their constitutional rights as Englishmen. For many years, the home government had permitted wide latitude to local colonial governments. Beginning in the 1760s London demanded the colonists pay taxes. The new foreign taxes on stamps and tea ignited a firestorm of opposition. The British responded with military force in Massachusetts, and shut down the system of local self government in what the colonists called the Intolerable Acts.

After fighting broke out in April 1775, the colonies ousted all royal officials and set up their own governments, which were coordinated out of Philadelphia by the Continental Congress. The American Revolution escalated into all-out war. Despite local King George loyalists, the new nation declared independence in July 1776 as the United States of America. After Americans captured the British invasion army in 1777, France became a military ally, and the war became a major international war with evenly balanced forces. With the capture of a second British invasion army at Yorktown in 1781, the British opened peace negotiations. The Treaty of Paris in 1783 proved highly favorable to the new nation.[3]

The new national government proved too weak, so a Constitutional Convention was called in 1787 to create an alternative. The resulting Constitution of the United States, ratified in 1788, created a federal government based on the ideology of republicanism, equal rights, and civic duty. The first ten amendments known as the Bill of Rights quickly followed, guaranteeing many individual rights from federal interference. The new national government under President George Washington built a strong economic system, designed by Alexander Hamilton, that settled the wartime debts, created a national bank and sought economic growth based on cities and trade, more than farming. Hamilton formed the Federalist Party to gain wide local support for the new policies, which were opposed by Thomas Jefferson. The Jay Treaty of 1795 opened a decade of trade with Britain, which was at war with revolutionary France. Jefferson, a friend of France who feared British influence would undermine republicanism, set up an opposition party, and the First Party System based on voters in every state, began operation in the mid-1790s. Jefferson tried to coerce the British into recognizing America's neutral rights, stopping the seizure of sailors on American ships and the aid of hostile Indians in the West. When that failed the U.S. declared the War of 1812 against Britain. The war was militarily indecisive but guaranteed American independence and friendly relations with the British Empire, which controlled Canada.

With the Louisiana Purchase in 1803 westward expansion of the United States crossed the Mississippi River. This was encouraged by the belief in Manifest Destiny, by which the United States would expand east to west, reaching the Pacific after the conquest of Mexico in 1848. The slaveholding South in 1861 tried to break away and form its own country in response to threats to its peculiar institution—slavery. The Civil War lasting four years became deadliest war in American history. Under the leadership of Republican Abraham Lincoln the rebellion was crushed, the nation reunified, the slaves freed, and the South put under Reconstruction for a decade.

Rapid economic growth, fueled by entrepreneurs who created great new industries in railroads, steel, coal, textiles, and machinery operated by millions of immigrants from Europe (and some from Asia), built new cities overnight, making the U.S. the world's leading industrial power. With Germany threatening to win World War I in part by sinking American ships, the U.S. entered the war in 1917, supplied the material, money and to a degree the soldiers needed to win. The U.S. partially dictated the peace terms, but refused to join the League of Nations, as it enjoyed unprecedented prosperity in the 1920s. The crash of 1929 started the worldwide Great Depression, which was long and severe for the entire country. A New Deal Coalition led by Franklin D. Roosevelt dominated national elections for years, and the New Deal in 1933-36 began a new era of federal regulation of the business, support for labor unions, and provision of relief for the unemployed and Social Security for the elderly.

The U.S. joined the Allied Forces of World War II in December 1941 after the Japanese attack on Pearl Harbor. Postwar hopes that the new United Nations would resolve the world's problems failed, as Europe was divided and the U.S. took the lead in the Cold War with a policy of containing Soviet expansion. Containment led to wars in Korea (a stalemate) and Vietnam (lost). Economic prosperity after the war empowered families to move to the suburbs and engage in a Baby Boom that pushed the population from 140 million in 1940 to 203 million in 1970. The industrial economy based on heavy industry gave way to a service economy featuring health care and education, as America led the way to a computerized world. The end of the Cold War came in 1991 as Soviet Communism collapsed. The U.S. was the only military superpower left, but it was challenged for economic supremacy by China, which remained on good terms with the U.S. as it embraced capitalism and by 2010 was growing much more rapidly than the U.S.

The Civil Rights Movement ended Jim Crow and empowered black voters in the 1960s, which allowed blacks to move into high government offices. However, the New Deal coalition collapsed in the mid 1960s in disputes over race and the Vietnam War. The Reagan Era of conservative national policies, deregulation and tax cuts took control with the election of Ronald Reagan in 1980. By 2010, political scientists were debating whether the election of Barack Obama in 2008 represented an end of the Reagan Era, or was only a reaction against the bubble economy of the 2000s, which burst in 2008 and became the Late-2000s recession with prolonged unemployment.[4][5]
Contents
[hide]

    * 1 Colonial period
          o 1.1 Spanish colonization
          o 1.2 Dutch colonization
          o 1.3 French colonization
          o 1.4 British colonization
          o 1.5 Political integration and autonomy
    * 2 Formation of the United States of America (1776–1789)
    * 3 Early national era (1789–1848)
    * 4 Civil War era (1849–1865)
    * 5 Reconstruction and the Gilded Age (1865–1890)
    * 6 Progressivism, imperialism, and World War I (1890–1918)
    * 7 Women's suffrage
    * 8 Post-World War I and the Great Depression (1918–1940)
    * 9 World War II (1941–1945)
          o 9.1 Battle against Germany
          o 9.2 Battle against Japan
    * 10 The Cold War begins (1945–1964)
    * 11 The Civil Rights Movement (1955–1970)
    * 12 The Women's Movement (1963–1982)
    * 13 The Counterculture Revolution and Cold War Détente (1964–1980)
    * 14 The end of the Cold War (1980–1991)
    * 15 The World Superpower (1991–present)
          o 15.1 1991-2001
          o 15.2 2001-present
    * 16 See also
    * 17 Notes
    * 18 References
    * 19 Further reading
    * 20 External links

Colonial period
Main article: Colonial history of the United States
The Mayflower, which transported Pilgrims to the New World

After a period of exploration by people from various European countries, Spanish, Dutch, English, French, Swedish, and Portuguese settlements were established. Although Leif Ericson was the first European to arrive in North America, Christopher Columbus is credited as the first European to set foot on what would one day become US territory when he came to Puerto Rico on November 19, 1493, during his second voyage.

In the 16th century, Europeans brought horses, cattle, and hogs to the Americas and, in turn, took back to Europe maize, potatoes, tobacco, beans, squash, and slave natives, many of whom died enroute.
Spanish colonization
See also: New Spain
Coronado Sets Out to the North (1540) by Frederic Remington, oil on canvas, 1905.

Spanish explorers came to what is now the United States beginning with Christopher Columbus' second expedition, which reached Puerto Rico on November 19, 1493.[6] The first confirmed landing in the continental US was by a Spaniard, Juan Ponce de León, who landed in 1513 on a lush shore he christened La Florida.[7]

Within three decades of Ponce de León's landing, the Spanish became the first Europeans to reach the Appalachian Mountains, the Mississippi River, the Grand Canyon[8] and the Great Plains. In 1540, Hernando de Soto undertook an extensive exploration of the present US and, in the same year, Francisco Vázquez de Coronado led 2,000 Spaniards and Native Mexican Americans across the modern Arizona–Mexico border and traveled as far as central Kansas.[9] Other Spanish explorers include Lucas Vásquez de Ayllón, Pánfilo de Narváez, Sebastián Vizcaíno, Juan Rodríguez Cabrillo, Gaspar de Portolà, Pedro Menéndez de Avilés, Álvar Núñez Cabeza de Vaca, Tristán de Luna y Arellano and Juan de Oñate.[10]

The Spanish sent some settlers, creating the first permanent European settlement in the continental United States at St. Augustine, Florida in 1565.[11] Later Spanish settlements included Santa Fe, Albuquerque, San Antonio, Tucson, San Diego, Los Angeles and San Francisco. Most Spanish settlements were along the California coast or the Santa Fe River in New Mexico.
Dutch colonization
Main article: New Netherland

Nieuw-Nederland, or New Netherland, was the seventeenth century Dutch colonial province on the eastern coast of North America. The claimed territory were the lands from the Delmarva Peninsula to Buzzards Bay, while the settled areas are now part of New Jersey, New York, Connecticut, Delaware, and Pennsylvania. Its capital, New Amsterdam, was located at the southern tip of the island of Manhattan on the Upper New York Bay and was renamed New York.
French colonization
See also: New France and Fort Caroline

New France was the area colonized by France in North America during a period extending from the exploration of the Saint Lawrence River, by Jacques Cartier in 1534, to the cession of New France to Spain and Britain in 1763. At its peak in 1712 (before the Treaty of Utrecht), the territory of New France extended from Newfoundland to the Rocky Mountains and from Hudson Bay to the Gulf of Mexico. The territory was divided in five colonies, each with its own administration: Canada, Acadia, Hudson Bay, Newfoundland and Louisiana.

Also during this period, French Huguenots, sailing under Jean Ribault, attempted to found a colony in what became the southeastern coast of the United States. Arriving in 1562, they established the ephemeral colony of Charlesfort on Parris Island in what is now South Carolina. When this failed, most of the colonists followed René Goulaine de Laudonnière and moved south, founding the colony of Fort Caroline at the mouth of the St. Johns River in what is now Jacksonville, Florida on June 22, 1564. Fort Caroline was destroyed in 1565 by the Spanish under Pedro Menéndez de Avilés, who moved in from St. Augustine, founded to the south earlier in the year.
British colonization
Main article: British colonization of the Americas
In 1607, the Virginia Company of London established the Jamestown Settlement on the James River, both named after King James I

The strip of land along the eastern seacoast was settled primarily by English colonists in the 17th century, along with much smaller numbers of Dutch and Swedes. Colonial America was defined by a severe labor shortage that gave birth to forms of unfree labor such as slavery and indentured servitude,[12] and by a British policy of benign neglect (salutary neglect) that permitted the development of an American spirit distinct from that of its European founders.[13] Over half of all European migrants to Colonial America arrived as indentured servants.[14]

The first successful English colony was established in 1607, on the James River at Jamestown. It languished for decades until a new wave of settlers arrived in the late 17th century and established commercial agriculture based on tobacco. Between the late 1610s and the Revolution, the British shipped an estimated 50,000 convicts to its American colonies.[15] During the Georgian era English officials exiled 1,000 prisoners across the Atlantic every year.[16] One example of conflict between Native Americans and English settlers was the 1622 Powhatan uprising in Virginia, in which Native Americans had killed hundreds of English settlers. The largest conflict between Native Americans and English settlers in the 17th century was King Philip's War in New England,[17] although the Yamasee War may have been bloodier.[18]

The Plymouth Colony was established in 1620. New England was initially settled primarily by Puritans who established the Massachusetts Bay Colony in 1630.[11] The Middle Colonies, consisting of the present-day states of New York, New Jersey, Pennsylvania, and Delaware, were characterized by a large degree of diversity. The first attempted English settlement south of Virginia was the Province of Carolina, with Georgia Colony the last of the Thirteen Colonies established in 1733.[19] Several colonies were used as penal settlements from the 1620s until the American Revolution.[20] Methodism became the prevalent religion among colonial citizens after the First Great Awakening, a religious revival led by preacher Jonathan Edwards in 1734.[11]
Political integration and autonomy
Join, or Die: This 1756 political cartoon by Benjamin Franklin urged the colonies to join together during the French and Indian War.

The French and Indian War (1754–1763) was a watershed event in the political development of the colonies. The influence of the main rivals of the British Crown in the colonies and Canada, the French and North American Indians, was significantly reduced. Moreover, the war effort resulted in greater political integration of the colonies, as symbolized by Benjamin Franklin's call for the colonies to "Join or Die".

Following Britain's acquisition of French territory in North America, King George III issued the Royal Proclamation of 1763 with the goal of organizing the new North American empire and stabilizing relations with the native Indians. In ensuing years, strains developed in the relations between the colonists and the Crown. The British Parliament passed the Stamp Act of 1765, imposing a tax on the colonies to help pay for troops stationed in North America following the British victory in the Seven Years' War.

The British government felt that the colonies were the primary beneficiaries of this military presence, and should pay at least a portion of the expense. The colonists did not share this view. Rather, with the French and Indian threat diminished, the primary outside influence remained that of Britain. A conflict of economic interests increased with the right of the British Parliament to govern the colonies without representation being called into question.
Two ships in a harbor, one in the distance. Onboard, men stripped to the waist and wearing feathers in their hair are throwing crates overboard. A large crowd, mostly men, is standing on the dock, waving hats and cheering. A few people wave their hats from windows in a nearby building.
Nathaniel Currier's 1846 depiction of the Boston Tea Party.[21]

The Boston Tea Party in 1773 was a direct action by colonists in the town of Boston to protest against the taxes levied by the British government. Parliament responded the next year with the Coercive Acts, which sparked outrage and resistance in the Thirteen Colonies. Colonists convened the First Continental Congress to coordinate their resistance to the Coercive Acts. The Congress called for a boycott of British trade, published a list of rights and grievances, and petitioned the king for redress of those grievances.

The Congress also called for another meeting if their petition did not halt enforcement of the Coercive Acts. Their appeal to the Crown had no effect, and so the Second Continental Congress was convened in 1775 to organize the defense of the colonies at the onset of the American Revolutionary War.
Formation of the United States of America (1776–1789)
Main article: History of the United States (1776–1789)
Washington's crossing of the Delaware River, one of the rebels' first successes in the Revolutionary War

The Thirteen Colonies began a rebellion against British rule in 1775 and proclaimed their independence in 1776. They subsequently constituted the first thirteen states of the United States of America, which became a nation state in 1781 with the ratification of the Articles of Confederation and Perpetual Union. The 1783 Treaty of Paris represented the Kingdom of Great Britain's formal acknowledgment of the United States as an independent nation.[3]

The United States defeated Britain with help from France, the United Provinces and Spain in the American Revolutionary War. The colonists' 1777 victory at Saratoga secured the Northeast and led the French into an open alliance with the United States.[22]

In 1781, a combined American and French Army, acting with the support of a French fleet, captured a large British army led by General Charles Cornwallis at Yorktown, Virginia. The surrender of General Cornwallis ended serious British efforts to find a military solution to their American problem. As Seymore Lipset observes, "The United States was the first major colony successfully to revolt against colonial rule. In this sense, it was the first 'new nation'."[23]
Trumbull's Declaration of Independence

Side by side with the states' efforts to gain independence through armed resistance, a political union was being developed and agreed upon by them. The first step was to formally declare independence from Great Britain. On July 4, 1776, the Second Continental Congress, still meeting in Philadelphia, declared the independence of "the united States of America" in the Declaration of Independence. July 4 is celebrated as the nation's birthday. The new nation was founded on Enlightenment ideals of liberalism and dedicated to principles of republicanism, which emphasized civic duty and a fear of corruption and hereditary aristocracy.[24] The new nation was governed by Congress, and followed the Articles of Confederation and Perpetual Union of 1777 (which was formally adopted in 1781).

After the war finally ended in 1783, there was a period of prosperity, with the entire world at peace. The national government was able to settle the issue of the western territories, which were ceded by the states to Congress and became territories (and after 1791 started to become states). Nationalists worried that the new nation was too fragile to withstand an international war, or even internal revolts such as the Shays' Rebellion of 1786 in Massachusetts. A series of attempts to organize a movement to outline and press reforms culminated in the Congress calling the Philadelphia Convention in 1787. The structure of the national government was profoundly changed on March 4, 1789, when the American people replaced the confederation-type government of the Articles with a federation-type government of the Constitution. The new government reflected a radical break from the normative governmental structures of the time, favoring representative, elective government with a power-sharing executive, rather than the monarchical structures common within the western traditions of the time. The system of republicanism borrowed heavily from the Enlightenment ideas and classical western philosophy: a primacy was placed upon preserving individual liberty and upon constraining the power of government through a system of separation of powers.[25]

To assuage the Anti-Federalists who feared a too-powerful national government, the nation adopted the United States Bill of Rights in 1791. Comprising the first ten amendments of the Constitution, it guaranteed individual liberties such as freedom of speech and religious practice, jury trials, and stated that citizens and states had reserved rights (which were not specified).[26]
Early national era (1789–1848)
Main article: History of the United States (1789–1849)
Economic growth in America per capita income

George Washington—a renowned hero of the American Revolutionary War, commander-in-chief of the Continental Army, and president of the Constitutional Convention—became the first President of the United States under the new US Constitution.

The major accomplishments of the Washington Administration were creating a strong national government that was recognized without question by all Americans, and, following the plans of Treasury Secretary Alexander Hamilton, assuming the debts of the states (the debt holders received federal bonds), creating the Bank of the United States to stabilize the financial system, setting up a uniform system of tariffs (taxes on imports) and other taxes to pay off the debt and provide a financial infrastructure. To support his programs Hamilton created a new political party--the first in the world based on voters--the Federalist Party. Thomas Jefferson and James Madison led the opposition, forming an opposition Republican Party (usually called the Democratic-Republican Party by historians). Hamilton and Washington presented the country in 1794 with the Jay Treaty that reestablished good relations with Britain. The Jeffersonians vehemently protested, and the voters aligned behind one party or the other, thus setting up the First Party System. The treaty passed, but politics became very heated.[27]

The Whiskey Rebellion in 1794, when settlers in the Pennsylvania counties west of the Allegheny Mountains protested against a federal tax on liquor and distilled drinks, was the first serious test of the federal government.[28]

At the end of his second presidential term, George Washington made his farewell address, which was published in the newspaper Independent Chronicle on September 26, 1796. In his address, Washington triumphed the benefits of federal government and importance of ethics and morality while warning against foreign alliances and formation of political parties.[29]

Vice-president John Adams, a Federalist, dedeated Jefferson in the 1796 election. War loomed with France and the Federalists used the opportunity to try to silence the Republicans with the Alien and Sedition Acts, build up a large army with Hamilton at the head, and prepare for a French invasion. However, the Federalists became divided after Adams sent a successful peace mission to France that ended the Quasi War of 1798. In 1800 Jefferson defeated Adams for the presidency in the 1800 election.[30]
Territorial expansion of the United States, omitting Oregon and other claims.

Although the Constitution included a Supreme Court, its functions were vague until John Marshall, the Chief Justice 1801-1835 defined them, especially the power to overturn acts of Congress that violated the Constitution, first enunciated in 1803 in Marbury v. Madison[31]


The Louisiana Purchase, in 1803, removed the French presence from the western border of the United States and provided US settlers with vast potential for expansion west of the Mississippi River.[32]


In response to multiple grievances, the Congress declared war on Britain in 1812. The grievances included humiliating the Americans in the "Chesapeake incident of 1807, continued British impressment of American sailors into the Royal Navy, restrictions on trade with France, and arming hostile Indians in Ohio and the western territories.[33] The War of 1812 ended in a draw after bitter fighting that lasted until January 8, 1815, during the Battle of New Orleans. The Americans gained no territory but were cheered by a sense of victory in what they called a "second wear of independence." The war was a major loss for Native American tribes in the Northwest and Southeast who had allied themselves with Britain and were defeated on the battlefield.

As strong opponents of the war, the Federalists held the Hartford Convention in 1814 that hinted at disunion. National euphoria after the victory at New Orleans ruined the prestige of the Federalists and they no longer played a significant role.[34] President Madison and most Republicans realized it had been a mistake to let the Bank of the United States close down, for its absence greatly hindered the financing of the war. So they chartered the Second Bank of the United States in 1816. The Republicans also imposed tariffs designed to protect the infant industries that had been created when Britain was blockading the U.S. With the collapse of the Federalists as a party, the adoption of many Federalist principles by the Republicans, and the systematic policy of President James Madison in his two terms (1817-25) to downplay partisanship, the nation entered an Era of Good Feelings, with far less partisanship than before (or after), and closed out the First Party System.[35][36]

The Monroe Doctrine, expressed in 1823, proclaimed the United States' opinion that European powers should no longer colonize or interfere in the Americas. This was a defining moment in the foreign policy of the United States. The Monroe Doctrine was adopted in response to American and British fears over Russian and French expansion into the Western Hemisphere.[37]

In 1830, Congress passed the Indian Removal Act, which authorized the president to negotiate treaties that exchanged Native American tribal lands in the eastern states for lands west of the Mississippi River. This established Andrew Jackson, a military hero and President, as a cunning tyrant in regards to native populations. The act resulted most notably in the forced migration of several native tribes to the West, with several thousand people dying en route, and the Creeks' violent opposition and eventual defeat. The Indian Removal Act also directly caused the ceding of Spanish Florida and led to the many Seminole Wars.[38]

After 1840 the abolitionist movement redefined itself, mobilized its supporters (especially among religious people in the Northeast affected by the Second Great Awakening), escalated its attacks, and proclaimed slave ownership a sin, not just an unfortunate social evil. It gained tens of thousands of followers. William Lloyd Garrison published the most influential of the many anti-slavery newspapers, The Liberator, while Frederick Douglass, an ex-slave, began writing for that newspaper around 1840 and started his own abolitionist newspaper North Star in 1847.[39]

The Republic of Texas was annexed by president John Tyler in 1845.[40] The US army, using regulars and large numbers of volunteers, defeated Mexico in 1848 during the Mexican-American War. Public sentiment in the US was divided as Whigs[41] and anti-slavery forces[42] opposed the war. The 1848 Treaty of Guadalupe Hidalgo ceded California, New Mexico, and adjacent areas to the United States, about thirty percent of Mexico. Westward expansion was enhanced further by the California Gold Rush, the discovery of gold in that state in 1848. Numerous "forty-niners" trekked to California in pursuit of gold; land-hungry European immigrants also contributed to the rising white population in the west.[11] In 1849 cholera spread along the California and Oregon Trails. An estimated 150,000 Americans died during the two cholera pandemics between 1832 and 1849.[43]
Civil War era (1849–1865)
Main article: History of the United States (1849–1865)
The Union: blue (free), yellow (slave);
The Confederacy: brown
*territories in light shades

In the middle of the 19th century, white Americans of the North and South were to reconcile fundamental differences in their approach to government, economics, society and African American slavery. The issue of slavery in the new territories was settled by the Compromise of 1850 brokered by Whig Henry Clay and Democrat Stephen Douglas; the Compromise included admission of California as a free state and the passage of the Fugitive Slave Act to make it easier for masters to reclaim runaway slaves.[40] In 1854, the proposed Kansas-Nebraska Act abrogated the Missouri Compromise by providing that each new state of the Union would decide its stance on slavery.[44]

By 1860, there were nearly four million slaves residing in the United States, nearly eight times as many from 1790; within the same time period, cotton production in the U.S. boomed from less than a thousand tons to nearly one million tons per year. There were some slave rebellions – including by Gabriel Prosser (1800), Denmark Vesey (1822), and Nat Turner (1831) – but they all failed and led to tighter slave oversight in the south.[45]
Abraham Lincoln with Allan Pinkerton and Major General John Alexander McClernand at the Battle of Antietam.

After Abraham Lincoln won the 1860 Election, eleven Southern states seceded from the union between late 1860 and 1861, establishing a new government, the Confederate States of America, on February 8, 1861.[46]

The Civil War began on April 12, 1861, when Confederate forces attacked a US military installation at Fort Sumter in South Carolina.[47] Along with the northwestern portion of Virginia, four of the five northernmost "slave states" did not secede and became known as the Border States.[46]

In response to this, on April 15, Lincoln called on the states to send detachments totaling 75,000 troops to recapture forts, protect the capital, and "preserve the Union", which in his view still existed intact despite the actions of the seceding states. The two armies had their first major clash at the First Battle of Bull Run, which ended in a surprising Union defeat, but, more importantly, proved to both the Union and Confederacy that the war was going be much longer and bloodier than they had originally anticipated.

The war soon divided into two theaters, the Eastern and Western theaters. In the western theater, the Union was quite successful, with major battles, such as Perryville ending up being strategic Union victories, destroying major confederate operations.
Anger at military conscription during the American Civil War led to the New York Draft Riots of 1863, one of the worst incidents of civil unrest in American history. The city's Irish and Excelsior brigades were among the five Union brigades with the most combat dead.

In the Eastern theater, things didn't start out well for the Union. In the summer of 1861, General Irvin McDowell was given the task of destroying the Confederacy in one quick battle with the newly created Army of Northeastern Virginia. Union and Confederate forces engaged in combat at Manassas junction, which resulted in a surprising Union defeat due in part to arrogant Confederate defense. Following McDowell's failure, Major General George B. McClellan was put in charge of the task at hand. After reorganizing the new Army of the Potomac, McClellan too failed to do so in his Peninsula Campaign and retreated after attacks from newly appointed Confederate General Robert E. Lee.

Feeling confident in his army after the Union defeat at the Second Bull Run, Lee decided to embark on an invasion of the north. However, his special order 191 was discovered two Union soldiers, and thus, McClellan could intercept and strategically stop Lee at the bloody Battle of Antietam. Despite this, McClellan was relieved from command for refusing to pursue Lee's crippled army.

General Ambrose Burnside was put in command of the Army of the Potomac. After receiving pressure from Lincoln, Burnside decided to cross the Rappahannock River at Fredericksburg, Virginia. However, Burnside's failure to cross the river in time resulted in Lee building strong defenses to oppose the Union. The humiliating Battle of Fredericksburg resulted in the Union retreat.

After Burnside's failed mud march, General "Fighting Joe" Hooker was placed in command of the Army.

On the following day in the west, Union forces under the command of General Ulysses S. Grant gained control of the Mississippi River at the Battle of Vicksburg, thereby splitting the Confederacy. At the beginning of 1864, Lincoln made General Grant commander of all Union armies. The following two years of the war ended up being bloody for both sides, with Grant launching a war of attrition against Confederate General Robert E. Lee's Army of Northern Virginia. This war of attrition was divided into three main campaigns.

The first of these, the Overland Campaign forced Lee to retreat into the city of Petersburg where Grant launched his second major offensive, the Richmond-Petersburg Campaign in which he sieged the city of Petersburg. After a near ten-month siege, the city of Petersburg surrendered. However, the defense of Fort Gregg allowed Lee to move his army out of Petersburg. Grant pursued and launched the final, Appomattox Campaign which resulted in Lee surrendering his Army of Northern Virginia on April 9, 1865, at Appomattox Court House.[46] When word of Lee's surrender spread across the country, many Confederate armies also surrendered, with Stand Watie being the last of the generals.

Based on 1860 census figures, 8% of all white males aged 13 to 43 died in the war, including 6% in the North and an extraordinary 18% in the South,[48] establishing the American Civil War as the deadliest war in American history. Its legacy includes ending slavery in the United States, restoring the Union, and strengthening the role of the federal government. The social, political, economic and racial issues of the war decisively shaped the reconstruction era, which lasted through 1877, and brought about changes that would eventually help make the country a united superpower.
Reconstruction and the Gilded Age (1865–1890)
Main article: History of the United States (1865–1918)
Completion of the Transcontinental Railroad (1869) at First Transcontinental Railroad, by Andrew J. Russell

Reconstruction took place for most of the decade following the Civil War. During this era, the "Reconstruction Amendments" were passed to expand civil rights for black Americans. Those amendments included the Thirteenth Amendment, which outlawed slavery, the Fourteenth Amendment that guaranteed citizenship for all people born or naturalized within U.S. territory, and the Fifteenth Amendment that granted the vote for all men regardless of race. While the Civil Rights Act of 1875 forbade discrimination in the service of public facilities, the Black Codes denied blacks privileges readily available to whites.[49]

In response to Reconstruction, the Ku Klux Klan (KKK) emerged around the late 1860s as a white-supremacist organization opposed to black civil rights. Congress passed the Ku Klux Klan Act of 1870 and vigorous enforcement closed down the Klan classified the KKK as a terrorist group. However, an 1883 Supreme Court decision nullified the Civil Rights Act of 1875 and ended federal efforts to stop private acts of violence designed to suppress legal rights.[50]

During the era, many regions of the southern U.S. were military-governed and often corrupt; Reconstruction ended after the disputed 1876 election between Republican candidate Rutherford B. Hayes and Democratic candidate Samuel J. Tilden. Hayes won the election, and the South soon re-entered the national political scene.[51]

The "Gilded Age" was a term that Mark Twain used to describe the period of the late nineteenth century when there had been a dramatic expansion of American wealth and propserity. Reform of the Age included the Civil Service Act, which mandated a competitive examination for applicants for government jobs. Other important legislation included the Interstate Commerce Act, which ended railroads' discrimination against small shippers, and the Sherman Antitrust Act, which outlawed monopolies in business. Twain believed that this age was corrupted by such elements as land speculators, scandalous politics, and unethical business practices.[52]

By century's end, American industrial production and per capita income exceeded those of all other world nations and ranked only behind Great Britain. In response to heavy debts and decreasing farm prices, wheat and cotton farmers joined the Populist Party.[53] Later, an unprecedented wave of immigration served both to provide the labor for American industry and create diverse communities in previously undeveloped areas. From 1880 to 1914, peak years of immigration, more than 22 million people migrated to the United States.[54] Abusive industrial practices led to the often violent rise of the labor movement in the United States.[55] Influential figures of the period included John D. Rockefeller and Andrew Carnegie.
Progressivism, imperialism, and World War I (1890–1918)
Main article: Progressive Era
Mulberry Street, along which Manhattan's Little Italy is centered. Lower East Side, circa 1900. Almost 97% of residents of the 10 largest American cities of 1900 were non-Hispanic whites.[56]

After the Gilded Age came the Progressive Era, whose followers called for reform over perceived industrial corruption. Viewpoints taken by progressives included greater federal regulation of anti-trust laws and the industries of meat-packing, drugs, and railroads. Four new constitutional amendments—the Sixteenth through Nineteenth—resulted from progressive activism.[57] The era lasted from 1900 to 1918, the year marking the end of World War I.[58]

U.S. Federal government policy, since the James Monroe Administration, had been to move the indigenous population beyond the reach of the federal frontier into a series of Indian reservations. Tribes were generally forced onto small reservations as farmers and ranchers took over their lands.
Ellis Island in 1902, the main immigration port for immigrants entering the United States in the late 19th and early 20th centuries.

The United States began its rise to international power in this period with substantial population and industrial growth domestically and numerous military ventures abroad, including the Spanish-American War, which began when the United States blamed the sinking of the USS Maine on Spain. Also at stake were U.S. interests in acquiring Cuba, an island nation fighting for independence from Spanish occupation; Puerto Rico and the Philippines were also two former Spanish colonies seeking liberation.

In December 1898, representatives of Spain and the U.S. signed the Treaty of Paris to end the war, with Cuba becoming an independent nation and Puerto Rico, Guam, and the Philippines becoming U.S. territories.[11][59] In 1900, Congress passed the Open Door Policy that required China to grant equal trading access to all foreign nations.[11]

President Woodrow Wilson declared U.S. entry into World War I in April 1917 following a yearlong neutrality policy; the U.S. had previously shown interest in world peace by participating in the Hague Conferences. American participation in the war proved essential to the Allied victory. Wilson also implemented a set of propositions titled the Fourteen Points to ensure peace, but they were denied at the 1919 Paris Peace Conference. Isolationist sentiment following the war also blocked the U.S. from participating in the League of Nations, an important part of the Treaty of Versailles.[11]
Women's suffrage
Main article: History of women's suffrage in the United States
Alice Paul stands before the Woman Suffrage Amendment's ratification banner. She immediately went on to write the Equal Right Amendment, whose passage would become an important goal of the Women's Liberation Movement half a century later.

These years of the early 20th century also saw the strengthening of the Woman Suffrage Movement. The movement had begun with the 1848 Seneca Falls Convention, organized by Elizabeth Cady Stanton and Lucretia Mott, and the Declaration of Sentiments demanding equal rights for women. The women's rights campaign during "first-wave feminism" was led by Mott, Stanton, Susan B. Anthony, Sojourner Truth, Lucy Stone, and Julia Ward Howe, among others. By the end of the 19th century only several states had granted women full voting rights, though women had made significant legal victories, gaining rights in areas such as property and child custody. In 1875 the Supreme Court ruled women, too, were American citizens (but this did not give them the right to vote).

Around 1912 the Feminist Movement, which had grown sluggish, began to reawaken. Protests became increasingly common as suffragette Alice Paul led parades through the capital and major cities. Paul split from the large National American Woman Suffrage Association (NAWSA), which favored a more moderate approach and supported the Democratic Party and Woodrow Wilson, led by Carrie Chapman Catt, and formed the more militant National Woman's Party. Suffragists were arrested during their "Silent Sentinal" pickets at the White House, the first time such a tactic was used, and were taken as political prisoners. In prison they were tortured and force-fed while on hunger strikes led by Alice Paul.

Finally, the suffragette were ordered released from prison, and Wilson addressed the Congress on woman suffrage, urging them to pass a Constitutional amendment enfranchising women, which they did in 1919. It became constitutional law on August 26, 1920, after ratification by the 36th required state. NAWSA became the League of Women Voters and the National Woman's Party began lobbying for full equality and the Equal Rights Amendment which would pass Congress during the second wave of the women's movement in 1972. Following ratification of the Nineteenth Amendment, a U.S. Court ruled the arrests of the over two hundred suffragists as unconstitutional, and the amendment was upheld by the Supreme Court after a legal challenge.
Post-World War I and the Great Depression (1918–1940)
Main article: History of the United States (1918–1945)

Following World War I, the U.S. grew steadily in stature as an economic and military world power. The United States Senate did not ratify the Treaty of Versailles imposed by its Allies on the defeated Central Powers; instead, the United States chose to pursue unilateralism, if not isolationism.[60] The aftershock of Russia's October Revolution resulted in real fears of communism in the United States, leading to a three-year Red Scare. In 1918 the U.S. lost 675,000 people to the Spanish flu pandemic.[61]
Prohibition agents destroying barrels of alcohol in Chicago, 1921

In 1920, the manufacture, sale, import and export of alcohol was prohibited by the Eighteenth Amendment to the United States Constitution. Prohibition encouraged illegal breweries and dealers to make substantial amounts of money selling alcohol illegally. The Prohibition ended in 1933, a failure. Additionally, the KKK re-formed during that decade and gathered nearly 4.5 million members by 1924, and the U.S. government passed the Immigration Act of 1924 restricting foreign immigration.[62] The 1920s were also known as the Roaring Twenties, due to the great economic prosperity during this period. Jazz became popular among the younger generation, and thus was also called the Jazz Age.
Dorothea Lange's Migrant Mother, depicts destitute pea pickers in California, centering on a mother of seven children, age thirty-two, in Nipomo, California, March 1936.

During most of the 1920s, the United States enjoyed a period of unbalanced prosperity: farm prices and wages fell, while new industries and industrial profits grew. The boom was fueled by an inflated stock market, which later led to a crash on October 29, 1929.[63] This, along with many other economic factors, triggered a worldwide depression known as the Great Depression. During this time, the United States experienced deflation, unemployment increased from 3% in 1929 to 25% in 1933, and manufacturing output collapsed by one-third.

In 1932, Democratic presidential nominee Franklin D. Roosevelt promised "a new deal for the American people", a phrase that has endured as a label for his administration and its many domestic achievements. The desperate economic situation, along with the substantial Democratic victories in the 1932 elections, gave Roosevelt unusual influence over Congress in the "First Hundred Days" of his administration. He used his leverage to win rapid passage of a series of measures to create welfare programs and regulate the banking system, stock market, industry and agriculture, along with many other government efforts to end the Great Depression and reform the American economy. Some programs that were a part of Roosevelt's New Deal include the Works Progress Administration (WPA) relief program, the Social Security Act, the Emergency Banking Act, and the Economy Act. The recovery was rapid in all areas except unemployment, which remained fairly high until 1940.
World War II (1941–1945)
Text document with red question mark.svg
	This section includes a list of references or external links, but its sources remain unclear because it has insufficient inline citations. Please help to improve this article by introducing more precise citations where appropriate. (September 2008)
Main articles: World War II and United States home front during World War II

As with World War I, the United States did not enter World War II until after the rest of the active Allied countries had done so. The United States first contribution to the war was simultaneously to cut off the oil and raw material supplies needed by Japan to maintain its offensive in China, and to increase military and financial aid to China. Contribution came to the Allies in September 1940 in the form of the Lend-Lease program with Britain.

On December 7, 1941 Japan launched a surprise attack on the American naval base in Pearl Harbor, citing America's recent trade embargo as justification. The following day, Franklin D. Roosevelt successfully urged a joint session of Congress to declare war on Japan, calling December 7, 1941 "a date which will live in infamy". Four days after the attack on Pearl Harbor, on December 11, Nazi Germany declared war on the United States, drawing the country into a two-theater war.
Battle against Germany
Further information: Europe first

Upon entering the war, the United States and its allies decided to concentrate the bulk of their efforts on fighting Hitler in Europe, while maintaining a defensive position in the Pacific until Hitler was defeated. The United States's first step was to set up a large airforce in Britain to concentrate on bombing raids into Germany. The American Air force relied on the B-17 Flying Fortress as its primary heavy bomber. Britain had ceased its daylight bombing raids, due to heavy casualties inflicted by the Luftwaffe. The USAAF suffered similar high losses until the introduction of the P-51 Mustang as a long range escort fighter for the bombers.
Landing at Normandy at Battle of Normandy, by Robert F. Sargent, United States Army. Total U. S. military deaths in battle and from other causes were 416,837.

The American army's first ground action was fighting alongside the British, Australian and New Zealand armies in North Africa. By May 1943, the British 8th Army had expelled the Germans from North Africa and the Allies controlled this vital link until the end of the war. The American navy also played an active role in the Atlantic protecting the convoys bringing vital American war material to Britain. By midway through 1943, the Allies were fighting the war from Britain with unbroken supply lines, while at the same time Hitler's armies were very much on the back foot, with heavy bombing taking its toll on production.

By early 1944, a planned invasion of Western Europe was underway. What followed on June 6, 1944, was Operation Overlord, or D-Day. The largest war armada ever assembled landed on the beaches of Normandy and began the penetration of Western Europe that eventually overthrew Hitler and Nazi Germany. Following the landing at Normandy, the Americans contributed greatly to the outcome of the war, with dogged fighting in the Battle of the Bulge resulting in Allied victories against the Germans.

The battles took a heavy toll on the Americans, who lost 19,000 men during the Battle of the Bulge alone. The allied bombing raids on Germany increased to unprecedented levels after the D-Day invasion, with over 70% of all bombs dropped on Germany occurring after this date. On April 30, 1945, with Berlin completely overrun with Russian forces and his country in tatters, Adolf Hitler committed suicide. On May 8, 1945, the war with Germany was over, following its unconditional surrender to the Allied forces.
Battle against Japan
Main article: Pacific War

Due to the United States commitment to defeating Hitler in Europe, the first years of the war against Japan was largely a defensive battle with the United States Navy attempting to prevent the Japanese Navy from asserting dominance of the Pacific region. Initially, Japan won most of its battles in a short time. Japan quickly defeated and created military bases in Guam, Thailand, Malaya, Hong Kong, Papua New Guinea, Indonesia and Burma. This was done virtually unopposed and with quicker speed than that of the German Blitzkrieg during the early stages of the war. This was important for Japan, as it had only 10% of the homeland industrial production capacity of the United States.
Douglas MacArthur lands at the Battle of Leyte, by U.S. Army Signal Corps

The turning point of the war was the Battle of Midway in June 1942. Following this, the Americans began fighting towards China where they could build an airbase suitable to commence bombing of mainland Japan with its B-29 Superfortress fleet. The Americans began by selecting smaller, lesser defended islands as targets as opposed to attacking the major Japanese strongholds. During this period, they inadvertently triggered what would become their most comprehensive victory in the entire war.

The Pacific war became the largest naval conflict in history. The American Navy emerged victorious, after at one point being stretched near to the breaking point, with almost complete destruction of the Japanese Navy. The American forces were then poised for an invasion of the Japanese mainland, to force the Japanese into unconditional surrender. On April 12, 1945, President Franklin D. Roosevelt died and Vice President Harry S. Truman was sworn in as the 33rd President of the United States. The use of atomic weapons against Japan was subsequently authorized.

The decision to use nuclear weapons to end the conflict has been one of the most controversial decisions of the war. Supporters of the use of the bombs argue that an invasion would have cost an enormous numbers of lives, while opponents argue that the large number of civilian casualties resulting from the bombings was unjustified. The first bomb was dropped on Hiroshima on August 6, 1945, and the second bomb was dropped on Nagasaki on August 9, 1945. On August 15, 1945, the Japanese surrendered unconditionally, ending World War II.
The Cold War begins (1945–1964)
Text document with red question mark.svg
	This section includes a list of references or external links, but its sources remain unclear because it has insufficient inline citations. Please help to improve this article by introducing more precise citations where appropriate. (September 2008)
Main article: History of the United States (1945–1964)
President Kennedy's address on Civil Rights, June 11, 1963.

Following World War II, the United States emerged as one of the two dominant superpowers. The U.S. Senate, on December 4, 1945, approved U.S. participation in the United Nations (UN), which marked a turn away from the traditional isolationism of the U.S. and toward more international involvement. The post-war era in the United States was defined internationally by the beginning of the Cold War, in which the United States and the Soviet Union attempted to expand their influence at the expense of the other, checked by each side's massive nuclear arsenal and the doctrine of mutual assured destruction. The result was a series of conflicts during this period including the Korean War and the tense nuclear showdown of the Cuban Missile Crisis. Within the United States, the Cold War prompted concerns about Communist influence, and also resulted in government efforts to focus mathematics and science toward efforts such as the space race.

In the decades after World War II, the United States became a global influence in economic, political, military, cultural, and technological affairs. Beginning in the 1950s, middle-class culture had a growing obsession with consumer goods. White Americans made up nearly 90% of the population in 1950.[64]

In 1960, John F. Kennedy was elected President. Known for his charisma, he is so far the only Roman Catholic to be President. The Kennedy family had brought a new life and vigor to the atmosphere of the White House. His time in office was marked by such notable events as the acceleration of the United States' role in the space race; the beginning of the escalation of the American role in the Vietnam War; the Cuban missile crisis; the Bay of Pigs Invasion of Cuba—events that aggravated the Cold War with the USSR; attack of the Freedom Rides; mob violence directed at James Meredith during the integration of the University of Mississippi; the jailing of Martin Luther King, Jr. during the Birmingham campaign; and the appointment of his brother Robert F. Kennedy to his Cabinet as Attorney General. He was assassinated in Dallas, Texas, on November 22, 1963 by Lee Harvey Oswald.
The Civil Rights Movement (1955–1970)
Main article: African American Civil Rights Movement
Martin Luther King gives his I Have a Dream speech at the 1963 March on Washington for Jobs and Freedom.

Meanwhile, the American people completed a great migration from farms into the cities and experienced a period of sustained economic expansion. At the same time, institutionalized racism across the United States, but especially in the American South, was increasingly challenged by the growing Civil Rights movement. The activism of African American leaders Rosa Parks and Martin Luther King, Jr. led to the Montgomery Bus Boycott, which launched the movement. For years African Americans would struggle with violence against them, but would achieve great steps towards equality with Supreme Court decisions, including Brown v. Board of Education and Loving v. Virginia, the Civil Rights Act of 1964, the Voting Rights Act of 1965, and the Fair Housing Act of 1968, which ended the Jim Crow laws that legalized racial segregation between Whites and Blacks.

Martin Luther King, Jr., who had won the Nobel Peace Prize for his efforts to achieve equality of the races, was assassinated in 1968. Following his death other leaders led the movement, most notably King's widow, Coretta Scott King, who was also active, like her husband, in the Opposition to the Vietnam War, and in the Women's Liberation Movement. Over the first nine months of 1967, 128 American cities suffered 164 riots.[65] The late 1960s and early 1970s saw the strengthening of Black Power, however the decade would ultimately bring about positive strides toward integration.
The Women's Movement (1963–1982)
Main article: Feminist Movement in the United States
Gloria Steinem at a meeting of the Women's Action Alliance, 1972.

A new consciousness of the inequality of American women began sweeping the nation, starting with the 1963 publication of Betty Friedan's best-seller, The Feminine Mystique, which explained how many housewives felt trapped and unfulfilled, assaulted American culture for its creation of the notion that women could only find fulfillment through their roles as wives, mothers, and keepers of the home, and argued that women were just as able as men to do every type of job. In 1966 Friedan and others established the National Organization for Women, or NOW, to act as an NAACP for women. Protests began, and the new "Women's Liberation Movement" grew in size and power, gained much media attention, and, by 1968, had replaced the Civil Rights Movement as the U.S.'s main social revolution. Marches, parades, rallies, boycotts, and pickets brought out thousands, sometimes millions; Friedan's Women's Strike for Equality (1970) was a nation-wide success. The Movement was factioned early on, however (NOW on the left, the Women's Equity Action League (WEAL) on the right, the National Women's Political Caucus (NWPC) in the center, and more radical groups formed by younger women on the far left). Along with Friedan, Gloria Steinem was an important feminist leader, co-founding the NWPC, the Women's Action Alliance, and editing the Movement's magazine, Ms. The proposed Equal Rights Amendment to the Constitution, passed by Congress in 1972 and favored by about seventy percent of the American public, failed to be ratified in 1982, with only three more states needed to make it law. However, many federal laws (i.e. those equalizing pay, employment, education, employment opportunites, credit, ending pregnancy discrimination, and requiring NASA, the Military Academies, and other organizations to admit women), state laws (i.e. those ending spousal abuse and marital rape), Supreme Court rulings (i.e. ruling the equal protection clause of the Fourteenth Amendment applied to women), and state ERAs established women's equal status under the law, and social custom and consciousness began to change, accepting women's equality. The controversial issue of abortion, legalized in 1973 is still a point of feminist debate today.
The Counterculture Revolution and Cold War Détente (1964–1980)
Main article: History of the United States (1964–1980)

Amid the Cold War, the United States entered the Vietnam War, whose growing unpopularity fed already existing social movements, including those among women, minorities and young people. President Lyndon B. Johnson's Great Society social programs and the judicial activism of the Warren Court added to the wide range of social reform during the 1960s and 1970s. Feminism and the environmental movement became political forces, and progress continued toward civil rights for all Americans. The Counterculture Revolution swept through the nation and much of the western world in the late sixties and early seventies, dividing the already hostile environment but also bringing forth more liberated social views.
United States Navy F-4 Phantom II intercepts a Soviet Tu-95 Bear D aircraft in the early 1970s

Johnson was succeeded by President Richard Nixon in 1969, who initially escalated the Vietnam War but soon negotiated a peace treaty in 1973, effectively ending American involvement in the war. The war had cost the lives of 58,000 American troops and millions of Vietnamese. Nixon used a conflict in the Eastern Bloc between the Soviet Union and China to the advantage of the United States, bolstering relations with the People's Republic of China.[66] A new era of Cold War relations known as détente (cooperation) was begun.[67]

The OPEC oil embargo led to a period of slow economic growth in 1973. The Watergate scandal, resulting from the break-in into the Democratic National Committee headquarters at the Watergate office complex in Washington, D.C. ultimately led to Nixon's resignation on August 9, 1974, as well as the indictment and conviction of several Nixon administration officials. During the years of his successor, Gerald Ford, the American-backed South Vietnamese government collapsed.

Jimmy Carter was elected in 1976 on the notion that he was not a part of the Washington political establishment.[68] The U.S. was afflicted with a recession, an energy crisis, slow economic growth, high unemployment, and high inflation coupled with high interest rates (the term stagflation was coined). On the world stage, Carter brokered the Camp David Accords between Israel and Egypt. In 1979, Iranian students stormed the U.S. embassy in Tehran and took 52 Americans hostage. Carter lost the 1980 election to Republican Ronald Reagan, whose campaign message advertised that his presidency would bring "Morning in America."[69]
The end of the Cold War (1980–1991)
Main article: History of the United States (1980–1991)
Ronald Reagan at the Brandenburg Gate challenges Gorbachev to tear down the Berlin Wall in 1987, shortly before the end of the Cold War

Ronald Reagan produced a major realignment with his 1980 and 1984 landslides. Reagan's economic policies (dubbed "Reaganomics") and the implementation of the Economic Recovery Tax Act of 1981 lowered income taxes from 70% to 28% over the course of seven years. Reagan continued to downsize government taxation and regulation.[70] The U.S. experienced a recession in 1982; unemployment and business failures soon entered rates close to Depression-era levels. These negative trends reversed the following year, when the inflation rate decreased from 11% to 2%, the unemployment rate decreased from 10.8% in December 1982 to 7.5% in November 1984,[71] and the economic growth rate increased from 4.5 to 7.2%.[72]

Reagan ordered a massive buildup of the U.S. military, incurring a costly budget deficit. Reagan introduced a complicated missile defense system known as the Strategic Defense Initiative (dubbed "Star Wars" by opponents) in which the U.S. could, in theory, shoot down missiles with laser systems in space. Though it was never fully developed or deployed,[73] the Soviets were genuinely concerned about the possible effects of the program[74] and the research and technologies of SDI paved the way for the anti-ballistic missile systems of today.[75]

The Reagan administration also provided covert funding and assistance to anti-Communist resistance movements worldwide. Reagan's interventions against Grenada and Libya were popular in the U.S., though his backing of the Contra rebels was mired in controversy.[76] The arms-for-hostages scandal led to the convictions of such figures as Oliver North and John Poindexter.[77]

Reagan met four times with Soviet Leader Mikhail Gorbachev, who ascended to power in 1985, and their summit conferences led to the signing of the INF Treaty. Gorbachev tried to save Communism in the Soviet Union first by ending the expensive arms race with America,[78] then by shedding the East European empire in 1989. The Soviet Union collapsed in 1991, ending the US-Soviet Cold War.
The World Superpower (1991–present)
Main article: History of the United States (1991–present)
Text document with red question mark.svg
	This section includes a list of references or external links, but its sources remain unclear because it has insufficient inline citations. Please help to improve this article by introducing more precise citations where appropriate. (March 2010)
1991-2001
USAF aircraft fly over Kuwaiti oil fires during the Gulf War

After the fall of the Soviet Union, the United States emerged as the world's sole remaining superpower and continued to involve itself in military action overseas, including the 1991 Gulf War. Following his election in 1992, President Bill Clinton oversaw unprecedented gains in securities values, a side effect of the digital revolution and new business opportunities created by the Internet (see Internet bubble). The 1990s saw one of the longest periods of economic expansion. Under Clinton an attempt to universalize health care, led by First Lady Hillary Rodham Clinton failed after almost two years of work on the controversial plan, however Hillary Rodham Clinton did succeed, along with a bipartisan coalition of members of congress, in establishing the Children's Health Insurance Program.[79]

The regime of Saddam Hussein in Iraq proved a continuing problem for the UN and Iraq's neighbors in its refusal to account for previously known stockpiles of chemical and biological weapons, its violations of UN resolutions, and its support for terrorism against Israel and other countries. After the 1991 Gulf War, the US, French, and British military's began patrolling the Iraqi no-fly zones to protect Iraq's Kurdish minority and Shi’ite Arab population – both of which suffered attacks from the Hussein regime before and after the 1991 Gulf War – in Iraq's northern and southern regions, respectively.[80] In the aftermath of Operation Desert Fox during December 1998, Iraq announced that it would no longer respect the no-fly zones and resumed its efforts in shooting down Allied aircraft.[81]

During the 1990s the al-Qaeda terrorist network and other Islamic fundamentalist groups attempted terrorist attacks against the United States and other nations. In 1993, Ramzi Yousef, a Kuwaiti national, and suspected al-Qaeda operative, planted explosives in the underground garage of One World Trade Center and detonated them, killing six people and injuring thousands. Later that year in the Battle of Mogadishu, US Army Rangers engaged Somali militias supported by al-Qaeda in an extended firefight that cost the lives of 19 American soldiers. President Clinton subsequently withdrew US combat forces from Somalia (there originally to support UN relief efforts).[82] Terrorist attacks occurred in the 1996 Khobar Towers bombing in Saudi Arabia, and the 1998 United States embassy bombings in Tanzania and Kenya. There was an attempted bombing at Los Angeles International Airport and other attempts of acts of terrorism during the 2000 millennium attack plots. In Yemen the USS Cole was bombed in October 2000, which the government associated with Osama bin Laden's al-Qaeda terrorist network.[83]

US responses to terrorist attacks included limited cruise missile strikes on Afghanistan and Sudan (August 1998), which failed to stop al-Qaeda's leaders and their Taliban supporters. Also in 1998, President Clinton signed the Iraq Liberation Act which called for regime change in Iraq because Saddam Hussein had possessed weapons of mass destruction, oppressed Iraqi citizens and attacked other Middle Eastern countries.[84]

Al-Qaeda and other Islamic fundamentalist groups were not the only groups responsible for terrorism during this time. In 1995, a domestic terrorist bombing took place at a federal building in Oklahoma City, which killed 168 people, and was then the biggest terrorist attack on US soil since World War II. The perpetrators, Timothy McVeigh and Terry Nichols, objected to the federal government and sought revenge for the sieges at Ruby Ridge (1992) and Waco (1993).[85]

In 1998, Clinton was impeached for charges of perjury and obstruction of justice that arose from lying about a sexual relationship with White House intern Monica Lewinsky. He was the second president to have been impeached. The House of Representatives voted 228 to 206 on December 19 to impeach Clinton,[86] but on February 12, 1999, the Senate voted 55 to 45 to acquit Clinton of the charges.[87]

2000 Election

The presidential election in 2000 between George W. Bush (R) and Al Gore (D) was one of the closest in the U.S. history, and helped lay the seeds for political polarization to come. Although Bush won the majority of electoral votes, Gore won the majority of the popular vote. In the days following Election Day, the state of Florida entered dispute over the counting of votes due to technical issues over certain Democratic votes in some counties.[88] The Supreme Court case Bush v. Gore was decided on December 12, 2000, ending the recount with a 5–4 vote and certifying Bush as president.[89]
2001-present

9-11 attack in 2001

New York under attack in the September 11 attacks

At the beginning of the new millennium, the United States found itself attacked by Islamic terrorism, with the September 11, 2001 attacks in which 19 Islamists hijacked four transcontinental airliners and intentionally crashed two of them into the twin towers of the World Trade Center and one into the Pentagon. The passengers on the fourth plane, United Airlines Flight 93, revolted causing the plane to crash into a field in Somerset County, Pennsylvania. 2,976 people and the 19 hijackers perished in the attacks.

According to the 9/11 Commission Report, that plane was intended to hit the US Capitol Building in Washington. The twin towers of the World Trade Center collapsed, destroying the entire complex. The United States soon found large amounts of evidence that suggested that the terrorist group al-Qaeda, spearheaded by Osama bin Laden, was responsible for the attacks.

War in Afghanistan and Iraq

George W. Bush in a televised address from the USS Abraham Lincoln.

In response to the attacks, under the administration of President George W. Bush, the United States (with the military support of NATO and the political support of some of the international community) launched Operation Enduring Freedom which overthrew the Taliban regime in Afghanistan which had protected and harbored bin Laden and al-Qaeda. With the support of large bipartisan majorities, the US Congress passed the Authorization for Use of Military Force Against Iraq Resolution of 2002.

With a coalition of other countries including Britain, Spain, Australia, Japan and Poland, in March 2003 President Bush ordered an invasion of Iraq dubbed Operation Iraqi Freedom which led to the overthrow and capture of Saddam Hussein. Using the language of 1998 Iraq Liberation Act and the Clinton Administration, the reasons cited by the Bush administration for the invasion included the spreading of democracy, the elimination of weapons of mass destruction[90] (a key demand of the UN as well, though later investigations found parts of the intelligence reports to be inaccurate)[91] and the liberation of the Iraqi people.[92] This second invasion fueled protest marches in many parts of the world.

Immigration

Despite tougher border scrutiny after 9/11, nearly 8 million immigrants came to the United States from 2000 to 2005 – more than in any other five-year period in the nation's history.[93] Almost half entered illegally.[94]

Oil

By 2006, rising prices saw Americans become increasingly conscious of the nation's dependence on supplies of petroleum for energy, with President Bush admitting a U.S. "addiction" to oil.[95] The possibility of serious economic disruption, should conflict overseas or declining production interrupt the flow, could not be ignored, given the instability in the Middle East and other oil-producing regions of the world. Many proposals and pilot projects for replacement energy sources, from ethanol to wind power and solar power, received more capital funding and were pursued more seriously in the 2000s than in previous decades. The 2006 midterm elections saw Congresswoman Nancy Pelosi become Speaker of the United States House of Representatives and the highest ranking woman in the history of the U.S. government.[96]

Counter-terrorism

In addition to military efforts abroad, in the aftermath of 9/11 the Bush Administration increased domestic efforts to prevent future attacks, a new cabinet level agency called the United States Department of Homeland Security was created to lead and coordinate federal counter-terrorism activities. The USA PATRIOT Act removed legal restrictions on information sharing between federal law enforcement and intelligence services and allowed for the investigation of suspected terrorists using means similar to those in place for other types of criminals. A new Terrorist Finance Tracking Program monitored the movements of terrorists' financial resources but was discontinued after being revealed by The New York Times.[97] Telecommunication usage by known and suspected terrorists was studied through the NSA electronic surveillance program.

Since 9/11, Islamic extremists made various attempts to attack the US homeland, with varying levels of organization and skill. For example, in 2001 vigilant passengers aboard a transatlantic flight to Miami prevented Richard Reid from detonating an explosive device.

After months of brutal violence against Iraqi civilians by Sunni and Shi’ite terrorist groups and militias—including al-Qaeda in Iraq—in January 2007 President Bush presented a new strategy for Operation Iraqi Freedom based upon counter-insurgency theories and tactics developed by General David Petraeus. The Iraq War troop surge of 2007 was part of this "new way forward".[98] The George W. Bush administration also increased allegations implicating Iran and Syria, in the development of weapons of mass destruction.

Late 2000s Recession

In December 2007, the United States entered the longest post-World War II recession,[99] which included a housing market correction, a subprime mortgage crisis, soaring oil prices, and a declining dollar value.[100] In February 2008, 63,000 jobs were lost, a 5-year record for a single month.[101][102] In September 2008, the crisis became much worse beginning with the government takeover of Fannie Mae and Freddie Mac followed by the collapse of Lehman Brothers.[103]

This economic crisis was considered the worst financial crisis since the Great Depression.[104][105] In November 2008, over 500,000 jobs were lost, which marked the largest loss of jobs in the United States in 34 years.[106] The Bureau of Labor Statistics reported that in the last four months of 2008, 1.9 million jobs were lost.[107] By the end of 2008, the U.S. had lost a total of 2.6 million jobs,[108] and the unemployment rate rose to 7.2%.[109]

2008 election

In the presidential election of 2008, Senator Barack Obama, having narrowly defeated Senator Hillary Rodham Clinton for the Democratic nomination, ran on a platform of "Hope and Change". The ticket of Obama and Senator Joe Biden was victorious against the Republican ticket of Senator John McCain and Governor Sarah Palin. On November 4, Obama became the first African American to be elected President of the United States; he was sworn into office as the 44th President on January 20, 2009.

During his first 100 days in office, Obama signed into law the American Recovery and Reinvestment Act of 2009, a $787 billion economic stimulus package aimed at helping the economy recover from the deepening worldwide recession.[110][111] The act included increased federal spending for health care, infrastructure, education, various tax breaks and incentives, and direct assistance to individuals,[112] which is being distributed over the course of several years, with about 25% due by the end of 2009. The Obama administration also enacted additional economic programs designed to stimulate the economy, such as the Car Allowance Rebate System,[113] the Public-Private Investment Program,[114] and the Automobile Industry Bailout.[115] In the third quarter of 2009, the U.S. economy expanded at a 2.2% annual pace,[116] after contracting for four consecutive quarters.[117] However, the unemployment rate continued to rise to 10.1%,[118][119] the highest level since 1983, and the underemployment rate continued to rise to 17.5%, the highest since records began being kept in 1994.[120]

Early in his presidency, Obama also moved to change the U.S. war strategy in Iraq and Afghanistan. In February 2009, Obama announced his plan to decrease troop levels in Iraq, stating that all combat troops would be withdrawn from Iraq by August 31, 2010, and that as many as 50,000 would remain in Iraq to train, equip and advise Iraqi forces, help protect withdrawing forces and work on counterterrorism until December 31, 2011.[121][122] He also announced that same month that the amount of troops in Afghanistan would be boosted by 17,000.[123] In December 2009, Obama announced that an additional 30,000 troops would be deployed to Afghanistan over a period of six months,[124] and also proposed to begin troop withdrawals 18 months from that date.[125][126]

As of 2010, debates continue over abortion, gun control, medical marijuana, same-sex marriage, immigration reform, climate change, health care reform, and the ongoing wars in Iraq and Afghanistan. In foreign policy, the U.S. maintains ongoing talks, led by United States Secretary of State Hillary Clinton, with North Korea over its nuclear weapons program, as well as with Israel and the Palestinian Authority over a two-state solution to the Israeli-Palestinian conflict; the Palestinian-Israeli talks began in 2007, an effort spearheaded by United States Secretary of State Condoleezza Rice.[127] China, holding an estimated $1.6 trillion of U.S. securities,[128] is the largest foreign financier of the record U.S. public debt.[129]

Bahia was the lead ship of a two-vessel class of scout cruisers built for Brazil by Armstrong Whitworth in the United Kingdom. Six months after her commissioning (May 1910), crewmen aboard Bahia, Deodoro, Minas Geraes, and São Paulo mutinied, beginning the Revolta da Chibata  (Revolt of the Whip). During the four-day rebellion, Brazil's capital city of Rio de Janeiro was held hostage by the possibility of a naval bombardment, leading the government to give in to the rebel demands, which included the abolition of flogging  in the navy. During the First World War, Bahia and her sister ship Rio Grande do Sul were assigned to the Divisão Naval em Operações de Guerra  (Naval Division in War Operations), the Brazilian Navy's main contribution in that conflict. Based out of Sierra Leone and Dakar, the squadron escorted convoys through an area believed to be heavily patrolled by U-boats.

In the mid-1920s, Bahia was extensively modernized. She received three new Brown–Curtis turbine engines and six new Thornycroft boilers, and, in the process, was converted from coal-burning to oil. The refit resulted in a striking aesthetic change, with the exhaust being trunked into three funnels instead of two. The armament was also modified; three 20.1 mm (0.79 in) Madsen guns, a 7 mm (0.28 in) Hotchkiss machine gun, and four 533 mm (21.0 in) torpedo tubes were added. In the 1930s, she served with government forces during multiple revolutions.

In the Second World War, Bahia was once again used as a convoy escort, sailing over 100,000 nautical miles (190,000 km; 120,000 mi) in the span of about a year. On 4 July 1945 she was acting as a plane guard for transport aircraft flying from the Atlantic to Pacific theaters of war. While Bahia's gunners were firing at a kite for anti-aircraft practice, one aimed too low and hit depth charges stored near the stern of the ship, resulting in a massive explosion that incapacitated the ship and sank her within minutes. Only a small portion of the crew survived the blast, and even fewer were still living when their rafts were discovered days later.
Contents
[hide]

    * 1 Construction and commissioning
    * 2 Mutiny
    * 3 First World War
    * 4 Modernization and inter-war years
    * 5 Second World War
          o 5.1 Loss
    * 6 See also
    * 7 Notes
    * 8 References
    * 9 Bibliography

[edit] Construction and commissioning

Bahia was part of a large 1904 naval building program by Brazil.[6][7] Also planned as part of this were the two Minas Geraes-class dreadnoughts, ten Pará-class destroyers, three submarines and a submarine tender.[7][8] With a design that borrowed heavily from the British Adventure-class scout cruisers,[1] Bahia's keel was laid on 19 August 1907 in Armstrong Whitworth's Elswick, Newcastle upon Tyne yard.[1][2] Construction took about a year and a half, and she was launched on 20 January 1909.[2][6][N 1] The process of fitting out pushed her completion date to 2 March 1910,[2] after which she sailed to Brazil, arriving in Recife on 6 May.[3] The new cruiser—the third ship of the Brazilian Navy to honor the state of Bahia[3][6]—was commissioned into the navy shortly thereafter on 21 May 1910.[3] As a class, Bahia and Rio Grande do Sul were the fastest cruisers in the world when they were commissioned,[1] and the first in the Brazilian Navy to utilize steam turbines for propulsion.[3]
[edit] Mutiny

Brazil's economy was suffering from a severe recession at the same time Bahia was commissioned.[10] This economic hardship, along with the racism prevalent in all branches of the Brazilian armed forces, and the severe discipline enforced on all navy ships, spawned a mutiny known as the Revolta da Chibata (Revolt of the Whip) among sailors on the most powerful ships.[10][11]

Unhappy with the violent treatment they were receiving, black sailors on the dreadnought battleship Minas Geraes began planning an uprising early in 1910, and chose João Cândido Felisberto—an experienced sailor later known as the "Black Admiral"—as leader.[10] In mid-November, a sailor was sentenced to be flogged 250 times in front of his fellow sailors, even though the practice had been banned by law.[10][11][12] The punishment was administered and continued even after the sailor fainted,[10] infuriating the nascent mutineers. Although they were not ready and could not revolt immediately, they quickened their preparations and rebelled on 21–22 November, earlier than originally planned.[11] They killed several officers and the captain of Minas Geraes, while other officers were forced off the ship.[11] The revolt quickly expanded to the battleship São Paulo, the elderly coastal defense ship Deodoro, and Bahia.[11] While joining the revolt, the crew of the scout cruiser murdered one of their officers.[6] During this time, discipline on the rebelling ships was not relaxed; daily drills were conducted and Felisberto ordered all liquor to be thrown overboard.[11]
Many men standing, smiling for the camera; most are dressed in white sailor uniforms, but a few in the center are clad in black suits with ties
João Cândido Felisberto with reporters, officers and sailors on aboard Minas Geraes on 26 November 1910, the final day of the rebellion

The crews of the torpedo boats remained loyal to the government,[11] and army troops moved to the presidential palace and the coastline, but neither group could stop the mutineers.[10] The fact that many who manned Rio de Janeiro's harbor defenses were sympathetic to the mutineers' cause,[11] coupled with chance that the capital might be bombarded by the mutinous ships, forced the National Congress of Brazil to give in to the rebels' demands.[10] These included the abolition of flogging, improved living conditions, and the granting of amnesty to all mutineers.[10][11] The government also issued official pardons and a statement of regret; its submission resulted in the rebellion's end on 26 November, when control of the four ships was handed back to the navy.[10]
[edit] First World War
See also: Brazil during World War I

In the opening years of the First World War, the Brazilian Navy was sent out to patrol the South Atlantic with French, British and American naval units, although its ships were not supposed to engage any threat outside territorial waters as Brazil was not at war with the Central Powers.[13] The country also tried to ensure that it remained totally neutral; Bahia and Rio Grande do Sul were sent to Santos in August 1914 when it was reported that the German raider Dresden was lying in wait off that port for British and American merchant ships.[14][N 2] Brazil joined the Entente and declared war on the Central Powers on 26 October 1917.[3][13]

On 21 December 1917, the Brazilian Navy—at the behest of the British—formed a small naval force with the intent of sending it to the other side of the Atlantic.[15] On 30 January 1918, Bahia was made the flagship of the newly organized Divisão Naval em Operações de Guerra (Naval Division in War Operations, abbreviated as DNOG), under the command of Rear Admiral Pedro Max Fernando Frontin.[3][6] The other ships assigned to the squadron were Bahia's sister Rio Grande do Sul, Pará-class destroyers Piauí, Paraíba, Rio Grande do Norte and Santa Catarina, tender Belmonte, and tugboat Laurindo Pita.[3][6][7][15]

The DNOG sailed for the British colony of Sierra Leone on 31 July. Since other allied countries helped with logistics, little was provided by Brazil aside from the ships themselves and the men crewing them.[15] Despite the threat of a U-boat attack, they were forced to stop several times so Belmonte could transfer necessities such as coal and water to the other ships.[15] They reached Freetown safely on 9 August and remained in the port until 23 August when they departed for Dakar.[15] While on this section of the voyage, Bahia, Rio Grande do Sul, Rio Grande do Norte, Belmonte and Laurindo Pita spotted an apparent torpedo heading for Belmonte, but it missed. Rio Grande do Norte then fired several shots and depth-charged what the force believed to be a U-boat.[3][16] While the official Brazilian history of the ship definitively claims to have sunk a submarine,[3] author Robert Scheina notes that this action was never confirmed,[16] and works published about U-boat losses in the war do not agree.[17]

After arriving in Dakar on 26 August, the DNOG was tasked with patrolling a triangle with corners at Dakar, Cape Verde and Gibraltar; the Allies believed that this area was rife with U-boats waiting for convoys to pass through.[16] As such, the Brazilian unit's mission was to patrol for mines laid by German minelaying submarines and to make sure that convoys passing through would be safe.[16] Complications arose when both Bahia and Rio Grande do Sul had problems with their condensers, a matter which was made much worse by the hot, tropical climate in which the ships were serving.[16]

In early September, the squadron was struck by the Spanish flu pandemic.[16] The contagion began aboard Bahia, spread to the other ships of the squadron and remained present for seven weeks.[16] At one point, 95% of some of the ships' crews were infected; 103 died overseas, and 250 died in Brazil after returning there.[16] On 3 November, Bahia, three of the four destroyers, and the tugboat were sent to Gibraltar for operations in the Mediterranean Sea.[16] They arrived on 9 or 10 November,[6][16][18][N 3] escorted by the American destroyer Israel,[6][18] but the fighting ceased on the 11th when the Armistice with Germany was signed.[16] Sometime in early 1919, Bahia, accompanied by four destroyers, voyaged to Portsmouth, England; they then traveled across the English Channel to Cherbourg, arriving there on 15 February.[19] The commander of the squadron, Admiral Pedro Max Fernando Frontin, met with the Maritime Prefect prior to the commencement of "social events"; these lasted until 23 February, when the ships moved to Toulon and Frontin journeyed overland to Paris.[19] The DNOG was dissolved on 25 August 1919.[3]
[edit] Modernization and inter-war years
A ship with a tall mast and low superstructure apparently at rest
Bahia sometime after her major modernization; the addition of a funnel was a striking change to the ship's appearance

In 1925–26,[1][6][N 4] Bahia underwent significant modernization.[3] The original five turbines were replaced by three Brown–Curtis turbines, while the original ten boilers were replaced by six Thornycroft oil-burning boilers, which necessitated the addition of a third funnel. The former coal bunkers, along with some of the space freed up by the decrease in boilers, were converted to hold 588,120 litres (155,360 US gal) of oil.[3] These modifications resulted in Bahia's top speed increasing to 28 knots (52 km/h).[1] All of the boats onboard were replaced, and three 20.1 mm (0.79 in) Madsen guns, a 7 mm (0.28 in) Hotchkiss machine gun, and four 533 mm (21.0 in) torpedo tubes were added to give the ship a defense against aircraft and more power against surface ships, respectively.[3] Still, in 1930 The New York Times labeled Bahia and the other warships in Brazil's navy as "obsolete" and noted that nearly all were "older than the ages considered effective by powers signatory to the Washington and London Naval Treaties."[22]

On 28 June 1926, the Ludington Daily News reported that Bahia would pay a visit to Philadelphia, accepting an invitation from the United States government to participate in the sesquicentennial celebrations.[21][N 5] In mid-1930, Bahia and Rio Grande do Sul—under the command of Heráclito Belford Gomes—escorted Brazil's President-elect Júlio Prestes to the United States.[23] Traveling onboard the Brazilian-Lloyd ocean liner Almirante Jacequay, Prestes was returning American then-President-elect Herbert Hoover's visit to Brazil in December 1928.[23][24] The cruisers USS Trenton and Marblehead met the three ships about 100 miles (160 km) off of Sandy Hook and honored Prestes with a 21-gun salute.[25][26] After spending five hours in the Ambrose Channel due to fog, Prestes traveled on a launch to a pier, during which Bahia rendered one 21-gun salute and Fort Jay offered two.[26] After arriving ashore, he traveled to City Hall before speeding down to Washington, D.C.[26] He stayed in the United States for eight days before departing for France on the White Star Line's Olympic.[27] Bahia and Rio Grande do Sul were berthed at the Brooklyn Navy Yard for the visit.[26]

During the Brazilian Revolution of 1930, Bahia served with Rio Grande do Sul—until that ship defected—and five or six destroyers off the coast of Santa Catarina; they were once again commanded by Belford Gomes.[3][28][N 6] Two years later, when the state of São Paulo rebelled in the Constitutionalist Revolution, Bahia—under the command of Frigate Captain Lucas Alexandre Boiteux—and other vessels blockaded the rebel-held port of Santos.[3][6] Bahia was under repair from 1934 into 1935.[6] In November 1935, Bahia and Rio Grande do Sul sailed to Natal, the capital of Rio Grande do Norte, to lend support against another rebellion.[30][31] As part of their mission, they were ordered to sink the steamship Santos on sight, as several escaping leaders of the revolution were embarked.[32]

From 17–22 May 1935,[33][34] Bahia and Rio Grande do Sul[N 7]—joined at an unknown point by the Argentine battleships Rivadavia and Moreno, the heavy cruisers Almirante Brown and Veinticinco de Mayo, and five destroyers[34]—escorted São Paulo, with Brazilian President Getúlio Dornelles Vargas embarked, up the Río de la Plata (River Plate) to Buenos Aires, the capital of Argentina.[6][33][34][35] Vargas was returning visits from the presidents of Argentina and Uruguay, Agustín Pedro Justo and Gabriel Terra.[6][33] Vargas and Justo planned to be present at the opening session of the Pan-American Commercial Conference on 26 May,[33] and open a Chaco War peace conference,[33][35] before São Paulo conveyed Vargas to Montevideo, Uruguay for meetings with Terra.[35]

On 2 March 1936, Bahia escorted Veinticinco de Mayo, which had the Argentine Navy Minister Rear Admiral Eleazar Videla embarked, and Almirante Brown in the last part of their journey to Rio de Janeiro.[36]
[edit] Second World War

After Brazil's entrance into the Second World War on 21 August 1942, which took effect on 31 August,[37] Bahia was used extensively for escorts and patrols; sources conflict as to the actual number—either 67 and 15[3] or 62 and 11.[20] In total, she traveled 101,971 nmi (188,850 km; 117,346 mi) in 358 days, and played a role in shepherding over 700 merchant ships,[3] despite being labeled by the United States Naval Institute's magazine Proceedings as being an "oversized destroyer" which was "relatively slow".[38]

On 3 June 1943, while Bahia was escorting the convoy BT 12, she located an underwater mine and destroyed it with one of her 20 mm (0.79 in)[clarification needed] Madsen guns.[3] On 10 July, while at 26°15′S 43°35′W﻿ / ﻿26.25°S 43.583°W﻿ / -26.25; -43.583, Bahia received a sonar contact and depth-charged what the Brazilian Navy's official history of the ship reports might have been the German submarine U-199, which was sunk later that month in the same area (off Rio de Janeiro) by American and Brazilian aircraft.[3][39] In November 1944, Bahia joined the American light cruiser Omaha and destroyer escort Gustafson in accompanying the 4th Squadron of the Brazilian Expeditionary Force as they were carried on the troop transport General M. C. Meigs to Italy.[3]

Bahia was modernized again twice during the war, in both 1942 and 1944; these modernizations were not as extensive as those of the 1920s. Two of her 47 mm (1.9 in) guns were replaced with 76 mm (3.0 in) L/23 AA guns, her Madsen guns were replaced with seven Oerlikon 20 mm cannons in single mounts, and a director for these guns was installed.[3] Two depth charge tracks were added, improved range-finders were added to the 120 mm (4.7 in) guns, and sonar and radar were fitted, in addition to other minor modifications.[3][N 8] The Brazilian Navy's official history of the ship reports these modifications, but does not specify which were undertaken in which year.[3]
[edit] Loss
A warship at sea under a bright sky, possibly underway
A profile of Bahia at some point after her 1920s modernization; note the men congregated on the foredeck

Various warships of the Allied nations, including Brazil's, were assigned to patrol in the Atlantic as rescue ships; they would lie in wait near routes frequented by military transport aircraft that were carrying personnel from the recently ended European theater to the continuing war in the Pacific.[6][40] Bahia was one such ship;[40] on 4 July 1945, she was stationed northeast of Brazil around 0°N 30°W﻿ / ﻿0°N 30°W﻿ / 0; -30, near the Arquipélago de São Pedro e São Paulo (Saint Peter and Saint Paul Archipelago).[3][4][6] For anti-aircraft target practice, crewmen were firing the ship's 20 mm guns at a kite that was being towed behind the ship. One of them shot it down, but also accidentally hit the depth charges on the stern—a direct consequence of the lack of guide rails that would normally prohibit the guns from being aimed at the ship.[4][6] The resulting explosion knocked out all power on the ship and sank her in about three minutes.[1][4][41]

The survivors of the blast endured four or five days of no food, high temperatures and full exposure to the sun on their makeshift rafts;[41][42] some, driven mad by these conditions, simply jumped into the water and were devoured by sharks.[42] From this point on, sources vary greatly. According to an article in Time, Bahia's loss was not discovered until 8 July, when 22 survivors were picked up by a freighter, Balfe.[41][43][N 9] Author Robert Scheina, however, says that the disaster was revealed when Rio Grande do Sul arrived on station four days after the sinking to take Bahia's place and could not find her.[4]

Sources also disagree on the number rescued and final death toll. The official history of the ship gives 36 rescued and 336 dead,[3] Poder Naval Online gives 36 and 339,[6][N 10] and Conway's All The World's Fighting Ships 1906–1921 gives no number for survivors but 294 for deaths.[1] Contemporary news articles also published varying numbers; in an article published a day after the accident became known, The Evening Independent stated that the ship carried 383 men, though it did not give any more information.[44] The New York Times gave figures of 28 saved and 347 lost,[42] while the St. Petersburg Times gave 32 and 395.[45] Sources do agree, however, that four American radiomen were killed.[3][6][46]

Rescued crewmen believed that they had hit a mine that detonated one of the ship's magazines.[41] Vice Admiral Jorge Dodsworth Martins—Brazil's chief of naval intelligence—thought that Bahia could have been mined or torpedoed by U-530,[42][47] which surrendered under strange circumstances in Argentina on 10 July (some two months after Germany's surrender), but the Argentine Naval Ministry stated that it would have been impossible for the submarine to travel from the site of the sinking to Mar del Plata in six days (4–10 July).[47][N 11] By late October, the Brazilian Navy had come to the conclusion that Bahia had been sunk by an accidental explosion.[46][49]
[edit] See also

    * USS Indianapolis (CA-35), an American heavy cruiser, also sunk in July 1945, whose survivors endured circumstances similar to Bahia's

[edit] Notes

   1. ^ The Miramar Ship Index—using information from contemporary builders' records—and Poder Naval Online record Bahia's launching date as 20 January 1909, and her sister ship Rio Grande do Sul's as 20 April 1909. Conway's All the World's Fighting Ships 1906–1921 and the Brazilian Navy's official history reverse these dates, giving 20 January for Rio Grande do Sul and 20 April for Bahia. This article uses the former date because of Miramar's use of builders' records, which, in this case, should be the most accurate source.[1][2][3][9]
   2. ^ The New York Times' article refers to Bremen, but that ship was in the Baltic Sea at the time. The only German cruiser in that area in August 1914 was Dresden. The misidentification was probably due to the fog of war.
   3. ^ Sources give different dates; Poder Naval Online and Israel's Dictionary of American Naval Fighting Ships entry give 9 November, while Scheina gives 10 November.[6][16][18]
   4. ^ The official history of the ship gives a 1924–1927 range,[3] while Scheina in Conway's, Poder Naval Online, and Whitley give 1925–1926.[1][6][20] Additional collaborating evidence for the latter date can be found in a June 1926 Ludington Daily News article which reported that Bahia was going to visit the United States—implying that the ship had been placed back into service.[21]
   5. ^ There appears to have been no follow-up article on what occurred after Bahia arrived.
   6. ^ Rio Grande do Sul defected at an unknown date,[28] and Bahia may have as well; on 6 October, a rebel general claimed that both ships had defected.[29]
   7. ^ The New York Times remarks that the two Brazilian cruisers "were bound for Ilha Grande and [w]inter manoeuvres", but it is not clear from other sources if they actually did this.[33]
   8. ^ Regarding the installation of sonar, it is not clear whether it was fitted for the first time in 1942 (and used in the 10 July depth charging) or whether a more modern sonar replaced an outmoded version in either 1942 or 1944.
   9. ^ The magazine also reports that additional survivors were rescued over the next few days, but does not give a definitive figure. Poder Naval Online, however, states that a total of 36 survivors were rescued by Balfe on the 8th.[6][41]
  10. ^ These figures contradict other information present in the article, however. Poder first says that 339 of 372 total crewmembers died, meaning that 33 survived, but the subsequent sentence states directly that 36 survived.[6]
  11. ^ Rumors persist today that either U-530 or U-977 sank Bahia
  
  Lead ship
From Wikipedia, the free encyclopedia
Jump to:navigation, search

The lead ship or class leader is the first of a series or class of ships all constructed according to the same general design. Almost always, this is only applicable for military ships and larger civilian craft.
[edit] Overview

Large ships are complicated internally and may take as much as five to ten years to construct. Any changes or advances that are available when building a ship are likely to be included, so it is rare to have two that are completely identical. Constructing one ship is also likely to reveal better ways of doing things, and even errors. For example, when the United States built the USS Ohio as the lead ship of her class of submarines, they found that the access hatches were not large enough to load some internal components only after construction was nearly complete.[citation needed]

The second and later ships are often started before the first one is completed, launched and tested. Nevertheless, building copies is still more efficient and cost-effective than building prototypes, and the lead ship will usually be followed by copies with some improvements rather than radically different versions. The improvements will sometimes be retrofitted to the lead ship. Occasionally, the lead ship will be launched and commissioned for shakedown testing before following ships are completed, making the lead ship a combination of template and prototype, rather than expending resources on a prototype that will never see actual use.
[edit] Naming

Ship classes are typically named in one of two ways; echoing the name of the lead ship, such as the Pennsylvania-class battleships, whose lead ship was USS Pennsylvania, or defining a theme by which vessels in the class are named, as in the Royal Navy's Tribal-class frigates, named after tribes of the world, such as HMS Mohawk. If a ship class is produced for another fleet, the first active unit will become the lead ship for that fleet; for example, the Oliver Hazard Perry-class frigates are known as the Adelaide-class in the Royal Australian Navy. Larger civilian craft, such as the Sun Princess, the lead ship of the Sun-class cruise ships sometimes follow this convention as well.

The same custom is often followed in fiction: the Constitution-class cruiser is the basis for the Enterprise of Star Trek (though it should be noted that in Star Trek the term pathfinder is also occasionally used in lieu of lead ship) and the Imperial-class Star Destroyer appears in Star Wars.
Cruiser
From Wikipedia, the free encyclopedia
Jump to:navigation, search
For other uses, see Cruiser (disambiguation).
Question book-new.svg
	This article needs additional citations for verification.
Please help improve this article by adding reliable references. Unsourced material may be challenged and removed. (November 2007)
USS Port Royal (CG-73), a Ticonderoga-class guided missile cruiser, launched in 1992.

Cruiser is a type of large warship, which had its prime period from the late 19th century to the end of the Cold War. The first cruisers were intended for individual raiding and protection missions on the seas. Over the years, the nature and role of the cruiser has changed considerably, and today the cruiser has largely been replaced by destroyers in its roles.

Historically, a 'cruiser' was not a type of ship but a warship role. Cruisers were ships—often frigates or smaller vessels—that were assigned a role largely independent from the fleet; in a sense, cruising independently. Typically, this might involve missions such as raiding enemy merchant shipping. In the late 19th century, the term 'cruiser' came to mean ships designed to fulfill such a role, and from the 1890s to the 1950s a 'cruiser' was a warship larger than a destroyer but smaller than a battleship.

For much of 19th century and the first half of the 20th century, cruisers were a navy's long-range 'force projection' weapons, while the larger ships stayed nearer home. Their main role was to attack enemy merchant vessels, so much so that this task came to be called cruiser warfare. Other roles included reconnaissance, and cruisers were often attached to the battle fleet. In the later 20th century, the decline of the battleship left the cruiser as the largest and most powerful surface combatant.

The role of the cruiser, however, increasingly became one of providing air defence for a fleet, rather than independent cruiser warfare. At the beginning of the 21st century, cruisers were the heaviest surface combatant ships in use, with only five nations (the United States, Russia, France, Italy, and Peru) operating them. Following the Italian Navy's 2003 decommissioning of Vittorio Veneto (550), only four nations currently operate cruisers.
Contents
[hide]

    * 1 Early history
    * 2 Steam cruisers
    * 3 Steel cruisers
          o 3.1 Torpedo cruisers
          o 3.2 Pre-dreadnought armored cruisers
    * 4 Cruisers from 1900 to 1914
          o 4.1 Battlecruisers
          o 4.2 Light cruisers
          o 4.3 Flotilla leaders
          o 4.4 Auxiliary cruisers
    * 5 World War I
    * 6 Cruisers from 1919-1945
          o 6.1 Heavy cruisers
          o 6.2 The German pocket battleships
          o 6.3 Anti-aircraft cruisers
    * 7 Later 20th century
          o 7.1 Aircraft cruisers
    * 8 Cruisers in service today
    * 9 The US Navy's "cruiser gap"
    * 10 See also
    * 11 References

[edit] Early history

The term "cruiser" or "cruizer"[1] was first commonly used in the 17th century to refer to an independent warship. "Cruiser" meant the purpose or mission of a ship, rather than a category of vessel. However, the term was nonetheless used to mean a smaller, faster warship suitable for such a role. In the 17th century, the ship of the line was generally too large, inflexible, and expensive to be dispatched on long-range missions (for instance, to the Americas), and too strategically important to be put at risk of fouling and foundering by continual patrol duties.

The Dutch navy was noted for its cruisers in the 17th century, while the Royal Navy—and later French and Spanish navies—subsequently caught up in terms of their numbers and deployment. The British Cruiser and Convoy Acts were an attempt by mercantile interests in Parliament to focus the Navy on commerce defence and raiding with cruisers, rather than the more scarce and expensive ships of the line.[2] During the 18th century the frigate became the preeminent type of cruiser. A frigate was a small, fast, long range, lightly armed (single gun-deck) ship used for scouting, carrying dispatches, and disrupting enemy trade. The other principal type of cruiser was the sloop, but many other miscellaneous types of ship were used as well.
[edit] Steam cruisers

During the 19th century, as steam propulsion became the norm, fleets started to use the term 'cruiser' more descriptively to refer to some ironclad warships as well as a miscellany of unarmored frigates, sloops, and corvettes, most of which had mixed steam and sail propulsion.

The first ironclads were, because of their single gun decks, still referred to as "frigates", even though they were more powerful than existing ships of the line. The French constructed a number of smaller ironclads for overseas cruising duties, starting with the Belliqueuse, commissioned 1865. These were the first armored cruisers.

By the 1870s, many other nations had produced ironclads specifically for fast, independent, raiding and patrol. These vessels were referred to as armored cruisers. Until the 1890s armored cruisers were still built with masts for a full sailing rig, to enable them to operate far from friendly coaling stations.[3]

Unarmored cruising warships, built out of wood, iron, steel or a combination of those materials, remained popular until towards the end of the 19th century. The ironclad's armor often meant that they were limited to short range under steam, and many ironclads were unsuited to long-range missions or for work in distant colonies. The unarmored cruiser - often a screw sloop or screw frigate - could continue in this role. Even though mid- or late-19th century cruisers typically carried up-to-date guns firing explosive shells, they were unable to face ironclads in combat. This was evidenced by the clash between HMS Shah, a modern British cruiser, and the Peruvian monitor Huáscar. Even though the Peruvian vessel was obsolescent by the time of the encounter, it stood up well to roughly 50 hits from British shells.
[edit] Steel cruisers
Main article: Protected cruiser
The Russian protected cruiser Aurora

In the 1880s naval architects began to use steel as a material for construction and armament. A steel cruiser could be lighter and faster than one built of iron or wood. The Jeune Ecole school of naval doctrine suggested that a fleet of fast unprotected steel cruisers were ideal for commerce raiding, while the torpedo boat would be able to destroy an enemy battleship fleet.

Steel also offered the cruiser a way of acquiring the protection needed to survive in combat. Steel armor was considerably stronger, for the same weight, than iron. By putting a relatively thin layer of steel armor above the vital parts of the ship, and by placing the coal bunkers where they might stop shellfire, a useful degree of protection could be achieved without slowing the ship too much.

The first protected cruiser was the groundbreaking Chilean ship Esmeralda. Produced by a shipyard at Elswick, in Britain, owned by Armstrong, she inspired a group of protected cruisers produced in the same yard and known as the "Elswick cruisers". Her forecastle, poop deck and the wooden board deck had been removed, replaced with an armored deck. Esmeralda's armament consisted of fore and aft 10-inch (25.4 cm) guns and 6-inch (15.2 cm) guns in the midships positions. It could reach a speed of 18 knots (33 km/h), and was propelled by steam alone. It also had a displacement of less than 3,000 tons. During the two following decades, this cruiser type came to be the inspiration for combining heavy artillery, high speed and low displacement.
[edit] Torpedo cruisers
Main article: Torpedo gunboat

The torpedo cruiser (known in the Royal Navy as the torpedo gunboat) was a smaller unarmored cruiser, which emerged in the 1880s-1890s. These ships could reach speeds up to 20 knots (37 km/h) and were armed with medium to small calibre guns as well as torpedoes. These ships were tasked with guard and reconnaissance duties, to repeat signals and all other fleet duties for which smaller vessels were suited. These ships could also function as flagships of torpedo boat flotillas. After the 1900s, these ships were usually traded for faster ships with better sea going qualities.
[edit] Pre-dreadnought armored cruisers

Steel also had an impact on the construction and role of armored cruisers. Steel meant that new designs of battleship, later known as pre-dreadnought battleships, would be able to combine firepower and armor with better endurance and speed than ever before. The armored cruisers of the 1890s greatly resembled the battleships of the day; they tended to carry slightly smaller main armament (9.2-inch (230 mm) rather than 12-inch) and have somewhat thinner armor in exchange for a faster speed (perhaps 21 knots (39 km/h) rather than 18). Because of their similarity, the lines between battleships and armored cruisers became blurred.
[edit] Cruisers from 1900 to 1914
The German light cruiser SMS Emden, launched in 1908.

Shortly after the turn of the 20th century there were difficult questions about the design of future cruisers. Modern armored cruisers, almost as powerful as battleships, were also fast enough to outrun older protected and unarmored cruisers. In the Royal Navy, Jackie Fisher cut back hugely on older vessels, including many cruisers of different sorts, calling them 'a miser's hoard of useless junk' that any modern cruiser would sweep from the seas.
[edit] Battlecruisers
Main article: Battlecruiser
HMS Repulse in 1919

The growing size and power of the armored cruiser resulted in the battlecruiser, larger than the armored cruiser with an armament similar to the revolutionary new dreadnought battleship, was the brainchild of British admiral Jackie Fisher. He believed that to ensure British naval dominance in its overseas colonial possessions, a fleet of large, fast, powerfully-armed vessels which would be able to hunt down and mop up enemy cruisers and armored cruisers with overwhelming fire superiority was needed. These vessels came to be known as the battlecruiser, and the first were commissioned into the Royal Navy in 1907. In spite of Fisher's lobbying, the concept never came to dominate naval warfare. However, Britain, Germany and eventually Japan all came to build squadrons of battlecruisers.
[edit] Light cruisers
HMS Caroline, a World War I era light cruiser, still serves as a headquarters and training vessel in Belfast.
Main article: Light cruiser

At around the same time as the battlecruiser was developed, the distinction between the armored and the unarmored cruiser finally disappeared. By the British Town-class cruiser (1910), it was possible for a small, fast cruiser to carry both belt and deck armor, particularly when turbine engines were adopted. These 'light armored cruisers' began to occupy the traditional cruiser role once it became clear that the battlecruiser squadrons were required to operate with the battle fleet.
[edit] Flotilla leaders
Main article: Flotilla leader

Some light cruisers were built specifically to act as the leaders of flotillas of destroyers.
[edit] Auxiliary cruisers
Main article: Auxiliary cruiser

The auxiliary cruiser was a merchant ship hastily armed with small guns on the outbreak of war. Auxiliary cruisers were used to fill gaps in their long-range lines or provide escort for other cargo ships, although they generally proved to be useless in this role because of their low speed, feeble firepower and lack of armor. In both world wars the Germans also used small merchant ships armed with cruiser guns to surprise Allied merchant ships. Some large liners were armed in the same way. In British service these were known as Armed Merchant Cruisers (AMC). The Germans and French used them in World War I as raiders because of their high speed (around 30 knots (56 km/h)), and they were used again as raiders in World War II by the Germans and Japanese. In both the First World War and in the early part of the Second, they were used as convoy escorts by the British.
[edit] World War I
Main article: Naval warfare of World War I

Cruisers were one of the workhorse types of ship of World War I.
[edit] Cruisers from 1919-1945

Naval construction in the 1920s and 1930s was limited by international treaties designed to prevent the repetition of the Dreadnought arms race of the early 20th century. The Washington Naval Treaty of 1922 placed limits on the construction of ships with a displacement of 10,000 tons or more and an armament of greater than 8-inch (200 mm) calibre. A number of navies commissioned classes of cruisers at the top end of this limit. The London Naval Treaty in 1930 then formalised the distinction between these 'heavy' cruisers and light cruisers: a 'heavy' cruiser was one with guns of 6.1-inch (150 mm) calibre or more. The Second London Naval Treaty attempted to reduce the tonnage of new cruisers to 8,000 or less, but this had little impact; Japan and Germany were not signatories, and navies had already begun to evade treaty limitations on warships.
[edit] Heavy cruisers
See also: Heavy cruisers

The heavy cruiser was a type of cruiser, a naval warship designed for long range, high speed and an armament of naval guns roughly 8-inch (200 mm) in calibre. The first heavy cruisers were built in 1915, although it only became a widespread classification following the Washington Naval Treaty in 1922. The heavy cruiser's immediate precursors were the light cruiser designs of the 1900s and 1910s. Heavy cruisers continued in use until after World War II.
[edit] The German pocket battleships

The German Deutschland-class was a series of three Panzerschiffe ("armored ships"), a form of heavily armed cruiser, built by the German Reichsmarine in accordance with restrictions imposed by the Treaty of Versailles. The class is named after the first ship of this class to be completed (the Deutschland). All three ships were launched between 1931 and 1934, and served with Germany's Kriegsmarine during World War II.

The British began referring to the vessels as pocket battleships, in reference to the heavy firepower contained in the relatively small vessels; they were considerably smaller than battleships and battlecruisers, and although their displacement was that of a heavy cruiser, they were armed with guns larger than the heavy cruisers of other nations. Deutschland-class ships continue to be called pocket battleships in some circles. The ships were actually two feet longer than the American Pennsylvania-class - although the latter was unusually stubby for a modern battleship.

Deutschland-class ships were initially classified as Panzerschiffe, but the Kriegsmarine reclassified them as heavy cruisers in February 1940.
[edit] Anti-aircraft cruisers
USS Atlanta (CL-51)

The development of the anti-aircraft cruiser began in 1935 when the Royal Navy re-armed HMS Coventry and HMS Curlew. Torpedo tubes and 6-inch (15 cm) low-angle guns were removed from these WWI light cruisers and replaced by ten 4-inch (10 cm) high-angle guns with appropriate fire-control equipment to provide larger warships with protection against high-altitude bombers.[4]

A tactical shortcoming was recognized after completing six additional conversions of C-class cruisers. Having sacrificed anti-ship weapons for anti-aircraft armament, the converted anti-aircraft cruisers might need protection themselves against surface units. New construction was undertaken to create cruisers of similar speed and displacement with dual-purpose guns. Dual-purpose guns offered good anti-aircraft protection with anti-surface capability for the traditional light cruiser role of defending capital ships from destroyers. The first purpose built anti-aircraft cruiser was the British Dido-class, completed shortly before the beginning of WWII. The US Navy Atlanta-class anti-aircraft cruisers (CLAA) were designed to match capabilities of the Royal Navy. Both Dido and Atlanta carried torpedo tubes.

The quick-firing dual-purpose gun anti-aircraft cruiser concept was embraced in several designs completed too late to see combat including USS Worcester (CL-144) and USS Roanoke (CL-145) completed in 1948 and 1949, two De Zeven Provinciën-class cruisers completed in 1953, De Grasse and Colbert completed in 1955 and 1959, and HMS Tiger, HMS Lion and HMS Blake completed between 1959 and 1961.[5]

Most post-WWII cruisers were tasked with air defense roles. In the early 1950s, advances in aviation technology forced the move from anti-aircraft artillery to anti-aircraft missiles. Therefore most cruisers of today are equipped with surface-to-air missiles as their main armament. The modern equivalent of the anti-aircraft cruiser is the guided missile cruiser (CAG/CLG/CG/CGN).
Russian Navy cruiser of the Kirov-class, Frunze.
The Ticonderoga-class cruiser USS Cape St. George (CG-71), firing a Tomahawk missile.
[edit] Later 20th century

The rise of air power during World War II dramatically changed the nature of naval combat. Even the fastest cruisers could not steer quickly enough to evade aerial attack, and aircraft now had torpedoes, allowing moderate-range standoff capabilities. This change led to the end of independent operations by single ships or very small task groups, and for the second half of the 20th century naval operations were based on very large fleets able to fend off all but the largest air attacks.

This has led most navies to change to fleets designed around ships dedicated to a single role, anti-submarine or anti-aircraft typically, and the large "generalist" ship has disappeared from most forces. The United States Navy, the Russian Navy, and the Peruvian Navy are the only remaining navies which operate cruisers. France operates a single cruiser, Jeanne d'Arc, which in the NATO pennant number system is classified as an aircraft carrier, but for training purposes only.

In the Soviet Navy, cruisers formed the basis of combat groups. In the immediate post-war era they built a fleet of large-gun ships, but replaced these fairly quickly with very large ships carrying huge numbers of guided missiles and anti aircraft missiles. The most recent ships of this type, the four Kirovs, were built in the 1970s and 1980s, and, with the exception of the two newest in the class, Pyotr Velikiy and Admiral Nakhimov, are no longer in service today. Russia also operates one Kara-class and four Slava-class cruisers, plus one Kuznetsov-class carrier which is officially designated as a cruiser.

The United States Navy has centered on the aircraft carrier since WWII. The Ticonderoga-class cruisers, built in the 1980s, were originally designed and designated as a class of destroyer, intended to provide a very powerful air-defense in these carrier-centered fleets. The ships were later redesignated largely as a public relations move, in order to highlight the capability of the Aegis combat system the ships were designed around.[citation needed] In the years since the launch of Ticonderoga in 1981 the class has received a number of upgrades that have dramatically improved their capabilities for anti-submarine and land attack (using the Tomahawk missile). Like their Soviet counterparts, the modern Ticonderogas can also be used as the basis for an entire battle group. Their cruiser designation was almost certainly deserved when first built, as their sensors and combat management systems enable them to act as 'flagships' for a surface warship flotilla if no carrier is present, but newer ships rated as destroyers and also equipped with AEGIS approach them very closely in capability, and once more blur the line between the two classes.
[edit] Aircraft cruisers
Main article: Aircraft cruiser

From time to time, some navies have experimented with aircraft-carrying cruisers. One example is the Swedish HMS Gotland. Another variant is the helicopter cruiser. The last example in service was the Soviet Navy's Kiev-class, the last unit of which has been converted to a pure aircraft carrier and sold to India. The Russian Navy's Kuznetsov is nominally designated as an aviation cruiser but otherwise resembles a standard medium aircraft carrier, albeit with an SSM battery. The Royal Navy's aircraft-carrying Invincible-class vessels were originally designated 'through-deck cruisers', but have since been designated as small aircraft carriers.
[edit] Cruisers in service today
Jeanne d'Arc of the French Navy, the only helicopter cruiser still in service, launched in 1961.

Few cruisers remain operational in the world navies. Those that do are:

    * United States Navy: 22[6] Ticonderoga-class guided missile cruisers.
    * Russian Navy: One Kirov-class large missile cruisers (sometimes referred to as battlecruisers due to their size) and three Slava-class missile cruisers and one Kara-class.
    * French Navy: The Jeanne d'Arc helicopter cruiser (now used as a training ship).
    * Peruvian Navy: The BAP Almirante Grau (CLM-81), a De Zeven Provinciën-class cruiser, was modernized as a guided-missile cruiser in the late eighties, is the world's last operational gun cruiser.

[edit] The US Navy's "cruiser gap"
Main article: United States Navy 1975 ship reclassification

Prior to the introduction of the Ticonderogas, the US Navy used odd naming conventions that left its fleet seemingly without many cruisers, although a number of their ships were cruisers in all but name. From the 1950s to the 1970s, US Navy "cruisers" were large vessels equipped with heavy offensive missiles (including the Regulus nuclear cruise missile) for wide-ranging combat against land-based and sea-based targets. All save one—USS Long Beach (CGN-9)—were converted from World War II Chicago-, Baltimore- and Cleveland-class cruisers. "Frigates" under this scheme were almost as large as the cruisers and optimized for anti-aircraft warfare, although they were capable anti-surface warfare combatants as well. In the late 1960s, the US government perceived a "cruiser gap"—at the time, the US Navy possessed six ships designated as "cruisers", compared to 19 for the Soviet Union, even though the USN possessed at the time 21 "frigates" with equal or superior capabilities to the Soviet cruisers—because of this, in 1975 the Navy performed a massive redesignation of its forces:

    * CVA/CVAN were redesignated CV/CVN (although USS Midway (CV-41) and USS Coral Sea (CV-43) never embarked anti-submarine squadrons).
    * DLG/DLGN (Frigate/Nuclear-powered Frigate) were redesignated CG/CGN (Guided Missile Cruiser/Nuclear-powered Guided Missile Cruiser).
    * Farragut-class guided missile frigates (DLG), being smaller and less capable than the others, were redesignated to DDGs (USS Coontz was the first ship of this class to be re-numbered; because of this the class is sometimes called the Coontz-class);
    * DE/DEG (Ocean Escort/Guided Missile Ocean Escort) were redesignated to FF/FFG (Guided Missile Frigates), bringing the US "Frigate" designation into line with the rest of the world.

Also, a series of Patrol Frigates of the Oliver Hazard Perry-class, originally designated PFG, were redesignated into the FFG line. The cruiser-destroyer-frigate realignment and the deletion of the Ocean Escort type brought the US Navy's ship designations into line with the rest of the world's, eliminating confusion with foreign navies. In 1980, the Navy's then-building DDG-47 class destroyers were redesignated as cruisers (CG-47 Ticonderoga-class guided missile cruiser) to emphasize the additional capability provided by the ships' Aegis combat systems.
Sierra Leone
From Wikipedia, the free encyclopedia
Jump to:navigation, search
Republic of Sierra Leone
Sierra Leone
	
Flag 	Coat of Arms
Motto: "Unity, Freedom, Justice"
Anthem: High We Exalt Thee, Realm of the Free
Capital
(and largest city) 	Freetown
8°29.067′N 13°14.067′W﻿ / ﻿8.48445°N 13.23445°W﻿ / 8.48445; -13.23445
Official language(s) 	English
National language 	Krio (de facto) spoken by 97% of the population[1][2]
Demonym 	Sierra Leonean
Government 	Constitutional republic
 -  	President 	Ernest Bai Koroma (APC)
 -  	Vice President 	Alhaji Samuel Sam-Sumana (APC)
 -  	Speaker of Parliament 	Abel Nathaniel Bankole Stronge (APC)
 -  	Chief Justice 	Umu Hawa Tejan Jalloh
Independence
 -  	from the United Kingdom 	April 27, 1961 
 -  	Republic declared 	April 19, 1971 
Area
 -  	Total 	71,740 km2 (119th)
27,699 sq mi 
 -  	Water (%) 	1.1
Population
 -  	July 2009 estimate 	6,440,053[2] 
 -  	Density 	79.4/km2 (114th1)
205.6/sq mi
GDP (PPP) 	2009 estimate
 -  	Total 	$4.585 billion[3] 
 -  	Per capita 	$759[3] 
GDP (nominal) 	2009 estimate
 -  	Total 	$1.877 billion[3] 
 -  	Per capita 	$311[3] 
Gini (2003) 	62.9 (high) 
HDI (2007) 	▲ 0.365 ( low) (180th)
Currency 	Leone (SLL)
Time zone 	GMT (UTC+0)
Drives on the 	right
Internet TLD 	.sl
Calling code 	232
1 Rank based on 2007 figures.

Sierra Leone (Loudspeaker.svg /siːˈɛrə liːˈoʊn/), officially the Republic of Sierra Leone, is a country in West Africa. It is bordered by Guinea in the north, Liberia in the southeast, and the Atlantic Ocean in the southwest. Sierra Leone covers a total area of 71,740 km2 (27,699 sq mi) [4] and has a population estimated at 6.4 million. The country is a constitutional republic comprising three provinces and the Western Area, which are further divided into fourteen districts.

The country has a tropical climate, with a diverse environment ranging from savannah to rainforests.[5] Freetown is the capital, largest city and economic center. The other major cities are Bo, Kenema, Koidu Town and Makeni.[4] English is the official language,[6] spoken at schools, government administration and by the media. However, the Krio language (a language derived from English and several African languages and native to the Sierra Leone Krio people) is the most widely spoken language in virtually all parts of the country. The Krio language is spoken by 97% of the country's population[2] and unites all the different ethnic groups, especially in their trade and interaction with each other.[1] Despite its common use throughout the country, the Krio language has no official status.

Sierra Leone is very rich in mineral resources, possessing most of the known mineral types of the world, many of which are found in significant quantities. The country has relied on mining, especially diamonds, for its economic base. The country is among the top 10 diamond producing nations in the world, and mineral exports remain the main foreign currency earner. Sierra Leone has the largest known titanium reserves in the world. The country is also one of the largest Bauxite producing nations on the planet. Sierra Leone is also a major producer of gold

Sierra Leone has one of the world's largest deposits of rutile, a titanium ore used as paint pigment and welding rod coatings. In September 2009, A United States oil firm Anadarko Petroleum Corporation announced the discovery of an "active petroleum deepwater oil well off the coast of Sierra Leone [1]. Despite being rich in mineral resources, the majority of its people live in poverty.

Sierra Leone is also home to the third largest natural harbour in the world, the Queen Elizabeth II Quay (also known as the QE II Quay and locally as the Deep Water Quay or Government Wharf)[7][8].

Early inhabitants of Sierra Leone included the Sherbro, Temne and Limba, and Tyra [disambiguation needed] peoples, and later the Mende,[9] who knew the country as Romarong, and the Kono who settled in the East of the country.[10] In 1462, it was visited by the Portuguese explorer Pedro da Cintra, who gave it its name Serra de Leão, meaning 'Lion Mountains'.[11][12] Sierra Leone later became an important centre of the transatlantic trade in slaves until 1792 when Freetown was founded by the Sierra Leone Company as a home for formerly enslaved African Americans.[13] In 1808, Freetown became a British Crown Colony, and in 1896, the interior of the country became a British Protectorate;[10] in 1961, the two combined and gained independence.

Over two decades of government neglect of the interior followed by the spilling over of the Liberian conflict into its borders eventually led to the Sierra Leone Civil War,[14] which began in 1991 and was resolved in 2000 after the struggling Nigerian led United Nations troops were heavily reinforced by a British force spearheaded by 42 Commando of the Royal Marines as well as several British Army units. The arrival of this force in what was codenamed Operation Palliser resulted in the defeat of rebel forces and restored the civilian government elected in 1998 to Freetown. Since then, almost 72,500 former combatants have been disarmed[15] and the country has reestablished a functioning democracy.[2] The Special Court for Sierra Leone was set up in 2002 to deal with war crimes and crimes against humanity committed since 1996.[16] Sierra Leone is the third-lowest-ranked country on the Human Development Index and seventh-lowest on the Human Poverty Index, suffering from endemic corruption[17] and suppression of the press.[18]
Contents
[hide]

    * 1 History
          o 1.1 Early History
          o 1.2 Enslavement and Freedom
          o 1.3 Colonial era
          o 1.4 An independent nation
          o 1.5 Multi-party constitution and Revolutionary United Front rebellion
    * 2 Geography and climate
          o 2.1 Climate
          o 2.2 Environment
    * 3 Government and politics
    * 4 Foreign relations
    * 5 Provinces and districts
          o 5.1 Major cities
    * 6 Economy
          o 6.1 Currency
    * 7 Demographics
    * 8 Religion
    * 9 Ethnic groups
    * 10 Education
    * 11 Health
    * 12 Military
    * 13 Law enforcement
    * 14 Media
    * 15 Music of Sierra Leone
    * 16 Transportation
          o 16.1 Air
                + 16.1.1 Prohibition from E.U. air operations
          o 16.2 Water
          o 16.3 Highways
    * 17 Sports
          o 17.1 Football
          o 17.2 Cricket
          o 17.3 Basketball
    * 18 See also
    * 19 Notes
    * 20 Book references
          o 20.1 Primary sources
          o 20.2 Secondary sources
    * 21 Further reading
    * 22 External links

[edit] History
Main article: History of Sierra Leone
[edit] Early History
Fragments of prehistoric pottery from Kamabai Rock Shelter

Archaeological finds show that Sierra Leone has been inhabited continuously for at least 2,500 years,[19] populated by successive movements from other parts of Africa.[20] The use of iron was introduced to Sierra Leone by the 9th century, and by AD 1000 agriculture was being practiced by coastal tribes.[21] Sierra Leone's dense tropical rainforest largely protected it from the influence of any precolonial African empires[22] and from further Islamic colonization, which were unable to penetrate through it until the 18th century.[23]

European contacts with Sierra Leone were among the first in West Africa. In 1462, Portuguese explorer Pedro da Cintra mapped the hills surrounding what is now Freetown Harbour, naming shaped formation Serra de Leão (Portuguese for Lion Mountains).[12] The Italian rendering of this geographic formation is Sierra Leone, which became the country's name. Soon after Portuguese traders arrived at the harbour and by 1495 a fort that acted as a trading post had been built.[24] The Portuguese were joined by the Dutch and French; all of them using Sierra Leone as a trading point for slaves.[25] In 1562 the English joined the trade in human beings when Sir John Hawkins shipped enslaved 300 people, he had acquired 'by the sword and partly by other means', to the new colonies in America.[26]
[edit] Enslavement and Freedom
An 1835 illustration of liberated Africans arriving in Sierra Leone.

In 1787, a plan was established to settle some of London's "Black Poor" in Sierra Leone in what was called the "Province of Freedom". A number of "Black Poor" arrived off the coast of Sierra Leone on May 15, 1787, accompanied by some English tradesmen. This was organized by the St. George's Bay Company, composed of British philanthropists who preferred it as a solution to continuing to financially support them in London. Many of the "Black poor" were African Americans, who had been promised their freedom for joining the British Army during the American Revolution, but also included other African and Asian inhabitants of London.

Disease and hostility from the indigenous people nearly eliminated the first group of colonists. Through intervention by Thomas Peters, the Sierra Leone Company was established to relocate another group of formerly enslaved Africans, this time nearly 1,200 Black Nova Scotians, most of whom had escaped enslavement in the United States. Given the most barren land in Nova Scotia, many had died from the harsh winters there. They established a settlement at Freetown in 1792 led by Peters. It was joined by other groups of freed Africans and became the first African-American haven for formerly enslaved Africans.
The colony of Freetown in 1856.

Though the English abolitionist Granville Sharp originally planned Sierra Leone as a utopian community, the directors of the Sierra Leone Company refused to allow the settlers to take freehold of the land. Knowing how Highland Clearances benefited Scottish landlords but not tenants, the settlers revolted in 1799. The revolt was only put down by the arrival of over 500 Jamaican Maroons, who also arrived via Nova Scotia.

Thousands of formerly enslaved Africans were returned to or liberated in Freetown. Most chose to remain in Sierra Leone. These returned Africans were from many areas of Africa, but principally the west coast. They joined the previous settlers and together became known as Creole or Krio people.

Cut off from their homes and traditions, they assimilated some aspects of British styles of inhabitants and built a flourishing trade of flowers and beads on the West African coast. The lingua franca of the colony was Krio, a creole language rooted in 18th century African American English, which quickly spread across the region as a common language of trade and Christian mission. In the 1790s, blacks voted for the first time in elections, as did women.[27]
[edit] Colonial era
Bai Bureh, leader of the 1898 rebellion against British rule

In the early 20th century, Freetown served as the residence of the British governor who also ruled the Gold Coast (now Ghana) and the Gambia settlements. Sierra Leone also served as the educational centre of British West Africa. Fourah Bay College, established in 1827, rapidly became a magnet for English-speaking Africans on the West Coast. For more than a century, it was the only European-style university in western Sub-Saharan Africa.

During Sierra Leone's colonial history, indigenous people mounted several unsuccessful revolts against British rule and Krio domination. The most notable was the Hut Tax war of 1898. Its first leader was Bai Bureh, a Temne chief who refused to recognize the British-imposed tax on "huts" (dwellings). The tax was generally regarded by the native chiefs as an attack on their sovereignty. After the British issued a warrant to arrest Bai Bureh alleging that he had refused to pay taxes, he brought fighters from several Temne villages under his command, and from Limba, Loko, Soso, Kissi, and Mandinka villages.

Bureh's fighters had the advantage over the vastly more powerful British for several months of the war. Hundreds of British troops and hundreds of Bureh's fighters were killed.[28] Bai Bureh was finally captured on November 11, 1898 and sent into exile in the Gold Coast (now Ghana), while 96 of his comrades were hanged by the British.

The defeat of the natives in the Hut Tax war ended large scale organised resistance to colonialism; however resistance continued throughout the colonial period in the form of intermittent rioting and chaotic labour disturbances. Riots in 1955 and 1956 involved "many tens of thousands" of natives in the protectorate.[29]

One notable event in 1935 was the granting of a monopoly on mineral mining to the Sierra Leone Selection Trust run by De Beers, which was scheduled to last 98 years.
[edit] An independent nation

The 1924 Sierra Leone constitution was divided into Colony and Protectorate with separate and different political systems. Antagonism between the two entities came to a heated debate in 1947 when proposals were introduced to provide for a single political system for both the Colony and the Protectorate, most of whom were to come from the Protectorate. The Creoles, lead by Isaac Wallace-Johnson naturally opposed the proposals whose effect would have been to diminish their political power. It was due to the astute politics of Sir Milton Margai an ethnic Mende and the leading politician from the protectorate that educated Protectorate elite were won over to join forces with the paramount chiefs in the face of Creole intransigence. Later, Sir Milton use the same skills to win over opposition leaders and moderate Creole elements for the achievement of independence.

In November 1951 Sir Milton Margai oversaw the drafting of a new Sierra Leone constitution which united the formerly separate Colonial and Protectorate legislatures and — most importantly — provided a framework for decolonization..[30] In 1953 Sierra Leone was granted local ministerial powers and Milton Margai was made Chief Minister under the new constitution.[30] The new constitution ensured Sierra Leone a parliamentary system within the Commonwealth of Nations and was formally adopted in 1958.[30]

Margai led the Sierra Leonean delegation at the constitutional conferences that were held with British Colonial Secretary Iain Macleod in London in 1960.[31] On April 27, 1961, Milton Margai led Sierra Leone to independence from the United Kingdom.[30] Thousands of Sierra Leoneans throughout the nation took to the street to celebrate their Independence. The nation held its first general elections on May 27, 1962 and Margai was elected Sierra Leone's first Prime Minister by a landslide.[30] Milton Margai's political party, the Sierra Leone People's Party (SLPP), won by large margins in the nation's first general election under universal adult suffrage in May 1962. An important aspect of Sir Milton's character was his self-effacement. He was neither corrupt nor did he make a lavish display of his power or status. Sir Milton's government was based on the rule of law and the notion of separation of powers, with multiparty political institutions and fairly vibrant representative structures. Milton Margai used his conservative ideology to lead Sierra Leone without much strife. He appointed government officials with a clear eye to satisfy various ethnic groups.

Upon Margai's death on April 28, 1964, his brother Sir Albert Margai, who was Sierra Leone's Minister of Finance under Milton's government was chosen by an overwhelming majority vote in Parliament to be the new leader of the SLPP and the country's next prime minister. Sir Albert Margai's leadership was briefly challenged by Sierra Leone's Foreign Minister John Karefa-Smart, an ethnic Sherbro, who questioned Sir Albert succession to the SLPP leadership position. Kareefa-Smart received little support in Parliament to have Margai strip of the SLPP leadership.

Sir Albert was sworn in as Sierra Leone's second Prime Minister the same day his brother died at a ceremony held at the state house in Freetown. Soon after Margai was sworn in as Prime Minister, he immediately dismissed Karefa-Smart and several other senior government officials who had served under his elder brother Sir Milton's government, as he viewed them as traitors and a threat to his administration.

Sir Albert was highly criticized during his three-year rule as prime minister. He was accused of corruption and of favouritism toward his own Mende ethnic group. Albert Margai, unlike his late brother Milton, was opposed to the colonial legacy of allowing the country's Paramount Chiefs executive powers and he was seen as a threat to the existence of the ruling houses across the country. This made him unpopular with the powerful paramount chiefs, most of whom were founding members of the SLPP. To strengthen support for his reform agenda for the party and the country the new Pime Minister brought into the executive of the SLPP and his government younger, western-educated, and more radicallised members of the party including Salia Jusu Sheriff (PhD). The party was thus divided with the traditionalist and more powerful old guard against the new and younger leaders.

Sir Albert also tried to establish a one-party state but with very little support in Parliament, even among his fellow SLPP members and was also met by fierce resistance from the main opposition the All People's Congress (APC)], which had become suddenly more popular than the rulling SLPP and ultimately abandoned the idea. During Albert Margai's administration, The Mende increased their influence both in the civil service and the army. Most of the top military and government positions were held by Mendes, and Mende country (the South-Eastern part of Sierra Leone) received preferential treatment.
APC political rally in Kabala, Koinadugu District outside the home of supporters of the rival SLPP in 1967.

In closely contested general elections in March 1967, Sierra Leone Governor General Henry Josiah Lightfoot Boston declared the new prime minister to be Siaka Stevens, an ethnic Limba and the candidate of the All People's Congress (APC) and the Mayor of Freetown. Stevens was sworn in as Sierra Leone's third Prime Minister on May 17, 1967 in Freetown. Hours after taking office, Stevens was ousted in a bloodless coup led by the commander of the Sierra Leone Armed Forces, Brigadier David Lansana, an ethnic Mende and a prominent supporter of Albert Margai who had appointed him in 1964 as the commander of the Sierra Leone Armed Forces. Brigadier Lansana insisted that the determination of office should await the election of the tribal representatives in Parliament, mostly from Mende chiefdoms in South-Eastern Sierra Leone.

Stevens was placed under house arrest and martial law was declared. However, on March 23, 1968, a group of senior military officers in the Sierra Leone Army, lead by Brigadier Andrew Juxon-Smith, an ethnic Creole, overrode this action by seizing control of the government, arresting Lansana and suspending the constitution. The group constituted itself as the National Reformation Council (NRC) with Brigadier Andrew Juxon-Smith as its chairman. In April 1968, the NRC was overthrown by a group of senior military officers in the Sierra Leone Army who called themselves the Anti-Corruption Revolutionary Movement (ACRM), led by Brigadier General John Amadu Bangura, an ethnic Limba. The ACRM imprisoned Brigadier Andrew Juxon-Smith and other senior NRC members, restored the constitution and reinstated Stevens as Prime Minister.

Stevens assumed power once again in 1968 with a great deal of promise and ambition. Much trust was placed upon him as he was then champion of multi-party politics. Upon taking power from the military, however, he soon strove to drive the SLPP from competitive politics in various general elections, using violence and intimidation.

After the return to civilian rule, by-elections were held (beginning in autumn 1968) and an all-APC cabinet was appointed. Calm was not completely restored. In November 1968, Stevens declared a state of emergency after disturbance in the provinces.

Stevens had campaigned on a platform of socialist principles. However, when he became Prime Minister he abandoned his pre-election promises and employed an authoritarian model of governance.[32] Many senior officers in the Sierra Leone military were greatly disappointed but none could confront Stevens. Brigadier General Bangura who had reinstated Stevens as Prime Minister was widely considered the only person who could put the brakes on Stevens. Bangura was a magnetic and popular figure among Sierra Leoneans. The army was devoted to him and this made him potentially dangerous to Steven's new agenda in the shifting political climate of Sierra Leone. In January 1970, Bangura was arrested and charged with conspiracy and plotting to commit a coup against the Stevens government. He was convicted and sentenced to death by execution. On March 29, 1970, Stevens had Bangura hanged at the Kissy Road in central Freetown.

In March 1971 the government survived an unsuccessful military coup. The coup leaders were executed, including several senior officers in the army and some senior government officials.

On April 19, 1971, parliament declared Sierra Leone a republic, with Siaka Stevens as President. Guinean troops requested by Stevens to support his government were in the country from 1971 to 1973. In May 1973 a general elections were held throughout the country, but the main opposition the SLPP boycotted the 1973 general election, alleging widespread intimidation and proceedural obstruction. .

In 1973 president Stevens and president William Tolbert of Liberia signed a treaty forming the Mano River Union to facilitate trade between Sierra Leone and Liberia – with Guinea joining in 1980 under president Sekou Toure. In 1975 Sierra Leone joint the Economic Community of West African States (commonly known as ECOWAS).

An alleged plot to overthrow president Stevens failed in 1974 and its leaders were executed. In March 1976, Stevens was elected without opposition for a second five-year term as president. On July 19, 1975, 14 senior army and government officials including Brigadier David Lansana, former cabinet minister Mohamed Sorie Forna, Brigadier General Ibrahim Bash Taqi and Lieutenant Habib Lansana Kamara were executed after being convicted for allegedly attempting a coup to topple president Stevens' government.

In early 1977 a major anti-government demonstration by students and youth occurred throughout the country against the APC government and deteriorating economic conditions. But the demonstration was put down by the police and the army.

In the national parliamentary election of May 1977, the APC won 74 seats and the main opposition, the SLPP, won 15. The SLPP condemned the election, alleged widespread vote-rigging and voter intimidation. In 1978, the APC dominant parliament approved a new constitution making the country a one-party state. The 1978 referendum made the APC the only legal political party in Sierra Leone. This move lead to another major demonstration in many parts of the country but again it was put dawn by the army and the police.

Under the APC regimes headed by Stevens, the Limba, Stevens' own ethnic group, enjoyed strong influence in the government and civil service. During the 1970s, another major ethnic group, the Temne joined the Mende in opposition to the APC government. But after Stevens appointed a Temne, Sorie Ibrahim Koroma as vice-president in 1978, the Temne appeared to have emerged as an influential group in the government.

Stevens is generally criticised for dictatorial methods and government corruption, but, on a positive note, he reduced the ethnic polarisation in government by incorporating members of various ethnic groups into his all-dominating APC government.

Siaka Stevens retired in November, 1985 after being in power for 18 years, but continued to be chairman of the APC. The APC named a new presidential candidate to succeed Stevens at their last delegate conference held in Freetown in November 1985. He was Major General Joseph Saidu Momoh, the commander of the Republic of Sierra Leone Armed Forces and Stevens' own choice to succeed him. As head of the Sierra Leone Armed Forces, Major General Momoh was very loyal to Stevens who had appointed him to the position. Like Stevens, Momoh was also a member of the minority Limba ethnic group. Joseph Saidu Momoh was elected President in a one-party parliament as the only contesting candidate. Momoh was sworn in as Sierra Leone's second president on November 28, 1985 at an inauguration ceremony held in Freetown. A one party parliamentary elections between APC members were held in May, 1986.

President Momoh's strong links with the army and his verbal attacks on corruption earned him much needed initial support among Sierra Leoneans. With the lack of new faces in the new APC cabinet under president Momoh and the return of many of the old faces from Stevens government, criticisms soon arose that Momoh was simply perpetuating the rule of Stevens. The next couple of years under the Momoh administration were characterised by corruption, which Momoh defused by sacking several senior cabinet ministers. To formalise his war against corruption, President Momoh announced a Code of Conduct for Political Leaders and Public Servants"

After an alleged attempt to overthrow President Momoh in March 1987, more than 60 senior government officials were arrested, including Vice-President Francis Minah, who was removed from office, convicted for plotting the coup, and executed by hanging in 1989 along with 5 others.
[edit] Multi-party constitution and Revolutionary United Front rebellion
See also: Sierra Leone Civil War

In October 1990, due to mounting pressure from both within and outside the country for political and economic reform, president Momoh set up a constitutional review commission to review the 1978 one-party constitution. Based on the commission's recommendations a constitution re-establishing a multi-party system was approved by the exclusive APC Parliament by a 60% majority vote, becoming effective on October 1, 1991. By November 1991, political oppostion became active once again in Sierra Leone. In late November 1991, president momoh proposed a multi-party presidential and parliamentary elections to be held in the country in October 1992.

There was great suspicion that president Momoh was not serious, and APC rule was increasingly marked by abuses of power. The APC was also alleged to have been horlding arms and planning a violent campaign against the opposition parties ahead of muti-party general elections scheduled for late 1992. Several senior government officials in the APC administration like Salia Jusu Sheriff, Abass Bundu, J.B. Dauda and [[Sama Bany] resigned from the APC government respectively to resuscitate the once dimolished SLPP party. While other senior government officials like Thaimu Bangura, Edward Kargbo and Desmond Luke resign from the APC and formed their own respective political parties to challenge the ruling APC.

Civil war broke out, mainly due to government corruption and mismanagement of diamond resources. The brutal civil war going on in neighboring Liberia played an undeniable role in the outbreak of fighting in Sierra Leone. Charles Taylor—then leader of the National Patriotic Front of Liberia—reportedly helped form the Revolutionary United Front (RUF) under the command of former Sierra Leonean army corporal Foday Saybana Sankoh, an ethnic Temne from Tonkolili District in Northern Sierra Leone. Sankoh was a British trained former army corporal who had also undergone guerrilla training in Libya. Taylor’s aim was for the RUF to attack the bases of Nigerian dominated peacekeeping troops in Freetown who were opposed to his rebel movement in Liberia.

The RUF, led by Sankoh and backed by Taylor, launched its first attack in villages in Kailahun District in Eastern Sierra Leone from Liberia on March 23, 1991.In 2003 Foday Sankoh was indicted by the Special Court for Sierra Leone for war crimes and crimes against humanity and died under UN custody before the trials could be concluded. Charles Taylor, who is a former president of Liberia, is currently in the International Criminal Court facing chargies of war crimes committed by Sankoh's RUF in Sierra Leone.

The government of Sierra Leone, overwhelmed by a crumbling economy and corruption, and a demoralised army was unable to put up significant resistance against the incursion of the RUF. Within a month of entering Sierra Leone from Liberia, the RUF controlled much of Eastern Sierra Leone, including the cash crop production areas of Kailahun and the government diamond minesin Kono District. Forced recruitment of child soldiers was also an early feature of the rebel strategy.

On April 29, 1992, a group of six young soldiers in the Sierra Leonean army, apparently frustrated by the government's failure to deal with the rebels and a revolt over salary pay launched a military coup which sent president Momoh into exile in Guinea [2]. The young soldiers were Second Lieutenant Tom Nyuma, an ethnic Kissi, captain Valentine E. M. Strasser, an ethnic Creole, Captain Solomon A. J. Musa, an ethnic Mende, Captain Samuel Komba Kambo, an ethnic Kono, Brigadier Julius Maada Bio, an ethnic Mende, Captain Komba Mondeh, an ethnic Kono and Colonel Yahya Kanu, an ethnic Temne. Colonel Yahya Kanu -was the very popular commander of the fearless Tiger Battalion which was at the forefront in the war against the RUF under president Momoh. The soldiers established the National Provisional Ruling Council (NPRC). Though he was not formally declared head of the new junta, and even refused to accept in an interview with the BBC Focus on Africa Program that the revolt was a coup by his men, Colonel Kanu was seen as the defacto leader of the NPRC. Kanu was later arrested and imprisoned by his junior officers, who accused him of trying to negotiate with the toppled APC administration. Kanu's arrest divided the army into two rival groups, namely, the Tiger Battalion and Tom Nyuma's Cobra Battalion and their respective supporters. On April 29, 1992, 27-year-old Valentine Strasser took over as leader and chairman of the NPRC and Head of State of Sierra Leone. Strasser became the youngest Head of State in the world, just three days after his 27 birthday. 25 year-old S.A.J. Musa, a close friend of Strasser and an officer in Kanu's feared Tiger Battalion, was named Vice-Chairman of the NPRC. Many Sierra Leoneans nationwide rushed into the streets to celebrate the NPRC's takeover from the 23-year dictatorial APC regime, which they perceived as corrupt. The NPRC junta immediately suspended the 1991 Constitution, declared a state of emergency, banned all political parties, limited freedom of speech and freedom of the press, and enacted a rule-by-decree policy, in which soldiers were granted unlimited powers of administrative detention without charge or trial, and challenges against such detentions in court were precluded. . The NPRC Junters maintained relations with ECOWAS and support for ECOMOG troops in Sierra Leone which was fighting in Liberia. In his first speech as head of state the young Strasser reassured the world of meeting his country's obligations to her creditors, and making a commitment to the IMF and the World Bank to accelerate the economic reform process started by Momoh's government in 1989 aimed at stabilizing the severely crippled economy and shortly after that went on to negotiate a Structural Adjustment Program with these two institutions. The junta formed the Supreme Council of State that was made up of only the leading figures of the NPRC including the six ring leaders and chaired by Strasser himself. They also appointed an advisory council which consisted of retired senior civil servants and academic, chaired by a retired UN administrator Ahmad Tejan Kabbah, who were all regarded as untainted by the 23 years of alleged APC mismanagement, curruption and abuse of power.

In December 1992, an alleged coup attempt led by Sgt. Lamin Bangura, an ethnic Temne and some junior army officers of the Tiger Battalion loyal to Colonel Yahya Kanu attempted a coup to topple the NPRC administration of Strasser and release the detained Colonel Yayah Kanu was foiled and it led to the execution of seventeen soldiers in the Sierra Leone Army, including Sgt. Bangura and Yayah Kanu, and some senior members of the overthrown APC government who had been in detention at the Pa Demba Road prison including the notorious Inspector General of Police Bambay Kamara and ministers, senior party members and thugs. By mid 1993 captain Strasser announced a plan to hand over the government to civilian rule by 1996. James Jonah, who was by then Deputy Secretary General of the United Nations, was appointed by the NPRC Junta as the chairman of the new Interim Sierra Leone National Electoral Commission, in charge of the demarcation of electoral boundaries and voters registration. In 1994 the NPRC junters proposed change in the age restriction in the Sierra Leonean constitution which stated only Sierra Leoneans over the age of 40 are eligible for the presidency, thus excluding Strasser and others in the NPRC.

The NPRC proved to be nearly as ineffectual as the Momoh-led APC government in repelling the RUF. More and more of the country fell to RUF fighters, and by 1995 they held much of the diamond-rich Eastern Province and were at the edge of Freetown. In response, the NPRC hired several hundred mercenaries from the private firm Executive Outcomes. Within a month they had driven RUF fighters back to enclaves along Sierra Leone’s borders. During this time corruption had erupted within senior NPRC members and there was great suspicious that SAJ Musa, who had suddenly become the most popular NPRC members among Sierra Leoneans was planning a coup to topple Strasser. On July 5, 1995 Strasser dismissed SAJ Musa as deputy chairman of the NPRC and appointed Julius Maada Bio to the position. Musa was arrested by his fellow NPRC members and was briefly placed under house arrest in Freetown. However, he was later released and given permission by Strasser to immediately leave the country. Senior NPRC members, including Bio, Nyuma and Mondeh, were becoming increasingly unhappy with Strasser's handling of the preparation for the pending elections, the peace negotiation with the RUF, and the transition to democratic civilian rule.

In January 1996, after nearly four years in power, Strasser was ousted in a polite coup led by his NPRC deputy Brigadier General Julius Maada Bio and was backed by several senior NPRC members including both Tom Nyuma and Komba Mondeh. Bio claimed that Strasser was attempting to unilaterally amend the age restriction aspect of the constitution in order to perpetrate his hold on power. Bio reinstated the Constitution and called for general elections. In the second round of presidential elections in early 1996, Ahmad Tejan Kabbah, and ethnic Mandingo and the candidate of the Sierra Leone People's Party (SLPP), won 59% of the vote, over John Karefa-Smart, an ethnic Sherbro and the candidate of the United National People's Party (UNPP) who won 41%. Bio fulfilled promises of a return to civilian rule, and handed power to Kabbah. President Tejan Kabbah's SLPP party also won a majority of the seats in Parliament.

For years Sierra Leonean Soldiers in the lower ranks were not paid a good salary and they were denied privileges and benefits. Soldiers were killed at the war front and no provision was made for their families. Major Johnny Paul Koroma, a senior officer in the Sierra Leone army who hail from the Limba ethnic group from Kono District was allegedly involved in an attempt to overthrow the government of president Kabbah. He was arrested, tried, convicted, and imprisoned at Freetown's Pademba Road Prison. On May 25, 1997, a group of seventeen junior officers in the Sierra Leone army, loyal to Major Koroma, who called themselves the Armed Forces Revolutionary Council (AFRC) led by Corporal Tamba Gborie and Seargon Alex Tamba Brima, both ethnic Kono, launch a military coup which sent president Kabbah into exil in Guinea [3]. Corporal Tamba Gborie quickly went to the SLBS FM 99.9 headquarter in Freetown to announce the coup to a shocked nation, and to alert all soldiers throughout the nation on guard duty. The AFRC released Koroma from prison and installed him as their chairman and Head of State of the country with Corporal Tamba Gborie as deputy in command of the AFRC. Koroma suspended the constitution, banned demonstrations, shut down all private radio stations in the country and invited the RUF to join the new junta government, with its leader Foday Sankoh as the Vice-Chairman of the new AFRC-RUF coalition junta government. Within days Freetown was overwhelmed by the presence of the RUF combatants who came to the city in their thousands. The Kamajors, a group of traditional fighters mostly from the Mende ethnic group under the command of deputy Defense Minister Samuel Hinga Norman remained loyal to President Kabbah. The Kamajors defended Bo, the country's second largest city from the Junter and continue their attack against the AFRC and RUF in south-eastern Sierra Leone

After 10 months in office, the junta was ousted by the Nigeria-led ECOMOG forces, and the democratically elected government of president Kabbah was reinstated in March 1998. Hundreds of civilians who had been accused of helping the AFRC government were illegally detained. Courts-martial were held for soldiers accused of assisting the AFRC government. Twenty-four of these were found guilty and were executed without appeal in October 1998. On January 6, 1999, AFRC made another unsuccessful attempt to overthrow the government, causing an estimated 3,000 deaths, raping women and girls, abdduction and subsequent conscirption of children, amputations of limbs, and much destruction of property in and around Freetown.

In October, the United Nations agreed to send peacekeepers to help restore order and disarm the rebels. The first of the 6,000-member force began arriving in December, and the UN Security Council voted in February 2000 to increase the force to 11,000, and later to 13,000. But in May, when nearly all Nigerian forces had left and UN forces were trying to disarm the RUF in eastern Sierra Leone, Sankoh's forces clashed with the UN troops, and some 500 peacekeepers were taken hostage as the peace accord effectively collapsed. The hostage crisis resulted in more fighting between the RUF and the government.

Between 1991 and 2001, about 50,000 people were killed in Sierra Leone's civil war. Hundreds of thousands of people were forced from their homes, and many became refugees in Guinea and Liberia. In 2001, UN forces moved into rebel-held areas and began to disarm rebel soldiers. By January 2002, the war was declared over. In May, Kabbah was reelected president. By 2004, the disarmament process was complete. Also in 2004, a UN-backed war crimes court began holding trials of senior leaders from both sides of the war. In December 2005, UN peacekeeping forces pulled out of Sierra Leone.

In August 2007, Sierra Leone held presidential and parliamentary elections. However, no presidential candidate won the 50% plus one votes majority stipulated in the constitution on the first round of voting. A runoff election was held in September 2007, and Ernest Bai Koroma, the candidate of the APC and ethnically a half Limba and half Temne from the north was elected president.

By 2007, there had been an increase in the number of drug cartels, many from Colombia, using Sierra Leone as a base to ship drugs on to Europe.[27] It was feared that this might lead to increased corruption and violence and turn the country, like neighboring Guinea-Bissau, into a narco state. However, the new government of president Koroma quickly amended the laws against drug trafficking in the country, updating the existing legislation from those inhereted at independence in 1961, to address the international concerns, increasing punishment for offenders both in terms of higher, if not prohibitive, fines, lengthier prison terms and provision for possible extradition of offenders wanted elsewhere including to the United States of America.
[edit] Geography and climate
Main article: Geography of Sierra Leone
Satellite image of Sierra Leone, generated from raster graphics data supplied by The Map Library
The road from Kenema to Kailahun District.

Sierra Leone is located on the west coast of Africa, between the 7th and 10th parallels north of the equator. Sierra Leone is bordered by Guinea to the north and northeast, Liberia to the south and southeast, and the Atlantic Ocean to the west.[33]

The country has a total area of 71,740 km2 (27,699 sq mi), divided into a land area of 71,620 km2 (27,653 sq mi) and water of 120 km2 (46 sq mi).[2] The country has four distinct geographical regions. In eastern Sierra Leone the plateau is interspersed with high mountains, where Mount Bintumani reaches 1,948 m (6,391 ft), the highest point in the country. The upper part of the drainage basin of the Moa River is located in the south of this region.

The centre of the country is a region of lowland plains, containing forests, bush and farmland,[33] that occupies about 43% of Sierra Leone's land area. The northern section of this has been categorised by the World Wildlife Fund as part of the Guinean forest-savanna mosaic ecoregion, while the south is rain-forested plains and farmland. In the west Sierra Leone has some 400 km (249 mi) of Atlantic coastline, giving it both bountiful marine resources and attractive tourist potential. The coast has areas of low-lying Guinean mangroves swamp. The national capital Freetown sits on a coastal peninsula, situated next to the Sierra Leone Harbor, the world's third largest natural harbour.
[edit] Climate
Main article: Climate of Sierra Leone

The climate is tropical, with two seasons determining the agricultural cycle: the rainy season from May to November, and a dry season from December to May, which includes harmattan, when cool, dry winds blow in off the Sahara Desert and the night-time temperature can be as low as 16 °C (60.8 °F). The average temperature is 26 °C (78.8 °F) and varies from around 26 °C (78.8 °F) to 36 °C (96.8 °F) during the year.[34][35]
[edit] Environment

Logging, mining, slash and burn, and deforestation for land conversion - such as cattle grazing - have dramatically diminished forested land in Sierra Leone since the 1980s. Correspondingly the habitat for the African Wild Dog, Lycaon pictus, has been decreased, such that this canid is deemed to have been extirpated in Sierra Leone.[36]

Until 2002, Sierra Leone lacked a forest management system due to a brutal civil war that caused tens of thousands of deaths. Deforestation rates have increased 7.3% since the end of the civil war[37]. On paper, 55 protected areas covered 4.5% of Sierra Leone as of 2003. The country has 2,090 known species of higher plants, 147 mammals, 626 birds, 67 reptiles, 35 amphibians, and 99 fish species.[37]

The Environmental Justice Foundation has documented how the number of illegal fishing vessels in Sierra Leone's waters has multiplied in recent years. The amount of illegal fishing has significantly depleted fish stocks, depriving local fishing communities of an important resource for survival. The situation is particularly serious as fishing provides the only source of income for many communities in a country still recovering from over a decade of civil war.[38]

In June 2005, the Royal Society for the Protection of Birds (RSPB) and Bird Life International agreed to support a conservation-sustainable development project in the Gola Forest in southeastern Sierra Leone,[39] an important surviving fragment of rainforest in Sierra Leone.
[edit] Government and politics
Main article: Politics of Sierra Leone

Sierra Leone is a constitutional republic with a directly elected president and a unicameral legislature. The current system of government in Sierra Leone, established under the 1991 Constitution, is modeled on the following structure of government: the Legislature, the Executive and the Judiciary.[40]

Within the confines of the 1991 Constitution, supreme legislative powers are vested in Parliament, which is the law making body of the nation. Supreme executive authority rests in the president and members of his cabinet and judicial power with the judiciary of which the Chief Justice is head.
Ernest Bai Koroma, current president of Sierra Leone

The president is the head of state, the head of government and the commander-in-chief of the Sierra Leone Armed Forces and the Sierra Leone Police. The president appoints and heads a cabinet of ministers, which must be approved by the Parliament. The president is elected by popular vote to a maximum of two five-year terms. The president is the highest and most influential position within the government of Sierra Leone.

To be elected president of Sierra Leone, a candidate must gain at least 55% of the vote. If no candidate gets 55%, there is to be a second-round runoff between the top two candidates.

The current president of Sierra Leone is Ernest Bai Koroma, who was sworn in on September 17, 2007, shortly after being declared the winner of a tense run-off election over the incumbent Vice president, Solomon Berewa of the Sierra Leone People's Party (SLPP).[41]

Next to the president is the Vice president, who is the second-highest ranking government official in the executive branch of the Sierra Leone Government. As designated by the Sierra Leone Constitution, the vice president is to become the new president of Sierra Leone upon the death, resignation, or removal of the president by parliament and to assume the Presidency temporarily while the president is otherwise temporarily unable to fulfill his or her duties. The vice president is elected jointly with the president as his or her running mate. Sierra Leone's current vice president is Samuel Sam-Sumana, sworn in on September 17, 2007.

The Parliament of Sierra Leone is unicameral, with 124 seats. Each of the country's fourteen districts is represented in parliament. 112 members are elected concurrently with the presidential elections; the other 12 seats are filled by paramount chiefs from each of the country's 12 administrative districts.

The current parliament in the August 2007 Parliamentary elections is made up of three political parties. The most recent parliamentary elections were held on August 11, 2007. The All People's Congress (APC), won 59 of 112 parliamentary seats; the Sierra Leone People's Party (SLPP) won 43; and the People's Movement for Democratic Change (PMDC) won 10. To be qualified as Member of Parliament, the person must be a citizen of Sierra Leone, must be at least 21 years old, must be able to speak, read and write the English language with a degree of proficiency to enable him to actively take part in proceedings in Parliament; and must not have any criminal conviction.[40]
The Sierra Leone Supreme Court in the capital Freetown, the highest and most powerful court in the country

Since independence in 1961, Sierra Leone's politics has been dominated by two major political parties, the Sierra Leone People's Party (SLPP), and the ruling All People's Congress (APC), although other minor political parties have also existed but with no significant supports.

The judicial power of Sierra Leone is vested in the judiciary, headed by the Chief Justice and comprising the Sierra Leone Supreme Court, which is the highest court in the country and its ruling therefore cannot be appealed; High Court of Justice; the Court of Appeal; the magistrate courts; and traditional courts in rural villages. The president appoints and parliament approves Justices for the three courts. The Judiciary have jurisdiction in all civil and criminal matters throughout the country. The current Sierra Leone's Chief Justice is Umu Hawa Tejan Jalloh, who was appointed by President Ernest Bai Koroma and took office on January 25, 2008 upon his confirmation by parliament. She is the first woman in the history of Sierra Leone to hold such position.[42]
[edit] Foreign relations
Main article: Foreign relations of Sierra Leone

The Sierra Leone Ministry of Foreign Affairs and International Relations, headed by Minister of Foreign Affairs Zainab Hawa Bangura is responsible for foreign policy of Sierra Leone. Sierra Leone has diplomatic relations that include China, Libya, Iran, and Cuba. Sierra Leone has good relations with the West, including the United States and has maintained historical ties with the United Kingdom and other former British colonies through membership of the Commonwealth of Nations.[43]

Former President Siaka Stevens' government had sought closer relations with other West African countries under the Economic Community of West African States (ECOWAS) a policy continued by the current. Sierra Leone, along with Liberia and Guinea form the Mano River Union (MRU) primarily designed to implement development projects and promote regional economic integration between the three countries.[44]

Sierra Leone is also a member of the United Nations and its specialized agencies, the African Union, the African Development Bank (AFDB), the Organization of the Islamic Conference (OIC), and the Non-Aligned Movement (NAM).[45] Sierra Leone is also a member of the International Criminal Court with a Bilateral Immunity Agreement of protection for the US military (as covered under Article 98).
[edit] Provinces and districts
Main articles: Provinces of Sierra Leone, Districts of Sierra Leone, and Chiefdoms of Sierra Leone
The 14 districts of Sierra Leone.

The Republic of Sierra Leone is composed of three provinces: the Northern Province, Southern Province and the Eastern Province and one other region called the Western Area. The provinces are further divided into 12 districts, and the districts are further divided into chiefdoms, except for the Western Area.
District 	Capital 	Area km2 	Province 	Population (2004 census)[46] 	Population (2008 estimates)
Bombali District 	Makeni 	7,985 	Northern Province 	408,390 	424,100[47]
Koinadugu District 	Kabala 	12,121 	265,758 	
Port Loko District 	Port Loko 	5,719 	455,746 	483,752[48]
Tonkolili District 	Magburaka 	7,003 	347,197 	370,425[49]
Kambia District 	Kambia 	3,108 	270,462 	299,725[50]
Kenema District 	Kenema 	6,053 	Eastern Province 	497,948 	522,656[51]
Kono District 	Koidu Town 	5,641 	335,401 	
Kailahun District 	Kailahun 	3,859 	358,190 	389,253[52]
Bo District 	Bo 	5,473.6[53] 	Southern Province 	463,668 	527,131[54]
Bonthe District 	Mattru Jong 	3,468 	129,947 	137,155 [4]
Pujehun District 	Gandorhun 	4,105 	228,392 	262,073[55]
Moyamba District 	Moyamba 	6,902 	260,910 	
Western Area Urban District 	Freetown 	3,568 	Western Area 	1,272,873 	1,473,873
Western Area Rural District 	Freetown 	4,175 	174,249 	205,400
[edit] Major cities
Freetown (population 1,070,200[56]) The capital, largest city and economic center of Sierra Leone.
Koidu Town (population 111,800) Sierra Leone's fourth largest city and a major center for diamond trade. The city is located in Kono District, the richest diamond producing District in Sierra Leone
City 	2004 census[46] 	Current population estimate
Freetown 	772,873 	1,070,200[56]
Bo 	149,957 	215,474 [5].
Kenema 	128,402 	164,125 [6].
Koidu Town 	80,025 	111,800[57]
Makeni 	82,840 	105,900[57]

    * The populations quoted above for the five largest cities are estimates from the sources cited. Different sources give different estimates. Some claim that Magburaka should be included in the above list, but one source estimates the population at only 14,915,[58] whilst another puts it as high as 85,313.[59]

[edit] Economy
	This section does not cite any references or sources.
Please help improve this article by adding citations to reliable sources. Unsourced material may be challenged and removed. (November 2008)
Diamond miners in Kono District.
Main article: Economy of Sierra Leone

Sierra Leone is slowly emerging from a protracted civil war and is showing signs of a successful transition. Investor and consumer confidence continue to rise, adding impetus to the country’s economic recovery. There is greater freedom of movement and the successful re-habitation and resettlement of residential areas.

Rich in minerals, Sierra Leone has relied on mining, especially diamonds, for its economic base. The country is among the top 10 diamond producing nations in the world. Mineral exports remain the main foreign currency earner. Sierra Leone is a major producer of gem-quality diamonds. Though rich in diamonds, it has historically struggled to manage their exploitation and export.

Annual production of Sierra Leone's diamond estimates range between $250–300 million U.S dollar. Some of that is smuggled, where it is possibly used for money laundering or financing illicit activities. Formal exports have dramatically improved since the civil war with efforts to improve the management of them having some success. In October 2000, a UN-approved certification system for exporting diamonds from the country was put in place and led to a dramatic increase in legal exports. In 2001, the government created a mining community development fund, which returns a portion of diamond export taxes to diamond mining communities. The fund was created to raise local communities' stake in the legal diamond trade

Sierra Leone is perhaps best known for its blood diamonds that were mined and sold for high prices during the civil war.[citation needed] In the 1970s and early 1980s, economic growth rate slowed because of a decline in the mining sector and increasing corruption among government officials.

By the 1990s economic activity was declining and economic infrastructure had become seriously degraded. Over the next decade much of the formal economy was destroyed in the country’s civil war. Since the end of hostilities in January 2002, massive infusions of outside assistance have helped Sierra Leone begin to recover. Much of the recovery will depend on the success of the government's efforts to limit corruption by officials, which many feel was the chief cause for the civil war. A key indicator of success will be the effectiveness of government management of its diamond sector.

Sierra Leone has one of the world's largest deposits of rutile, a titanium ore used as paint pigment and welding rod coatings. Sierra Rutile Limited, owned by a consortium of United States and European investors, began commercial mining operations near the city of Bonthe, in the Southern Province, in early 1979. It was then the largest non-petroleum US investment in West Africa. The export of 88,000 tons realized $75 million in export earnings in 1990. In 1990, the company and the government made a new agreement on the terms of the company's concession in Sierra Leone. Rutile and bauxite mining operations were suspended when rebels invaded the mining sites in 1995, but exports resumed in 2005.

About two-thirds of the population engages in subsistence agriculture, which accounts for 52.5% of national income. The government is trying to increase food and cash crop production and upgrade small farmer skills. The government works with several foreign donors to operate integrated rural development and agricultural projects.

Despite its successes and development, the Sierra Leone economy still faces significant challenges. There is high unemployment, particularly among the youth and ex-combatants. Authorities have been slow to implement reforms in the civil service, and the pace of the privatisation programme is also slacking and donors have urged its advancement.
[edit] Currency

Sierra Leone’s currency is the Leone. The central bank of the country is the Bank of Sierra Leone which is located in the capital, Freetown.

Sierra Leone operates a floating exchange rate system, and foreign currencies can be exchanged at any of the commercial banks, recognised foreign exchange bureaux and most hotels.

Credit card use is limited in Sierra Leone, though they may be used at some hotels and restaurants. There are a few internationally linked automated teller machines that accept Visa cards in Freetown operated by ProCredit Bank.
[edit] Demographics
Main article: Demographics of Sierra Leone
A Mende woman in the village of Jojoima in Kailahun District

The 2009 UN estimate of Sierra Leone's population is 6.4m. Freetown, with an estimated population of 1,070,200, is the capital, largest city and the hub of the economy, commercial, educational and cultural centre of the country. Bo is the second city with an estimated population of up to 269,000. Other cities with an estimated population over 100,000 are Kenema, Koidu Town and Makeni.
Sierra Leonean children in Koindu, Kailahun District playing next to a school damaged during the Sierra Leone Civil War

Although English is the official language[6] spoken at schools, government administration and by the media, Krio (language derived from English and several African languages and native to the Sierra Leone Krio people) is the most widely spoken language in virtually all parts of Sierra Leone. The Krio language is spoken by 97%[2] of the country's population and unites all the different ethnic groups, especially in their trade and interaction with each other.[1]

According to the World Refugee Survey 2008, published by the U.S. Committee for Refugees and Immigrants, Sierra Leone had a population of 8,700 refugees and asylum seekers at the end of 2007. Nearly 20,000 Liberian refugees voluntarily returned to Liberia over the course of 2007. Of the refugees remaining in Sierra Leone, nearly all were Liberian.[60]

The life expectancy of Sierra Leone is 41 years.[61]
[edit] Religion
Further information: Islam in Sierra Leone, Roman Catholicism in Sierra Leone, and Hinduism in Sierra Leone
Religions in Sierra Leone
religion 			percent 	
Islam 	
  
	60%
Christianity 	
  
	30%
Indigenous religions 	
  
	10%

Sierra Leone is a predominantly Muslim nation. Followers of Islam are estimated to comprise 60% of Sierra Leone's population. Muslims predominate in all of the country's three provinces and plus the Western Area, though formerly they were concentrated in the north, with the south being mainly Christian. Followers of Christianity comprise about 30% of the population, and those following only African indigenous religion about 10%, though many inhabitants combine traditional beliefs with one of the newer faiths. [7][8][9] There are small numbers of adherents to other faiths such as Bahāʾī, Hinduism, and Judaism.[10].

The Sierra Leone constitution provides freedom of religion and the government generally protects this right and does not tolerate its abuse. Unlike many other African countries, the religious diversity of Sierra Leone has seldom led to conflict.
[edit] Ethnic groups
Ethnic groups of Sierra Leone
Mende
Temne
Limba
Kono
Mandingo
Krio
Fula
Kuranko
Sherbro
Susu
Loko
Kissi
Yalunka
Vai

The Sierra Leone government officially recognizes fifteen ethnic groups,[62] each with its own language and custom. Unlike most African nations, Sierra Leone has no serious ethnic divisions and no serious religious divisions. People often married across tribal and religious boundaries.

The two largest and most dominant are the Mende and Temne, each comprises 30% of the population[63] (about 1,888,000 members each). The Mende predominate in the South-Eastern Provinces; the Temne likewise predominate in the Northern Province and the Western Area. Sierra Leone's national politics centers on the competition between the north, dominated by the Temne and the south-east dominated by the Mende.

The third largest ethnic group are the Limba at (8.5%) of the population and they are a close ally of the Temne. The Limba are primarily found in Northern Sierra Leone, particularly in Bombali District and they are one of the earliest inhabitant of the country. Sierra Leone's first president Siaka Stevens and the country's second president Joseph Saidu Momoh are ethnic Limba.

The fourth largest ethnic group are the Kono people at around (8%) of the population and they are primarily found in Kono District in Eastern Sierra Leone. The Kono are primarily diamond miners and farmers. Several notable ethnic Kono include the current vice president of Sierra Leone Alhaji Samuel Sam-Sumana and the country's current first lady Sia Koroma, the daughter of former Sierra Leone Attorney General Abu Aiah Koroma.

After the Kono, are the Mandingo (also known as Mandinka) at (7.4%) (they are the descendants of the Mandinka traders from Guinea who immigrated to Sierra Leone between 1840 to about 1898). The Mandinka predominantly fround in the east and the northern part of the country, and they are the largest inhabitant of the large towns, most notably Kabal and Falaba in [[Koinadugu District in the north and Yengema, Kono District in the east of the country. Historically, the Mandinka intense rivals have been the Temne and Limba. Sierra Leone's third president Ahmad Tejan Kabbah is an ethnic Mandingo.

After the Mandinka, are the Fula at (6%) (descendants of 17th- and 18th-century Fulani settlers from the Fouta Djalon region of Guinea) they live primarily in the north and the Western Area of Sierra Leone. Some notable ethnic Fula include the country current chief justice Umu Hawa Tejan Jalloh

Behind the Fula, are the Creole (at 5%) (descendants of freed West Indians slaves from the West Indies and freed African American slaves from the United States which landed in Freetown between 1787 and about 1885) are primarily found in the capital city of Freetown and its surrounding Western Area. Creole culture is unlike that of all other ethnic groups in Sierra Leone, and it is typical of Western culture and ideals. Former Sierrer Leone's Head of State Valentine Strasser is an ethnic Creole.

Much smaller ethnic groups are the Kuranko, in the north; the Loko in the north, with the Susu and Yalunka in the far north in Kambia District around the border with Guinea. The Kissi ) and the much smaller group of Vai are further inland in Kailahun District in the East next to the border with Liberia. On the coast in Bonthe District in the south are the Sherbro.

In the past, Sierra Leoneans were noted for their educational achievements, trading activity, entrepreneurial skills, and arts and crafts work, particularly wood carving. Many are part of larger ethnic networks extending into several countries, which link West African states in the area. But the level of education and infrastructure has declined sharply over the last 30 years.[64]

List of Sierra Leoneans
[edit] Education
Second grade class in Koidu Town.
Main article: Education in Sierra Leone

Education in Sierra Leone is legally required for all children for six years at primary level (Class P1-P6) and three years in junior secondary education,[65] but a shortage of schools and teachers has made implementation impossible.[66] Two thirds of the adult population of the country are illiterate.[67] The Sierra Leone Civil War resulted in the destruction of 1,270 primary schools and in 2001 67 percent of all school-age children were out of school.[66] The situation has improved considerably since then with primary school enrollment doubling between 2001 and 2005 and the reconstruction of many schools since the end of the war.[68] Students at primary schools are usually 6 to 12 years old, and in secondary schools 13 to 18. Primary education is free and compulsory in government-sponsored public schools.

The country has three universities, the University of Sierra Leone, founded as Fourah Bay College in 1827 (the oldest university in West Africa),[69] and Njala University, primarily located in Bo District, which was established as the Njala Agricultural Experimental Station in 1910 and became a university in 2005.[70] Teacher training colleges and religious seminaries are found in many parts of the country.
[edit] Health
Main article: Healthcare in Sierra Leone
The Kailahun Government Hospital at its reopening in 2004. It is the main hospital that serves Kailahun District.

Healthcare is provided by the government, among others. All medical care is generally charged for in Sierra Leone[71] The country has a very high infant mortality and a very low life expectancy. The maternal death rates are also the highest in the world, at 2,000 deaths per 100,000 live births. The country suffers from epidemic outbreaks of diseases including yellow fever, cholera, lassa fever and meningitis.[72] The prevalence of HIV/AIDS in the population is 1.6 percent, higher than the world average of 1 percent but lower than the average of 6.1 percent in Sub-Saharan Africa.[73]
[edit] Military
Main article: Military of Sierra Leone

The Military of Sierra Leone, officially the Republic of Sierra Leone Armed Forces (RSLAF), are the unified armed forces of Sierra Leone responsible for the territorial security of Sierra Leone's border and defending the national interests of Sierra Leone within the framework of its international obligations. The armed forces were formed after independence in 1961, on the basis of elements of the former British Royal West African Frontier Force present in the country. The Sierra Leone Armed Forces currently consist of around 15,500 personnel, comprising the largest Sierra Leone Army,[74] the Sierra Leone Navy and the Sierra Leone Air Wing [75]. The president of Sierra Leone is the Commander in Chief of the military, with the Minister of Defence responsible for defense policy and the formulation of the armed forces. The current Sierra Leone Defense Minister is Ret. Major Alfred Paolo Conteh. The Military of Sierra Leone also has a Chief of the Defence Staff who is a uniformed military official responsible for the administration and the operational control of the Sierra Leone military [76]. Brigadier General Alfred Nelson-Williams who was appointed by president Koroma succeeded the retired Major General Edward Sam M’boma on 12 September 2008 as the Chief of Defense Staff of the Military.[77]

Before Sierra Leone gained independence in 1961 the military was known as the Royal Sierra Leone Military Force. The military seized control in 1968, bringing the National Reformation Council into power. On 19 April 1971, when Sierra Leone became a republic, the Royal Sierra Leone Military Forces were renamed the Republic of Sierra Leone Military Force (RSLMF).[78] The RSLMF remained a single service organization until 1979, when the Sierra Leone Navy was established. It then remained largely unchanged for 16 years until in 1995 when Defence Headquarters was established and the Sierra Leone Air Wing formed. This gave the need for the RSLMF to be renamed the Armed Forces of the Republic of Sierra Leone (AFRSL).
[edit] Law enforcement

Law enforcement in Sierra Leone is primarily the responsibility of the Sierra Leone Police (SLP). Sierra Leone Police was established by the British colony back in 1894 and is one of the oldest police forces in West Africa. The key mission of the Sierra Leone Police include to prevent crime, to protect life and property, to detect and prosecute offenders, to maintain public order, to ensure safety and security, to enhance access to justice. The Sierra Leone Police is headed by the Inspector General of Police, the professional head of the Sierra Leone Police force and is appointed by the President of Sierra Leone. Each one of Sierra Leone's 14 districts is headed by a District Police commissioner who is the professional head of their respective district. The Districts Police Commissioners report directly to the Inspector General of Police at the Sierra Leone Police headquarters in Freetown. The current Inspector General of Police is Brima Acha Kamara who was appointed to the position by former president Ahmad Tejan Kabbah.
[edit] Media
Main article: Media in Sierra Leone
Radio listener in Kailahun

Media in Sierra Leone began with the introduction of the first printing press in Africa at the start of the nineteenth century. A strong journalistic tradition developed with the creation of a number of newspapers. In the 1860s, the country became a journalist hub for Africa, with professionals travelling to the country from across the continent. At the end of the nineteenth century, the industry went into decline, and when radio was introduced in the 1930s, it became the primary communication media in the country. The Sierra Leone Broadcasting Service (SLBS) was created by the government in 1934 making it the earliest English language radio broadcaster service in West Africa. The service began broadcasting television in 1963, with coverage extended to all the districts in the country in 1978.

Print media is not widely read in Sierra Leone, especially outside Freetown, partially due to the low levels of literacy in the country.[79] In 2007 there were 15 daily newspapers in the country, as well as those published weekly.[80] Among newspaper readership, young people are likely to read newspapers weekly and older people daily. The majority of newspapers are privately run and are often critical of the government. The standard of print journalism tends to be low due to lack of training, and people trust the information published in newspapers less than that found on the radio.[79]
Isata Mahoi shown editing radio programmes in Talking Drum studio Freetown, she is also an actress in Sierra Leone radio soap opera Atunda Ayenda

Radio is the most-popular and most-trusted media in Sierra Leone, with 85% of people having access to a radio and 72% of people in the country listening to the radio daily.[79] These levels do vary between areas of the country, with the Western Area having the highest levels and Kailahun the lowest. Stations mainly consist of local commercial stations with a limited broadcast range, combined with a few stations with national coverage. The United Nations Mission in Sierra Leone (UNIOSIL) runs one of the most popular stations in the country, broadcasting programs in a range of languages. Content includes news of UN activities and human rights information, as well as music and news. The UN missions will withdraw in 2008 and the UN Radio's future is uncertain. There is also a government station run by the SLBS that transmits on FM and short-wave. FM relays of BBC World Service, Radio France Internationale and Voice of America are also broadcast.

Outside the capital Freetown television is not watched by a great many people. There are two national, free terrestrial television stations in Sierra Leone, one run by the government SLBS and the other a private station, ABC Television-Africa (ABC). In 2007, a pay-per-view service was also introduced by GTV as part of a pan-African television service in addition to the nine year old sub-saharan Digital satellite television service (DStv) originating from Multichoice Africa in South Africa. Internet access in Sierra Leone has been sparse but is on the increase, especially since the introduction of wireless services across the country. There are nine Internet Service Providers (ISPs) operating in the country. Freetown has a city wide wireless network and Internet cafes and other businesses offering internet access. Problems experienced with access to the Internet include an intermittent electricity supply and a slow connection speed in the country outside Freetown.

The Sierra Leone constitution guarantees freedom of speech, and freedom of the press; however, the government maintains strong control of media, and at times restricts these rights in practice. Some subjects are seen as taboo by society and members of the political elite; imprisonment and violence have been used by the political establishment against journalists.[81][82] Under legislation enacted in 1980, all newspapers must register with the Ministry of Information and pay sizable registration fees. The Criminal Libel Law, including Seditious Libel Law of 1965, is used to control what is published in the media.[83] In 2006, President Ahmad Tejan Kabbah committed to reforming the laws governing the press and media to create a freer system for journalists to work in,[83] but in 2007, Sierra Leone was ranked as having the 121st least-free press in the world, with the press less-free, in comparison to other countries, than in 2006.[84]
[edit] Music of Sierra Leone
Main article: Music of Sierra Leone

see also:Palm-wine music, Gumbe, Afropop
[edit] Transportation
Main article: Transport in Sierra Leone

There are a number of systems of transport in Sierra Leone, which has a road, air and water infrastructure, including a network of highways and several airports.
[edit] Air

There are ten regional airports in Sierra Leone, and one international airport. The Lungi International Airport located in the coastal town of Lungi in Northern Sierra Leone is the primary airport for domestic and international travel to or from Sierra Leone. Passengers cross the river to Aberdeen Heliports in Freetown by hovercraft, ferry or a helicopter. Helicopters are also available from the airport to other major cities in the country. The airport has paved runways longer than 3,047m. The other airports have unpaved runways, and seven have runways 914 to 1,523 metres long; the remaining two have shorter runways.
[edit] Prohibition from E.U. air operations

This country appears on the E.U. list of prohibited countries with regard to the certification of airlines. This means that no airline which is Sierra Leone registered may operate services of any kind within the European Union. This is due to substandard safety standards.[85]
[edit] Water

Sierra Leone has the third largest natural harbor in the world, where international shipping berth at the Queen Elizabeth II Quay in Government Wharf in central Freetown. There are 800 km of waterways in Sierra Leone, of which 600 km are navigable year-round. Major port cities are Bonthe, Freetown, Sherbro Island and Pepel.
[edit] Highways

There are 11,700 kilometers (about 7,270 Miles) of highways in Sierra Leone, of which 936 km (about 581 miles) are paved (about 8% of the roads). Sierra Leone highways are linked to Conakry, Guinea, and Monrovia, Liberia.
[edit] Sports
Main article: Sport in Sierra Leone
[edit] Football
Main article: Football in Sierra Leone
Sierra Leonean football star Sheriff Suma just after a Leone Stars training session on 4 Sept. 2008 at the National Stadium in Freetown.

Football (soccer) is by far the most popular sport in Sierra Leone. The national football team, popularly known as the Leone Stars, represents the country in international competitions. It has never qualified for the FIFA World Cup but participated in the 1994 and 1996 African Cup of Nations. The country's national television network, The Sierra Leone Broadcasting Service (SLBS) broadcasts the live match, along with several radio stations throughout the country. Some well known Sierra Leonean footballers include the team captain Mohamed Kallon, Julius Gibrilla Woobay, Al Bangura, Paul Kpaka, Rodney Strasser, Ahmed Deen, Samuel Barlay, Kewullay Conteh Albert Jarrett and Kei Kamara

The Sierra Leone National Premier League is the top football league, controlled by the Sierra Leone Football Association. The two biggest and most successful football clubs are East End Lions and Mighty Blackpool, but Kallon F.C. has enjoyed contemporary success. Kallon F.C. won the Premier League and the Sierra Leonean FA Cup in 2006, and eliminated 2006 Nigerian Premier League Champions Ocean Boys FC in the 2007 CAF Champions League first qualifying round, but later lost to ASEC Mimosas of Ivory Coast in the second qualifying round for the group stage.

The Sierra Leone U-17 football team, nicknamed the Sierra Stars, finished as runner-up at the 2003 African U-17 Championship in Swaziland, but came in last place in their group at the 2003 FIFA U-17 World Championship in Finland.
[edit] Cricket
Main article: Sierra Leone national cricket team

The Sierra Leone cricket team represents Sierra Leone in international cricket competitions, and is among the best in West Africa. It became an affiliate member of the International Cricket Council in 2002. It made its international debut at the 2004 African Affiliates Championship, where it finished last of eight teams. But at the equivalent tournament in 2006, Division Three of the African region of the World Cricket League, it finished as runner-up to Mozambique, and just missed a promotion to Division Two.
[edit] Basketball
Main article: Sierra Leone national basketball team

The Sierra Leone national basketball team represents Sierra Leone in international men's basketball competitions and is controlled by the Sierra Leone Basketball Federation. The squad is mostly home-based, with a few foreign players.

Dakar
From Wikipedia, the free encyclopedia
Jump to:navigation, search
For the rally, see Dakar Rally. For the Israeli submarine, see INS Dakar.
Dakar
Ville de Dakar
—  City  —
Skyline of Central Dakar

Coat of arms
City of Dakar, divided into 19 communes d'arrondissement
Dakar is located in Senegal
Dakar
Location within Senegal
Coordinates: 14°41′34″N 17°26′48″W﻿ / ﻿14.69278°N 17.44667°W﻿ / 14.69278; -17.44667Coordinates: 14°41′34″N 17°26′48″W﻿ / ﻿14.69278°N 17.44667°W﻿ / 14.69278; -17.44667
Country 	 Senegal
Région 	Dakar
Département 	Dakar
Settled 	15th century
Communes d'arrondissement 	
19[show]
Cambérène
Parcelles Assaines
Pattie d'Oies
Hann Bel-Air
Dieuppeul Derklé
HLM
Biscuiterie
Grand Dakar
Plateau
Médina
Fass-Gueule Tapée-Colobane
Fann Point-E
Mermoz-Sacré-Coeur
Ouakam
Yoff
Ngor
Liberté
Grand-Yoff
Cape Verde Peninsula
Government
 - Mayor 	Khalifa Sall (2009)[1] (BSS/PS)
 - Regional president 	Abdoulaye Wade (since 2002)
Area [2]
 - City 	82.38 km2 (31.8 sq mi)
 - Metro 	547 km2 (211.2 sq mi)
Population (December 31, 2005 estimate)[3]
 - City 	1,030,594
 - Density 	12,510/km2 (32,400.8/sq mi)
 - Metro 	2,452,656
 - Metro Density 	4,484/km2 (11,613.5/sq mi)
  	Data here are for the administrative Dakar région, which matches almost exactly the limits of the metropolitan area
Time zone 	GMT (UTC+0)
Website 	http://www.dakarville.sn
Not to be confused with Dhaka.

Dakar is the capital city of Senegal, located on the Cap-Vert Peninsula, on the country's Atlantic coast. It is Senegal's largest city. Its position, on the western edge of Africa (it is the westernmost city on the African mainland), is an advantageous departure point for trans-Atlantic and European trade; this fact aided its growth into a major regional port.

According to December 31, 2005 official estimates, the city of Dakar proper has a population of 1,030,594, whereas the population of the Dakar metropolitan area is estimated at 2.45 million people.[3]

Dakar is a major administrative centre, home to the National Assembly of Senegal and Senegal's President's Palace.
Contents
[hide]

    * 1 History
    * 2 Geography and climate
    * 3 Administration
    * 4 Notable places
    * 5 Notable natives and residents
    * 6 International relations
          o 6.1 Twin towns — Sister cities
    * 7 See also
    * 8 Notes
    * 9 External links

[edit] History
Dakar in 1850.
Dakar in 1888.

The Cape Verde Peninsula was settled, no later than the 15th century, by the Lebou, an ethnic group related to the neighboring Wolof and Sereer. The original villages: Ouakam, Ngor, Yoff and Hann, still constitute distinctively Lebou neighborhoods of the city today. Meanwhile, in 1444, the Portuguese arrived on the island of Gorée and founded a settlement there. By 1536, they had begun using it as a base for the export of slaves. The mainland of Cap-Vert, however, was under control of the Jolof Empire, as part of the western province of Cayor which seceded from Jolof in its own right in 1549. A new Lebou village, called Ndakaaru, was established directly across from Gorée in the 17th century to service the European trading factory with food and drinking water. Gorée was captured by the United Netherlands in 1588, which gave it its present name (spelled Goeree, after Goeree-Overflakkee in Holland). The island was to switch hands between the Portuguese and Dutch several more times before falling to the English under Admiral Robert Holmes on January 23, 1664, and finally to the French in 1677. Though under continuous French administration since, Métis families, descendant from Dutch and French traders and African wives, dominated the slave trade. The infamous "House of Slaves" was built here in 1776.

In 1795 the Lebou of Cape Verde revolted against Cayor rule. A new theocratic state, subsequently called the "Lebou Republic" by the French, was established under the leadership of the Diop, a Muslim clerical family originally from Koki in Cayor. The capital of the republic was established at Ndakaaru. In 1857 the French established a military post at Ndakaaru (which they called "Dakar") and annexed the Lebou Republic, though its institutions continued to function nominally. The Serigne (also spelled Sëriñ, "Lord") of Ndakaaru is still recognized as the traditional political authority of the Lebou by the Senegalese State today.

The slave trade was abolished by France in February 1794. However, Napoleon reinstated it in May 1802, then finally abolished it permanently in March 1815. Despite Napoleon's abolition, a clandestine slave trade continued at Gorée until 1848, when it was abolished throughout all French territories. To replace trade in slaves, the French promoted peanut cultivation on the mainland. As the peanut trade boomed, tiny Gorée Island, whose population had grown to 6,000 residents, proved ineffectual as a port. Traders from Gorée decided to move to the continent and a "factory" with warehouses was established in Rufisque in 1840.

Large public expenditure for infrastructure was allocated by the colonial authorities to Dakar's development. The port facilities were improved with jetties, a telegraph line was established along the coast to Saint-Louis and the Dakar-Saint-Louis railway was completed in 1885, at which point the city became an important base for the conquest of the western Sudan.

Gorée, including Dakar, was recognised as a French commune in 1872. Dakar itself was split off from Gorée as a separate commune in 1887. The citizens of the city elected their own mayor and municipal council and helped send an elected representative to the National Assembly in Paris. Dakar replaced Saint-Louis as the capital of French West Africa in 1902. A second major railroad, the Dakar-Niger built from 1906–1923, linked Dakar to Bamako and consolidated the city's position at the head of France's West African empire. In 1929, the commune of Gorée Island, now with only a few hundred inhabitants, was merged into Dakar.

Urbanization during the colonial period was marked by forms of racial and social segregation—often expressed in terms of health and hygiene—which continue to structure the city today. Following a plague epidemic in 1914, the authorities forced most of the African population out of old neighborhoods, or "Plateau", and into a new quarter, called Médina, separated from it by a "sanitary cordon". As first occupants of the land, the Lebou inhabitants of the city successfully resisted this expropriation. They were supported by Blaise Diagne, the first African to be elected Deputy to the National Assembly. Nonetheless, the Plateau thereafter became an administrative, commercial, and residential district increasingly reserved for Europeans and served as model for similar exclusionary administrative enclaves in French Africa's other colonial capitals (Bamako, Conakry, Abidjan, Brazzaville). Meanwhile, the Layene Sufi order, established by Seydina Mouhammadou Limamou Laye, was thriving among the Lebou in Yoff and in a new village called Cambérène. Since independence, urbanization has sprawled eastward past Pikine, a commuter suburb whose population (2001 est. 1,200,000) is greater than that of Dakar proper, to Rufisque, creating a conurbation of almost 3 million (over a quarter of the national population).

In its colonial heyday Dakar was one of the major cities of the French Empire, comparable to Hanoi or Beirut. French trading firms established branch offices there and industrial investments (mills, breweries, refineries, canneries) were attracted by its port and rail facilities. It was also strategically important to France, which maintained an important naval base and coaling station in its harbor and which integrated it into its earliest air force and airmail circuits, most notably with the legendary Mermoz airfield (no longer extant).

During the Battle of Dakar, which took place off the coast of Dakar on September 23, 1940 – September 25, 1940, the British navy attempted to rally the colonial administration in Dakar to the Allied cause and detach it from Vichy. In November 1944 West African conscripts of the French army mutinied against poor conditions at the Thiaroye camp, on the outskirts of the city. The mutiny was seen as an indictment of the colonial system and constituted a watershed for the nationalist movement.

Dakar was the capital of the short-lived Mali Federation from 1959 to 1960, after which it became the capital of Senegal.

Dakar is a major financial center, home to a dozen national and regional banks (including the BCEAO which manages the unified West African CFA currency), and to numerous international organizations, NGOs and international research centers. Dakar has a large Lebanese community (concentrated in the import-export sector) that dates to the 1920s, a community of Moroccan business people, as well as Mauritanian, Cape Verdean, and Guinean communities. The city is home to as many as 20,000 French expatriates. France still maintains an air force base at Yoff and the French fleet is serviced in Dakar's port.

Beginning 1978, Dakar has frequently been the ending point of the Dakar Rally. The rally has brought worldwide attention to the poverty of Senegal and Dakar.[citation needed]
[edit] Geography and climate

Dakar has a hot semi-arid climate (Köppen climate classification BSh), with a short rainy season and a lengthy dry season. Dakar's rainy season lasts from July to October while the dry season covers the remaining eight months. The city sees approximately 540 mm of precipitation per year.

Dakar between December and April is usually pleasantly warm. Nights during this time of the year are comfortable. Between May and November, the city becomes decidedly warmer. However, it's not quite as hot in Dakar as it is in African cities located further inland such as Niamey and N'Djamena.


Climate data for Dakar [hide]Month 	Jan 	Feb 	Mar 	Apr 	May 	Jun 	Jul 	Aug 	Sep 	Oct 	Nov 	Dec 	Year
Record high °C (°F) 	39
(102) 	38
(100) 	43
(109) 	38
(100) 	38
(100) 	38
(100) 	37
(99) 	37
(99) 	38
(100) 	38
(100) 	37
(99) 	35
(95) 	43
(109)
Average high °C (°F) 	26
(79) 	27
(81) 	27
(81) 	27
(81) 	29
(84) 	31
(88) 	31
(88) 	31
(88) 	32
(90) 	32
(90) 	30
(86) 	27
(81) 	29
(84)
Average low °C (°F) 	18
(64) 	17
(63) 	18
(64) 	18
(64) 	20
(68) 	23
(73) 	24
(75) 	24
(75) 	24
(75) 	24
(75) 	23
(73) 	19
(66) 	21
(70)
Record low °C (°F) 	13
(55) 	14
(57) 	15
(59) 	16
(61) 	16
(61) 	18
(64) 	21
(70) 	21
(70) 	21
(70) 	21
(70) 	18
(64) 	12
(54) 	12
(54)
Precipitation mm (inches) 	0
(0) 	0
(0) 	0
(0) 	0
(0) 	0
(0) 	18
(0.71) 	89
(3.5) 	254
(10) 	132
(5.2) 	38
(1.5) 	3
(0.12) 	8
(0.31) 	542
(21.34)
Source: BBC Weather [4] 2009-08-23
[edit] Administration
Market street in the working class Gueule Tapée quarter

The city of Dakar is a commune, (also sometimes known as commune de ville), one of the 67 communes of Senegal. The commune of Dakar was created by the French colonial administration on June 17, 1887 by detaching it from the commune of Gorée. The commune of Gorée, created in 1872, was itself one of the oldest Western-style municipalities in Africa (along with the municipalities of Algeria and South Africa).

The commune of Dakar has been in continuous existence since 1887, being preserved by the new state of Senegal after independence in 1960, although its limits have varied considerably over time. The limits of the commune of Dakar have been unchanged since 1983. The commune of Dakar is ruled by a democratically elected municipal council (conseil municipal) serving five years, and a mayor elected by the municipal council. There have been 20 mayors in Dakar since 1887. The first black mayor was Blaise Diagne, mayor of Dakar from 1924 to 1934. The longest serving mayor was Mamadou Diop, mayor for 18 years between 1984 and 2002.

The commune of Dakar is also a département, one of the 34 départements of Senegal. This situation is quite similar to Paris, which is both a commune and a département. However, contrary to French départements, départements in Senegal have no political power (no departmental assembly), and are merely local administrative structures of the central state, in charge of carrying out some administrative services as well as controlling the activities of the communes within the département.

The département of Dakar is divided into four arrondissements: Almadies, Grand Dakar, Parcelles Assainies (which literally means "drained lots"; this is the most populous arrondissement of Dakar), and Plateau/Gorée (downtown Dakar). These arrondissements are quite different from the arrondissements of Paris, being merely local administrative structures of the central state, like the Senegalese départements, and are thus more comparable to French departmental arrondissements.
Residential street in the upscale Mermoz quarter
The Assemblée nationale on the Plateau, the heart of old Dakar

In 1996 a massive reform of the administrative and political divisions of Senegal was voted by the Parliament of Senegal. The commune of Dakar, whose population approached 1 million inhabitants, was deemed too large and too populated to be properly managed by a central municipality, and thus on August 30, 1996 Dakar was divided into 19 communes d'arrondissement. These communes d'arrondissement were given extensive powers, and are very much like regular communes. They have more powers than the arrondissements of Paris, and are more akin to the London boroughs. The commune of Dakar was maintained above these 19 communes d'arrondissement, and it coordinates the activities of the communes d'arrondissement, much as Greater London coordinates the activities of the London boroughs. The 19 communes d'arrondissement belong to either of the four arrondissements of Dakar, and the sous-préfet of each arrondissement is in charge of controlling the activities of the communes d'arrondissement in his arrondissement.

The commune d'arrondissement of Dakar-Plateau (34,626 inhabitants), in the arrondissement of Plateau/Gorée, is the historical heart of the city, and most ministries and public administrations are located there. The densest and most populous commune d'arrondissement is Médina (136,697 inhabitants), in the arrondissement of Plateau/Gorée. The commune d'arrondissement of Yoff (55,995 inhabitants), in the arrondissement of Almadies, is the largest one, while the smallest one is the commune d'arrondissement of Île de Gorée (1,034 inhabitants), in the arrondissement of Plateau/Gorée.

The département of Dakar is one of the four départements of the Dakar région, which is one of the 11 régions of Senegal. The Dakar région encompasses the city of Dakar and all its suburbs along the Cape Verde Peninsula. Its territory is thus roughly the same as the territory of the metropolitan area of Dakar. Since the administrative reforms of 1996, the régions of Senegal, which until then were merely local administrative structures of the central state, have been turned into full-fledged political units, with democratically elected regional councils, and regional presidents. They were given extensive powers, and manage economic development, transportation, or environmental protection issues at the regional level, thus coordinating the actions of the communes below them.

Following the political transition of 2000 when Abdoulaye Wade, leader of the opposition (Senegalese Democratic Party, or PDS), defeated President Abdou Diouf (Socialist Party of Senegal), local elections were held in 2002. Two leaders of the PDS, Pape Diop and Abdoulaye Wade, ambitioned to become mayor of Dakar. Eventually, a compromise was found: Pape Diop would run for the municipal election of Dakar, while Abdoulaye Wade would run for the regional election of Dakar. The local elections of Senegal were held on May 12, 2002, and saw the PDS largely defeating the Socialists. Pape Diop was elected mayor of Dakar, defeating the long time Socialist mayor Mamadou Diop, while Abdoulaye Wade was elected president of the regional council of the Dakar région, defeating the Socialists who hitherto controlled the région.
[edit] Notable places
The Dakar Railway Station
The Dakar Cathedral

Attractions in Dakar include major markets, Dakar Grand Mosque (built in 1964), Dakar Cathedral, Gorée Island, the IFAN Museum of West African culture, the newly completed African Renaissance Monument, clifftop walks and beaches, and Hann Park, home to Senegal Zoo.

The town serves as a port and is home to the Léopold Sédar Senghor International Airport. It is also the terminus of the Dakar-Niger railroad line.

Dakar used to be the finishing point of the Dakar Rally and is a member of the Organization of World Heritage Cities. Cheikh Anta Diop University also known as the University of Dakar, was established in 1957.
[edit] Notable natives and residents

    * Youssou N'Dour, singer and percussionist
    * Thione Seck, Singer and songwriter
    * Baaba Maal, singer and guitarist
    * Assane Ndiaye, Singer and songwriter, Cousin of Thione Seck
    * Baobab Orchestra,
    * Ségolène Royal, French politician born in Dakar
    * Akon, R&B singer, Real name - Alioune Thiam
    * Issa, R&B singer
    * Ousmane Barro, basketball player, Marquette University
    * Patrick Vieira, footballer, Manchester City
    * Ibrahim Ba, former footballer
    * DeSagana Diop, basketball player, Charlotte Bobcats
    * Papa Bouba Diop, footballer, Portsmouth F.C.
    * Boris Diaw, basketball player, Charlotte Bobcats
    * Cheikh Samb, basketball player, former Los Angeles Clippers
    * Patrice Evra, footballer, Manchester United
    * Macoumba Kandji, footballer, New York Red Bulls
    * Almamy, pop singer
    * Abdoulaye Diagne-Faye, footballer, Stoke City
    * Abdoulaye Salam Fall, Founder Seneweb.com
    * Mame Biram Diouf, footballer, Manchester United

	
	
	Depth charge
From Wikipedia, the free encyclopedia
Jump to:navigation, search
For other uses, see Depth charge (disambiguation).
Question book-new.svg
	This article needs additional citations for verification.
Please help improve this article by adding reliable references. Unsourced material may be challenged and removed. (September 2008)
Mark IX Depth Charge used by U.S. Navy later in World War II. Unlike the cylindrical, barrel-shaped depth charge used earlier, the Mark IX is streamlined and equipped with canted fins to impart rotation on the depth charge, allowing it to fall in a straight trajectory with less chance of drifting off target. This type of depth charge contained 200 pounds (90 kg) of Torpex.

The depth charge is an anti-submarine weapon intended to defeat its target by the shock of exploding near it. Most use explosives and a fuze set to go off at a predetermined depth. Some have been designed to use nuclear warheads. Depth charges can be deployed by both ships and aircraft.
Contents
[hide]

    * 1 History
    * 2 Delivery mechanisms
    * 3 Effectiveness
          o 3.1 Pacific theater
    * 4 Later developments
    * 5 Underwater explosions
    * 6 See also
    * 7 References
    * 8 External links

[edit] History

The concept of a "dropping mine" was first discussed in 1910, and the idea was developed into practicality when the British Royal Navy’s Commander-in-Chief of the Home Fleets, Admiral Sir George Callaghan, requested its production in 1914.[citation needed] The design work was carried out by Herbert Taylor at HMS Vernon Torpedo and Mine School in Portsmouth, England. The first effective depth charge, the "Type D", became available in January 1916. These were barrel-like casings containing a high explosive, usually TNT or amatol. There were initially two sizes—a 300-pound (140 kg) charge for fast ships and a 120-pound (55 kg) charge for ships too slow to clear the danger area of the more powerful charge.[1]

A hydrostatic pistol actuated by water pressure at a pre-selected depth detonated the charge.[1] Initial depth settings were 40 feet and 80 feet (12 and 24 meters.)[1] Anti-submarine vessels initially carried only two depth charges to be released from a chute at the stern of the ship.[1] The first success was the sinking of SM U-68 off Kerry, Ireland, on 22 March 1916 by the Q-ship Farnborough.[1] Germany became aware of the depth charge following unsuccessful attacks on U-67 on 15 April 1916 and U-69 on 20 April 1916.[1] UC-19 and UB-29 were the only other submarines sunk by depth charge during 1916.[1]

Numbers of depth charges carried per ship increased to 4 in June 1917, to 6 in August, and to 30 or 40 by 1918.[2] Improved pistols allowed greater depth settings in 50-foot (15-meter) increments to 200 feet (60 meters.)[2] Even the slower ships could safely use the 300-pound depth charge at the greater depths, so the relatively ineffective 120-pound depth charge was withdrawn from service.[2] Monthly use of depth charges increased from 100 to 300 per month during 1917 to an average of 1745 per month during the last 6 months of World War I.[2] The “Type D” could be detonated as deep as 300 feet (91.44 metres) by that date.

The depth charge was such a successful device that it attracted the attention of the United States, who requested full working drawings of the devices in March 1917. Having received them, Commander Fullinwider of the U.S. Bureau of Naval Ordnance and U.S. Navy engineer Minkler made some modifications and then patented it in the U.S. It has been argued[who?] this was done to avoid paying the original inventor.

The Royal Navy Type D depth charge was designated the Mark VII by 1939.[3] Initial sinking speed was 7 feet per second (2.1 m/s) with a terminal velocity of 9.9 feet per second (3 m/s) reached at a depth of 250 feet (76 m) if rolled off the stern, or upon water contact from a depth charge thrower.[3] Cast iron weights of 150 pounds (70 kg) were attached to the Mark VII at the end of 1940 to increase sinking velocity to 16.8 feet per second (5.1 m/s).[3] New hydrostatic fuzes increased the maximum detonation depth to 900 feet.[3] The Mark VII's 290 pound (130 kg) Amatol charge was estimated capable of splitting a 7/8-inch (22 mm) submarine pressure hull at a distance of 20 feet (6.1 m), and forcing the submarine to surface at twice that distance.[3] Change of explosive to Torpex (or Minol) at the end of 1942 was estimated to increase those distances to 26 feet and 52 feet (7.9 m and 15.8 m).[3]

The British Mark X depth charge weighed 3000 pounds (1400 kg) and was launched from 21-inch (53 cm) torpedo tubes of older destroyers to achieve a sinking velocity of 21 feet per second (6.4 m/s).[3] The launching ship needed to clear the area at 11 knots to avoid damage, and the charge was seldom used.[3]

The tear-drop-shaped United States Mark 9 depth charge entered service in the spring of 1943.[4] The charge was 200 pounds (91 kg) Torpex with a sinking speed of 14.4 feet per second (4.4 m/s) and depth settings up to 600 feet.[4] Later versions increased depth to 1000 feet (300 m) and sinking speed was increased to 22.7 feet per second (6.9 m/s) with increased weight and improved streamlining.[4]

Although the explosions of the standard United States 600-pound (270 kg) Mark 4 or Mark 7 depth charge used in World War II were nerve-wracking to the target, an undamaged U-boat’s pressure hull would not rupture unless the charge detonated closer than about five meters. Placing the weapon within this range was entirely a matter of chance and quite unlikely as the target maneuvered evasively during the attack. Most U-boats sunk by depth charges were destroyed by damage accumulated from a long barrage rather than by a single carefully-aimed attack. Many survived hundreds of depth charge detonations over a period of many hours; U-427 survived 678 depth charge blasts aimed at her in April 1945, though many may have detonated a considerable distance from the target.
[edit] Delivery mechanisms
Loading a drum-type Mark VII depth charge onto the K-gun of the Flower class corvette HMS Dianthus.

The first delivery mechanism was to simply roll the "ashcans" off racks at the stern of the attacking vessel. Originally depth charges were simply placed at the top of a ramp and allowed to let roll. Improved racks, which could hold several depth charges and release them remotely with a trigger, were developed towards the end of the First World War. These racks remained in use throughout World War II, because they were simple and easy to reload.

Some Royal Navy trawlers used for anti-submarine work during 1917–1918 had a thrower on the forecastle for a single depth charge, but there do not seem to be any records of it being used in action.[2] Specialized depth charge projectors were developed to generate a wider dispersal pattern when used in conjunction with rack-deployed charges.[2] The first of these projectors could throw a charge 40 yards (40 m) and became operational in August 1917.[2] Projectors called Y-guns (in reference to their basic shape) became available in 1918. Mounted on the centerline of the ship with the arms of the "Y" pointing towards the sides of the ship, a depth charge was cradled on a shuttle inserted into each arm. An explosive propellant charge was detonated in the vertical column of the Y-gun to propel a depth charge about 150 feet (50 meters) over each side of the ship. The main disadvantage of the Y-gun is that it must be mounted on the centerline of a ship's deck, which may otherwise be occupied by superstructure, masts, or gun turrets.
A depth charge explodes after it had been dropped from HMS Ceylon.

The K-gun, made standard in 1942, replaced the Y-gun as the primary depth charge projector. K-guns could be mounted on the periphery of a ship's deck, thus freeing up valuable centerline space. The K-guns were often used together with stern racks to create patterns of six to ten charges. In all cases, the attacking ship needed to be moving above a certain speed or it would be damaged by the force of its own weapons.

Depth-charges can also be dropped from an attacking aircraft against submarines. At the start of World War II, Britain's aerial anti-submarine weapon was the 100 lb (45 kg) anti-submarine bomb. This weapon was too light and ultimately, a failure. Indeed, on September 5, 1939, a Royal Air Force Avro Anson of 233 squadron was destroyed when its own A/S bomb skipped off the surface of the water and detonated under the aircraft.[citation needed] To remedy the failure of this weapon, the Royal Navy's 450 lb (200 kg) Mark VII depth charge was modified for aerial use by the addition of a streamlined nose fairing and stabilising fins on the tail.

The first to use depth charges on airplanes in actual combat were the Finns, though. Experiencing the same problems as RAF with insufficient charges on anti-submarine bombs, Captain Birger Ek of Finnish Air Force squadron LeLv 6 contacted one of his Navy friends and suggested testing of using aerial use of standard Finnish Navy depth charges. The tests proved successful, and the Tupolev SB-2 bombers of LeLv 6 were modified in early 1942 to carry depth charges. The success of the anti-submarine missions reached also the RAF Coastal Command, which promptly began modifying depth charges for aerial use.[citation needed]

Later depth charges would be developed specifically for aerial use. Such weapons still have utility today and are in limited use, particularly for shallow-water situations where a homing torpedo may not be suitable. Depth charges are especially useful for "flushing the prey" in the event of a diesel submarine lying on the bottom or otherwise hiding, with all machinery shut down. Homing torpedoes can be used for the same purpose, but the cost is prohibitive and aircraft and shipboard inventories limited. An example of such a weapon is the BAE Systems Mark 11, deployed by the British Fleet Air Arm.
[edit] Effectiveness
To be effective depth charges had to be set to the correct depth. To ensure this, a pattern of charges set to different depths would be laid atop the submarine's suspected position.

The effective use of depth charges required the combined resources and skills of many individuals during an attack. Sonar, helm, depth charge crews and the movement of other ships had to be carefully coordinated. Aircraft depth charge tactics depended upon locating the sub during the day and at night, then quickly attacking once it had been located, as the sub would normally crash dive to escape attack.

As the Battle of the Atlantic wore on, British and Commonwealth forces became particularly adept at depth charge tactics, and formed some of the first destroyer hunter-killer groups to actively seek out and destroy German U-boats.

The shortcoming of the depth charge as deployed by surface ships was not the weapon itself, but how it was delivered. An attacking vessel would usually detect a submerged contact using its sonar (or in British parlance, ASDIC). However, to drop its depth charges it had to pass over the contact to drop them over the stern. As such, sonar contact would be lost immediately prior to attack, thus rendering the hunter blind at the crucial moment. A skillful submarine commander therefore had an opportunity to take successful evasive action. This situation would be remedied by the adoption of the ahead-throwing weapon, allowing contacts to be engaged at a stand-off distance while still in sonar contact.
[edit] Pacific theater

In the Pacific, Japanese depth charge attacks initially proved fairly unsuccessful against U.S. and British submarines. Unless caught in shallow water, a U.S. submarine commander could normally dive to a deeper depth in order to escape destruction.

The deficiencies of Japanese depth-charge tactics were revealed in a press conference held by U.S. Congressman Andrew J. May, a member of the House Military Affairs Committee who had visited the Pacific theater and received many intelligence and operational briefings. Incredibly, May mentioned the highly sensitive fact that American submarines had a high survivability rate because Japanese depth charges were fuzed to explode at too shallow a depth.

Various press associations sent this leaked news story over their wires, compounding the danger, and many newspapers (including one in Honolulu, Hawaii) published it. Soon, Japanese forces were resetting their depth charges to explode at a more effective average depth of 75 m (250 feet), to the detriment of American submariners. Vice Admiral Charles A. Lockwood, commander of the U.S. submarine fleet in the Pacific, later estimated that May's revelation cost the United States Navy as many as ten submarines and 800 seamen killed in action.[5]
[edit] Later developments

For the reasons expressed above, the depth charge was generally replaced as an anti-submarine weapon. Initially, this was by ahead-throwing weapons such as the British-developed Hedgehog and later Squid. These weapons threw a pattern of warheads ahead of the attacking vessel to bracket a submerged contact. Hedgehog was contact fuzed, but Squid fired small depth-charges with hydrostatic arming. Later developments included the Mark 24 "Fido" acoustic homing torpedo (and later such weapons) or the SUBROC, which was armed with a nuclear depth charge. The USSR, United States and United Kingdom developed anti-submarine weapons using nuclear warheads and these are sometimes referred to as Nuclear Depth Bombs (NDB).
[edit] Underwater explosions
USS Agerholm (DD-826) launched an ASROC anti-submarine rocket armed with a nuclear depth bomb during the Swordfish test of 1962

The high explosive in a depth charge undergoes a rapid chemical reaction at an approximate rate of 8,000 meters per second (25,000 ft/s). The gaseous products of that reaction momentarily occupy the volume previously occupied by the solid explosive, but at very high pressure. This pressure is the source of the damage and is proportional to the explosive density and the square of the detonation velocity. A depth charge gas bubble expands to reach the pressure of the surrounding water. This gas expansion propagates a shock wave. The density difference of the expanding gas bubble from the surrounding water causes the bubble to rise toward the surface. Unless the explosion is shallow enough to vent the gas bubble to the atmosphere during its initial expansion, the momentum of water moving away from the gas bubble will create a gaseous void of lower pressure than the surrounding water. Surrounding water pressure then collapses the gas bubble with inward momentum causing excess pressure within the gas bubble. Re-expansion of the gas bubble then propagates another potentially damaging shock wave. Cyclical expansion and contraction continues until the gas bubble vents to the atmosphere.[6] Consequently, explosions where the depth charge is detonated at a shallow depth and the gas bubble vents into the atmosphere very soon after the detonation are quite ineffective, even though they are more dramatic and therefore preferred in movies. A sign of an effective detonation depth is that the surface just slightly rises and only after a while vents into a water burst.

Very large depth charges, including nuclear weapons, may be detonated at sufficient depth to create multiple damaging shock waves. Very large depth charges may produce damage at distance where reflected shock waves from the ocean floor and/or ocean surface converge to amplify radial shock waves. Submarines or surface ships may be damaged if operating in convergence zones of their own depth charge detonations.[6]

The damage that an underwater explosion inflicts on a submarine comes from a primary and a secondary shock wave. The primary shock wave is the initial shock wave from the depth charge, and will cause damage to personnel and equipment inside the submarine if detonated close enough. The secondary shock wave is a result from the cyclical expansion and contraction of the gas bubble and will bend the submarine back and forth and cause catastrophic hull breach, in a way that can be best described as bending a plastic ruler back and forth until it snaps. Up to sixteen cycles of the secondary shock wave have been recorded in tests. The effect of the secondary shock wave can be reinforced if another depth charge detonates on the other side of the hull in a close proximity in time of the first detonation, which is why depth charges normally are launched in pairs with different pre-set detonation depths.

The killing radius of a depth charge depends on the payload of the depth charge and the size and strength of the submarine hull. A depth charge of approximately 100 kg of TNT (4 MJ) would normally have a killing radius (hull breach) of only 3–4 meters (10–13 ft) against a conventional 1,000-long-ton (1,000 t) submarine, while the disablement radius (where the submarine is not sunk but put out of commission) would be approximately 8–10 meters (26–33 ft). A higher payload only increases the radius by a few meters due to the fact that the effect of an underwater explosion decreases with the distance cubed. The killing range would be greater against a larger submarine and shorter against a smaller submarine. It is doubtful if the hull of a midget submarine with a titanium hull could be sunk by a depth charge by anything less than a direct hit, even though it could be decommissioned with less.

Brazilian cruiser Bahia
From Wikipedia, the free encyclopedia
Jump to:navigation, search
A sleek ship with two funnels is at sea; smoke is streaming to the right
Bahia sometime before her mid-1920s modernization, as indicated by her two funnels[1]
Career (Brazil) 	
Name: 	Bahia
Namesake: 	The Brazilian state of Bahia
Builder: 	Armstrong Whitworth[1][2]
Yard number: 	809[2]
Laid down: 	19 August 1907[1][2]
Launched: 	20 January 1909[2]
Commissioned: 	21 May 1910[3]
Fate: 	Sunk by an explosion, 4 July 1945[3][4]
General characteristics
Class and type: 	Bahia-class cruiser
Displacement: 	3,100 tonnes (3,050 LT; 3,420 ST) [1][3]
Length: 	122.38 m (401.5 ft) oa
115.82 m (380.0 ft) pp[3]
Beam: 	11.89–11.91 m (39.0–39.1 ft)[3]
Draft: 	

3.81 m (12.5 ft) forward[3]
4.75 m (15.6 ft) amidships[3]
4.42 m (14.5 ft) aft[3]
Propulsion: 	Five Parsons steam turbines,[5] ten Yarrow boilers[3]
Coal normal 150 t (148 LT; 165 ST)[5]
Maximum 650 t (640 LT; 717 ST)[5]
Speed: 	27.016 knots (50.034 km/h; 31.089 mph) trial[1]
25 knots (46 km/h; 29 mph) at full load[3]
Endurance: 	1,400 nautical miles (2,600 km; 1,600 mi) at 23.5 knots (43.5 km/h; 27.0 mph)[5]
3,500 nautical miles (6,500 km; 4,000 mi) at 10 knots (19 km/h; 12 mph)[5]
Complement: 	320[5] to 357[3]
Armament: 	ten × 120 mm (4.724 in)/50 caliber,[3]
six × 3 pdr 47 mm (1.85 in)/50 caliber,[1][3]
two × 457 mm (18.0 in) torpedo tubes[1]
Armor: 	

Deck: 19 mm (0.748 in)[1]
Conning tower: 76 mm (2.992 in)[1]
Notes: 	Specifications given are prior to the 1925–26 modernization.

Bahia was the lead ship of a two-vessel class of scout cruisers built for Brazil by Armstrong Whitworth in the United Kingdom. Six months after her commissioning (May 1910), crewmen aboard Bahia, Deodoro, Minas Geraes, and São Paulo mutinied, beginning the Revolta da Chibata (Revolt of the Whip). During the four-day rebellion, Brazil's capital city of Rio de Janeiro was held hostage by the possibility of a naval bombardment, leading the government to give in to the rebel demands, which included the abolition of flogging in the navy. During the First World War, Bahia and her sister ship Rio Grande do Sul were assigned to the Divisão Naval em Operações de Guerra (Naval Division in War Operations), the Brazilian Navy's main contribution in that conflict. Based out of Sierra Leone and Dakar, the squadron escorted convoys through an area believed to be heavily patrolled by U-boats.

In the mid-1920s, Bahia was extensively modernized. She received three new Brown–Curtis turbine engines and six new Thornycroft boilers, and, in the process, was converted from coal-burning to oil. The refit resulted in a striking aesthetic change, with the exhaust being trunked into three funnels instead of two. The armament was also modified; three 20.1 mm (0.79 in) Madsen guns, a 7 mm (0.28 in) Hotchkiss machine gun, and four 533 mm (21.0 in) torpedo tubes were added. In the 1930s, she served with government forces during multiple revolutions.

In the Second World War, Bahia was once again used as a convoy escort, sailing over 100,000 nautical miles (190,000 km; 120,000 mi) in the span of about a year. On 4 July 1945 she was acting as a plane guard for transport aircraft flying from the Atlantic to Pacific theaters of war. While Bahia's gunners were firing at a kite for anti-aircraft practice, one aimed too low and hit depth charges stored near the stern of the ship, resulting in a massive explosion that incapacitated the ship and sank her within minutes. Only a small portion of the crew survived the blast, and even fewer were still living when their rafts were discovered days later.
Contents
[hide]

    * 1 Construction and commissioning
    * 2 Mutiny
    * 3 First World War
    * 4 Modernization and inter-war years
    * 5 Second World War
          o 5.1 Loss
    * 6 See also
    * 7 Notes
    * 8 References
    * 9 Bibliography

[edit] Construction and commissioning

Bahia was part of a large 1904 naval building program by Brazil.[6][7] Also planned as part of this were the two Minas Geraes-class dreadnoughts, ten Pará-class destroyers, three submarines and a submarine tender.[7][8] With a design that borrowed heavily from the British Adventure-class scout cruisers,[1] Bahia's keel was laid on 19 August 1907 in Armstrong Whitworth's Elswick, Newcastle upon Tyne yard.[1][2] Construction took about a year and a half, and she was launched on 20 January 1909.[2][6][N 1] The process of fitting out pushed her completion date to 2 March 1910,[2] after which she sailed to Brazil, arriving in Recife on 6 May.[3] The new cruiser—the third ship of the Brazilian Navy to honor the state of Bahia[3][6]—was commissioned into the navy shortly thereafter on 21 May 1910.[3] As a class, Bahia and Rio Grande do Sul were the fastest cruisers in the world when they were commissioned,[1] and the first in the Brazilian Navy to utilize steam turbines for propulsion.[3]
[edit] Mutiny

Brazil's economy was suffering from a severe recession at the same time Bahia was commissioned.[10] This economic hardship, along with the racism prevalent in all branches of the Brazilian armed forces, and the severe discipline enforced on all navy ships, spawned a mutiny known as the Revolta da Chibata (Revolt of the Whip) among sailors on the most powerful ships.[10][11]

Unhappy with the violent treatment they were receiving, black sailors on the dreadnought battleship Minas Geraes began planning an uprising early in 1910, and chose João Cândido Felisberto—an experienced sailor later known as the "Black Admiral"—as leader.[10] In mid-November, a sailor was sentenced to be flogged 250 times in front of his fellow sailors, even though the practice had been banned by law.[10][11][12] The punishment was administered and continued even after the sailor fainted,[10] infuriating the nascent mutineers. Although they were not ready and could not revolt immediately, they quickened their preparations and rebelled on 21–22 November, earlier than originally planned.[11] They killed several officers and the captain of Minas Geraes, while other officers were forced off the ship.[11] The revolt quickly expanded to the battleship São Paulo, the elderly coastal defense ship Deodoro, and Bahia.[11] While joining the revolt, the crew of the scout cruiser murdered one of their officers.[6] During this time, discipline on the rebelling ships was not relaxed; daily drills were conducted and Felisberto ordered all liquor to be thrown overboard.[11]
Many men standing, smiling for the camera; most are dressed in white sailor uniforms, but a few in the center are clad in black suits with ties
João Cândido Felisberto with reporters, officers and sailors on aboard Minas Geraes on 26 November 1910, the final day of the rebellion

The crews of the torpedo boats remained loyal to the government,[11] and army troops moved to the presidential palace and the coastline, but neither group could stop the mutineers.[10] The fact that many who manned Rio de Janeiro's harbor defenses were sympathetic to the mutineers' cause,[11] coupled with chance that the capital might be bombarded by the mutinous ships, forced the National Congress of Brazil to give in to the rebels' demands.[10] These included the abolition of flogging, improved living conditions, and the granting of amnesty to all mutineers.[10][11] The government also issued official pardons and a statement of regret; its submission resulted in the rebellion's end on 26 November, when control of the four ships was handed back to the navy.[10]
[edit] First World War
See also: Brazil during World War I

In the opening years of the First World War, the Brazilian Navy was sent out to patrol the South Atlantic with French, British and American naval units, although its ships were not supposed to engage any threat outside territorial waters as Brazil was not at war with the Central Powers.[13] The country also tried to ensure that it remained totally neutral; Bahia and Rio Grande do Sul were sent to Santos in August 1914 when it was reported that the German raider Dresden was lying in wait off that port for British and American merchant ships.[14][N 2] Brazil joined the Entente and declared war on the Central Powers on 26 October 1917.[3][13]

On 21 December 1917, the Brazilian Navy—at the behest of the British—formed a small naval force with the intent of sending it to the other side of the Atlantic.[15] On 30 January 1918, Bahia was made the flagship of the newly organized Divisão Naval em Operações de Guerra (Naval Division in War Operations, abbreviated as DNOG), under the command of Rear Admiral Pedro Max Fernando Frontin.[3][6] The other ships assigned to the squadron were Bahia's sister Rio Grande do Sul, Pará-class destroyers Piauí, Paraíba, Rio Grande do Norte and Santa Catarina, tender Belmonte, and tugboat Laurindo Pita.[3][6][7][15]

The DNOG sailed for the British colony of Sierra Leone on 31 July. Since other allied countries helped with logistics, little was provided by Brazil aside from the ships themselves and the men crewing them.[15] Despite the threat of a U-boat attack, they were forced to stop several times so Belmonte could transfer necessities such as coal and water to the other ships.[15] They reached Freetown safely on 9 August and remained in the port until 23 August when they departed for Dakar.[15] While on this section of the voyage, Bahia, Rio Grande do Sul, Rio Grande do Norte, Belmonte and Laurindo Pita spotted an apparent torpedo heading for Belmonte, but it missed. Rio Grande do Norte then fired several shots and depth-charged what the force believed to be a U-boat.[3][16] While the official Brazilian history of the ship definitively claims to have sunk a submarine,[3] author Robert Scheina notes that this action was never confirmed,[16] and works published about U-boat losses in the war do not agree.[17]

After arriving in Dakar on 26 August, the DNOG was tasked with patrolling a triangle with corners at Dakar, Cape Verde and Gibraltar; the Allies believed that this area was rife with U-boats waiting for convoys to pass through.[16] As such, the Brazilian unit's mission was to patrol for mines laid by German minelaying submarines and to make sure that convoys passing through would be safe.[16] Complications arose when both Bahia and Rio Grande do Sul had problems with their condensers, a matter which was made much worse by the hot, tropical climate in which the ships were serving.[16]

In early September, the squadron was struck by the Spanish flu pandemic.[16] The contagion began aboard Bahia, spread to the other ships of the squadron and remained present for seven weeks.[16] At one point, 95% of some of the ships' crews were infected; 103 died overseas, and 250 died in Brazil after returning there.[16] On 3 November, Bahia, three of the four destroyers, and the tugboat were sent to Gibraltar for operations in the Mediterranean Sea.[16] They arrived on 9 or 10 November,[6][16][18][N 3] escorted by the American destroyer Israel,[6][18] but the fighting ceased on the 11th when the Armistice with Germany was signed.[16] Sometime in early 1919, Bahia, accompanied by four destroyers, voyaged to Portsmouth, England; they then traveled across the English Channel to Cherbourg, arriving there on 15 February.[19] The commander of the squadron, Admiral Pedro Max Fernando Frontin, met with the Maritime Prefect prior to the commencement of "social events"; these lasted until 23 February, when the ships moved to Toulon and Frontin journeyed overland to Paris.[19] The DNOG was dissolved on 25 August 1919.[3]
[edit] Modernization and inter-war years
A ship with a tall mast and low superstructure apparently at rest
Bahia sometime after her major modernization; the addition of a funnel was a striking change to the ship's appearance

In 1925–26,[1][6][N 4] Bahia underwent significant modernization.[3] The original five turbines were replaced by three Brown–Curtis turbines, while the original ten boilers were replaced by six Thornycroft oil-burning boilers, which necessitated the addition of a third funnel. The former coal bunkers, along with some of the space freed up by the decrease in boilers, were converted to hold 588,120 litres (155,360 US gal) of oil.[3] These modifications resulted in Bahia's top speed increasing to 28 knots (52 km/h).[1] All of the boats onboard were replaced, and three 20.1 mm (0.79 in) Madsen guns, a 7 mm (0.28 in) Hotchkiss machine gun, and four 533 mm (21.0 in) torpedo tubes were added to give the ship a defense against aircraft and more power against surface ships, respectively.[3] Still, in 1930 The New York Times labeled Bahia and the other warships in Brazil's navy as "obsolete" and noted that nearly all were "older than the ages considered effective by powers signatory to the Washington and London Naval Treaties."[22]

On 28 June 1926, the Ludington Daily News reported that Bahia would pay a visit to Philadelphia, accepting an invitation from the United States government to participate in the sesquicentennial celebrations.[21][N 5] In mid-1930, Bahia and Rio Grande do Sul—under the command of Heráclito Belford Gomes—escorted Brazil's President-elect Júlio Prestes to the United States.[23] Traveling onboard the Brazilian-Lloyd ocean liner Almirante Jacequay, Prestes was returning American then-President-elect Herbert Hoover's visit to Brazil in December 1928.[23][24] The cruisers USS Trenton and Marblehead met the three ships about 100 miles (160 km) off of Sandy Hook and honored Prestes with a 21-gun salute.[25][26] After spending five hours in the Ambrose Channel due to fog, Prestes traveled on a launch to a pier, during which Bahia rendered one 21-gun salute and Fort Jay offered two.[26] After arriving ashore, he traveled to City Hall before speeding down to Washington, D.C.[26] He stayed in the United States for eight days before departing for France on the White Star Line's Olympic.[27] Bahia and Rio Grande do Sul were berthed at the Brooklyn Navy Yard for the visit.[26]

During the Brazilian Revolution of 1930, Bahia served with Rio Grande do Sul—until that ship defected—and five or six destroyers off the coast of Santa Catarina; they were once again commanded by Belford Gomes.[3][28][N 6] Two years later, when the state of São Paulo rebelled in the Constitutionalist Revolution, Bahia—under the command of Frigate Captain Lucas Alexandre Boiteux—and other vessels blockaded the rebel-held port of Santos.[3][6] Bahia was under repair from 1934 into 1935.[6] In November 1935, Bahia and Rio Grande do Sul sailed to Natal, the capital of Rio Grande do Norte, to lend support against another rebellion.[30][31] As part of their mission, they were ordered to sink the steamship Santos on sight, as several escaping leaders of the revolution were embarked.[32]

From 17–22 May 1935,[33][34] Bahia and Rio Grande do Sul[N 7]—joined at an unknown point by the Argentine battleships Rivadavia and Moreno, the heavy cruisers Almirante Brown and Veinticinco de Mayo, and five destroyers[34]—escorted São Paulo, with Brazilian President Getúlio Dornelles Vargas embarked, up the Río de la Plata (River Plate) to Buenos Aires, the capital of Argentina.[6][33][34][35] Vargas was returning visits from the presidents of Argentina and Uruguay, Agustín Pedro Justo and Gabriel Terra.[6][33] Vargas and Justo planned to be present at the opening session of the Pan-American Commercial Conference on 26 May,[33] and open a Chaco War peace conference,[33][35] before São Paulo conveyed Vargas to Montevideo, Uruguay for meetings with Terra.[35]

On 2 March 1936, Bahia escorted Veinticinco de Mayo, which had the Argentine Navy Minister Rear Admiral Eleazar Videla embarked, and Almirante Brown in the last part of their journey to Rio de Janeiro.[36]
[edit] Second World War

After Brazil's entrance into the Second World War on 21 August 1942, which took effect on 31 August,[37] Bahia was used extensively for escorts and patrols; sources conflict as to the actual number—either 67 and 15[3] or 62 and 11.[20] In total, she traveled 101,971 nmi (188,850 km; 117,346 mi) in 358 days, and played a role in shepherding over 700 merchant ships,[3] despite being labeled by the United States Naval Institute's magazine Proceedings as being an "oversized destroyer" which was "relatively slow".[38]

On 3 June 1943, while Bahia was escorting the convoy BT 12, she located an underwater mine and destroyed it with one of her 20 mm (0.79 in)[clarification needed] Madsen guns.[3] On 10 July, while at 26°15′S 43°35′W﻿ / ﻿26.25°S 43.583°W﻿ / -26.25; -43.583, Bahia received a sonar contact and depth-charged what the Brazilian Navy's official history of the ship reports might have been the German submarine U-199, which was sunk later that month in the same area (off Rio de Janeiro) by American and Brazilian aircraft.[3][39] In November 1944, Bahia joined the American light cruiser Omaha and destroyer escort Gustafson in accompanying the 4th Squadron of the Brazilian Expeditionary Force as they were carried on the troop transport General M. C. Meigs to Italy.[3]

Bahia was modernized again twice during the war, in both 1942 and 1944; these modernizations were not as extensive as those of the 1920s. Two of her 47 mm (1.9 in) guns were replaced with 76 mm (3.0 in) L/23 AA guns, her Madsen guns were replaced with seven Oerlikon 20 mm cannons in single mounts, and a director for these guns was installed.[3] Two depth charge tracks were added, improved range-finders were added to the 120 mm (4.7 in) guns, and sonar and radar were fitted, in addition to other minor modifications.[3][N 8] The Brazilian Navy's official history of the ship reports these modifications, but does not specify which were undertaken in which year.[3]
[edit] Loss
A warship at sea under a bright sky, possibly underway
A profile of Bahia at some point after her 1920s modernization; note the men congregated on the foredeck

Various warships of the Allied nations, including Brazil's, were assigned to patrol in the Atlantic as rescue ships; they would lie in wait near routes frequented by military transport aircraft that were carrying personnel from the recently ended European theater to the continuing war in the Pacific.[6][40] Bahia was one such ship;[40] on 4 July 1945, she was stationed northeast of Brazil around 0°N 30°W﻿ / ﻿0°N 30°W﻿ / 0; -30, near the Arquipélago de São Pedro e São Paulo (Saint Peter and Saint Paul Archipelago).[3][4][6] For anti-aircraft target practice, crewmen were firing the ship's 20 mm guns at a kite that was being towed behind the ship. One of them shot it down, but also accidentally hit the depth charges on the stern—a direct consequence of the lack of guide rails that would normally prohibit the guns from being aimed at the ship.[4][6] The resulting explosion knocked out all power on the ship and sank her in about three minutes.[1][4][41]

The survivors of the blast endured four or five days of no food, high temperatures and full exposure to the sun on their makeshift rafts;[41][42] some, driven mad by these conditions, simply jumped into the water and were devoured by sharks.[42] From this point on, sources vary greatly. According to an article in Time, Bahia's loss was not discovered until 8 July, when 22 survivors were picked up by a freighter, Balfe.[41][43][N 9] Author Robert Scheina, however, says that the disaster was revealed when Rio Grande do Sul arrived on station four days after the sinking to take Bahia's place and could not find her.[4]

Sources also disagree on the number rescued and final death toll. The official history of the ship gives 36 rescued and 336 dead,[3] Poder Naval Online gives 36 and 339,[6][N 10] and Conway's All The World's Fighting Ships 1906–1921 gives no number for survivors but 294 for deaths.[1] Contemporary news articles also published varying numbers; in an article published a day after the accident became known, The Evening Independent stated that the ship carried 383 men, though it did not give any more information.[44] The New York Times gave figures of 28 saved and 347 lost,[42] while the St. Petersburg Times gave 32 and 395.[45] Sources do agree, however, that four American radiomen were killed.[3][6][46]

Rescued crewmen believed that they had hit a mine that detonated one of the ship's magazines.[41] Vice Admiral Jorge Dodsworth Martins—Brazil's chief of naval intelligence—thought that Bahia could have been mined or torpedoed by U-530,[42][47] which surrendered under strange circumstances in Argentina on 10 July (some two months after Germany's surrender), but the Argentine Naval Ministry stated that it would have been impossible for the submarine to travel from the site of the sinking to Mar del Plata in six days (4–10 July).[47][N 11] By late October, the Brazilian Navy had come to the conclusion that Bahia had been sunk by an accidental explosion.[46][49]
[edit] See also

    * USS Indianapolis (CA-35), an American heavy cruiser, also sunk in July 1945, whose survivors endured circumstances similar to Bahia's

[edit] Notes

   1. ^ The Miramar Ship Index—using information from contemporary builders' records—and Poder Naval Online record Bahia's launching date as 20 January 1909, and her sister ship Rio Grande do Sul's as 20 April 1909. Conway's All the World's Fighting Ships 1906–1921 and the Brazilian Navy's official history reverse these dates, giving 20 January for Rio Grande do Sul and 20 April for Bahia. This article uses the former date because of Miramar's use of builders' records, which, in this case, should be the most accurate source.[1][2][3][9]
   2. ^ The New York Times' article refers to Bremen, but that ship was in the Baltic Sea at the time. The only German cruiser in that area in August 1914 was Dresden. The misidentification was probably due to the fog of war.
   3. ^ Sources give different dates; Poder Naval Online and Israel's Dictionary of American Naval Fighting Ships entry give 9 November, while Scheina gives 10 November.[6][16][18]
   4. ^ The official history of the ship gives a 1924–1927 range,[3] while Scheina in Conway's, Poder Naval Online, and Whitley give 1925–1926.[1][6][20] Additional collaborating evidence for the latter date can be found in a June 1926 Ludington Daily News article which reported that Bahia was going to visit the United States—implying that the ship had been placed back into service.[21]
   5. ^ There appears to have been no follow-up article on what occurred after Bahia arrived.
   6. ^ Rio Grande do Sul defected at an unknown date,[28] and Bahia may have as well; on 6 October, a rebel general claimed that both ships had defected.[29]
   7. ^ The New York Times remarks that the two Brazilian cruisers "were bound for Ilha Grande and [w]inter manoeuvres", but it is not clear from other sources if they actually did this.[33]
   8. ^ Regarding the installation of sonar, it is not clear whether it was fitted for the first time in 1942 (and used in the 10 July depth charging) or whether a more modern sonar replaced an outmoded version in either 1942 or 1944.
   9. ^ The magazine also reports that additional survivors were rescued over the next few days, but does not give a definitive figure. Poder Naval Online, however, states that a total of 36 survivors were rescued by Balfe on the 8th.[6][41]
  10. ^ These figures contradict other information present in the article, however. Poder first says that 339 of 372 total crewmembers died, meaning that 33 survived, but the subsequent sentence states directly that 36 survived.[6]
  11. ^ Rumors persist today that either U-530 or U-977 sank Bahia.[48]

List of places of worship in Hastings
From Wikipedia, the free encyclopedia
Jump to:navigation, search
All Saints Church, one of two surviving medieval churches in the centre of Hastings, overlooks the Old Town.
The distinctive Elim Pentecostal church was built in the 1980s.
[show]Map of all coordinates from Google

Map of all coordinates from Bing
Export all coordinates as KML
Export all coordinates as GeoRSS
Map of all microformated coordinates

The borough of Hastings, one of six local government districts in the English county of East Sussex, has more than 50 extant places of worship serving a wide range of religious denominations. A further 16 buildings formerly used for public worship, but now closed or used for other purposes, also exist. The borough is made up of the ancient port and seaside resort of Hastings, the neighbouring planned resort of St Leonards-on-Sea (united with its former rival in 1888)[1] and their 19th- and 20th-century suburbs, some of which (such as Ore and Hollington) were autonomous villages until they were absorbed into the growing urban area. Ancient churches existed in the Old Town of Hastings and in the villages, although some were lost in the medieval era; growth stimulated by transport improvements and the popularity of sea-bathing encouraged a rush of church-building in the Victorian era; and more churches and congregations were established throughout the 20th century, despite periods of stagnation and decline.

Most residents of Hastings identify themselves as Christian, and churches representing many Christian denominations exist in the town. The largest number of these belong to the Church of England, the country's officially established church. Roman Catholic and Protestant Nonconformist churches of many types are also prevalent, and St Leonards-on-Sea has a mosque. The spread of housing inland in the 20th century, in suburbs such as Silverhill Park, Broomgrove and the vastly expanded Hollington (which was transformed from a haphazard collection of cottages among fields into a 1960s council estate), resulted in the founding of new churches, partly offsetting the loss through demolition of others in Hastings town centre.

English Heritage has awarded listed status to several current and former church buildings in Hastings. A building is defined as "listed" when it is placed on a statutory register of buildings of "special architectural or historic interest" in accordance with the Planning (Listed Buildings and Conservation Areas) Act 1990.[2] The Department for Culture, Media and Sport, a Government department, is responsible for this; English Heritage, a non-departmental public body, acts as an agency of the department to administer the process and advise the department on relevant issues.[3] There are three grades of listing status. Grade I, the highest, is defined as being of "exceptional interest"; Grade II* is used for "particularly important buildings of more than special interest"; and Grade II, the lowest, is used for buildings of "special interest".[4] Some Anglican churches are still graded according to an old system in which Grades A, B and C were equivalent to I, II* and II respectively.
Contents
[hide]

    * 1 History of Hastings and its places of worship
    * 2 Religious affiliation
    * 3 Administration
    * 4 Open places of worship
    * 5 Closed or disused places of worship
    * 6 References
          o 6.1 Notes
          o 6.2 Bibliography

[edit] History of Hastings and its places of worship
Hastings shown within East Sussex
East Sussex shown within England

Hastings is a seaside town on the southeast coast of England, facing the English Channel. The borough covers 7,340 acres (2,970 ha) and had a population of 85,029 at the time of the 2001 United Kingdom Census.[5] Hastings is most famous for the battle fought nearby in 1066, in which William the Conqueror's Norman army defeated the English troops of King Harold II,[6] but its recorded history is much longer: 5th-century origins have been attributed, Roman settlement on the site has never been proved but is considered likely,[7] and a town had developed by 928, when it was important enough to have its own mint.[8][9] By the 12th century, it was the main member of the Cinque Ports, and its castle dominated the cliff below which the ancient settlement developed.[10][11] There were seven churches in 1291, when Pope Nicholas IV ordered a survey of all places of worship in England, but decline set in during the 14th century and two French raids wrecked the town. By 1801, just two of the old churches—All Saints and St Clement's—survived.[10]

The common thread throughout the town's history has been fishing: in 1329 a priest was threatened with excommunication for failing to pay the Bishop of Chichester the 2,000 herring demanded by custom,[12] and a beach-based fishing fleet still exists in the 21st century.[13] The fishermen even had their own church from 1854 until World War II: the rectors of All Saints and St Clement's got together to provide a chapel of ease on the beach to serve their spiritual needs. The former St Nicholas' Church is now Hastings Fishermen's Museum.[14] The town's focus moved away from this industry and towards tourism and leisure from the early 19th century, though, as development spread west from the old town.[15] Improved transport opened the town up to day-trippers, especially from London; sea-bathing, promenading and other seaside leisure activities became increasingly fashionable; and James Burton capitalised on the demand for growth by founding an entirely new town, St Leonards-on-Sea, immediately west of Hastings—spurring its older rival into further growth.[8] The population rose from 2,982 to 6,051 between 1801 and 1821,[15] and the need to build more churches was recognised. In 1824, St Mary-in-the-Castle Church, which took its dedication from a ruined collegiate church in the castle grounds, was the first new Anglican church to be built outside Hastings Old Town;[10] but a Baptist chapel had already been established in 1817,[16] and another followed at Wellington Square in the 1830s.[17] Development was so rapid that Holy Trinity Church, the second town church in Hastings, had to be crowded into a "crazy site" when it was built in the 1850s.[18] St Leonards-on-Sea gained its first church, St Leonard's, in 1837, followed by St Mary Magdalen's Church in 1852 and an array of Nonconformist places of worship. (Dissenters were not universally welcomed: the town's first Congregational chapel, planned in 1807, had to be built in London and taken to the town by sea because no local firm wanted to build it. The weatherboarded chapel's successor survived until 1972.)[19][20] Hastings and St Leonards-on-Sea each have a large 1880s Roman Catholic church,[21][18] but worship by that community dates back to 1848, when the now disused St Michael's Chapel was opened for public use.[19]

In 1897, an Act of Parliament brought several surrounding villages into the borough of Hastings; nine years earlier the same had happened to St Leonards-on-Sea.[1] Places such as Ore and Hollington had become suburbanised but retained ancient churches as well as gaining new ones: Ore's 12th-century St Helen's Church was ruined in the 19th century, but a replacement was built nearby[22] and a second, Christ Church (distinguished by the "very naughty turret" on its roof), was provided to serve the village's Victorian suburbs;[23] and Hollington's 13th-century church in the middle of a wood[24] was supplemented by a second Anglican church and one for Methodists. The scattered village was later redeveloped into Hastings' largest council estate, and more places of worship were added for various congregations.[25]
[edit] Religious affiliation

According to the 2001 United Kingdom Census, 85,029 people lived in Hastings. Of these, 67.4% identified themselves as Christian, 0.75% were Muslim, 0.27% were Hindu, 0.04% were Sikh, 0.32% were Buddhist, 0.13% were Jewish, 0.47% followed another religion, 21.3% claimed no religious affiliation and 9.3% did not state their religion. The proportion of Christians was lower than the 71.7% in England as a whole, and affiliation with most other religions was also much less widespread than in England overall: in 2001, 3.1% of people in England were Muslim, 1.1% were Hindu, 0.7% were Sikh and 0.52% were Jewish. The proportion of people with no religious affiliation was correspondingly higher than the national figure of 14.6%.[26]
[edit] Administration

All Anglican churches in the borough of Hastings are part of the Diocese of Chichester, whose cathedral is at Chichester in West Sussex.[27] The Rural Deanery of Hastings—one of eight deaneries in the Archdeaconry of Lewes and Hastings, which is in turn one of three archdeaconries in the diocese[28]—covers the whole borough and includes all 18 open Anglican churches.[29] Four churches which are no longer used for Anglican worship are still nominally included. St Mary Magdalen's Church in St Leonards-on-Sea, now used by the Greek Orthodox community, has been included in the parish of Christ Church.[30] The ruined St Helen's Church, near Ore, is part of the new St Helen's parish, together with St Barnabas' Church.[31] All Souls Church at Clive Vale has been included in the parish of Christ Church in Ore.[32] The long-closed church of St Mary-in-the-Castle on Hastings seafront is now in Emmanuel Church's parish.[33]

The Roman Catholic Diocese of Arundel and Brighton, whose cathedral is at Arundel,[34] administers the borough's three Roman Catholic churches. St Leonards-on-Sea Deanery, one of 13 deaneries in the diocese,[35] has five parishes, two of which cover the borough in its entirety.[36] St Mary Star of the Sea Church is the parish church of Hastings; [37] and the churches at St Leonards-on-Sea and Hollington are part of a joint parish.[38] The deanery's other three parishes are centred on the towns of Bexhill, Rye and Battle in the neighbouring districts of Rother and Wealden.[36]

Hastings' four United Reformed congregations—at Clive Vale,[39] Silverhill (St Luke's),[40] Blacklands (St Mark's)[41] and Robertson Street in Hastings town centre[42]—are in the Southern Synod, one of 13 Synods in Great Britain.[43] The Synod is responsible for 168 United Reformed churches in southeast England.[44]

The Hastings, Bexhill & Rye Methodist Circuit, a circuit in the Methodist Church's South East District, covers 13 churches of that denomination in the Hastings area. Four of those are in the borough: the Calvert Memorial church at Halton, the churches at Hollington and St Helen's, and the Park Road Church in Bohemia.[45]
[edit] Open places of worship
Name↓ 	Image 	Location/
Coordinates↓ 	Denomination/
Affiliation↓ 	Grade↓ 	Notes 	Refs
All Saints Church 	All Saints Church, Old Town, Hastings (IoE Code 293639).JPG 	Old Town
50°51′33″N 0°35′47″E﻿ / ﻿50.8593°N 0.5964°E﻿ / 50.8593; 0.5964﻿ (All Saints Church, Old Town) 	Anglican 	B ﻿B 	The medieval town's "upper church", so called because of its hilltop position, is an early 15th-century Perpendicular Gothic structure of rubble and flint. William Butterfield's restoration of 1870 included a large east window. Titus Oates was a curate in the 17th century. 	[46][47]
[48][49]
[50]
Christ Church 	Christ Church, St Leonards, Hastings (IoE Code 293985).JPG 	St Leonards-on-Sea
50°51′14″N 0°33′33″E﻿ / ﻿50.8538°N 0.5593°E﻿ / 50.8538; 0.5593﻿ (Christ Church, St Leonards-on-Sea) 	Anglican 	B ﻿II* 	Arthur Blomfield's tall, Early English-style church of 1875 (consecrated nine years later) replaced an earlier building which still stands on the south side. Always High church in its liturgical tradition, its first vicar Rev. Charles Lyndhurst Vaughan was key to the town's religious and social development. 	[46][19]
[23][51]
[52][53]
[54][55]
[56]
Holy Trinity Church 	Holy Trinity Church, Hastings (IoE Code 294055).jpg 	Hastings
50°51′18″N 0°34′36″E﻿ / ﻿50.8551°N 0.5767°E﻿ / 50.8551; 0.5767﻿ (Holy Trinity Church, Hastings) 	Anglican 	B ﻿II* 	The second 19th-century Anglican church in Hastings was planned for Cambridge Road, but problems with the site caused Samuel Sanders Teulon to reconfigure his design to an awkward ("crazy", to Nikolaus Pevsner) location nearby. The stone and rubble exterior conceals a highly ornate interior. The dedication recalls a lost 12th-century priory nearby. 	[19][46]
[10][18]
[57][58]
[59][60]
St Clement's Church 	St Clement's Church, Old Town, Hastings (IoE Code 294078).jpg 	Old Town
50°51′27″N 0°35′27″E﻿ / ﻿50.8574°N 0.5909°E﻿ / 50.8574; 0.5909﻿ (St Clement's Church, Old Town) 	Anglican 	B ﻿II* 	The "Town church" as it is known locally was rebuilt after the French attacks on Hastings in 1377 and restored in 1875 by William Butterfield, who did the same to neighbouring All Saints Church. The vast range of memorials in the Perpendicular Gothic church include one for the marriage of Dante Gabriel Rossetti and Elizabeth Siddal. 	[46][47]
[61][62]
[63]
St John the Evangelist's Church 	St John the Evangelist's Church, St Leonards, Hastings (IoE Code 294099).JPG 	St Leonards-on-Sea
50°51′21″N 0°33′11″E﻿ / ﻿50.8559°N 0.5530°E﻿ / 50.8559; 0.5530﻿ (St John the Evangelist's Church, St Leonards-on-Sea) 	Anglican 	B ﻿II* 	Only the octagonal tower survived World War II bombing; the rest of Arthur Blomfield's 1881 brick and stone church serving Upper St Leonards was rebuilt by Harry Stuart Goodhart-Rendel in 1951. English Heritage describe it as a "particularly eclectic mix". 	[64][46]
[65][66]
[67][68]
[69]
St Matthew's Church 	St Matthew's Church, Bohemia, Hastings (IoE Code 294070).JPG 	Silverhill
50°51′56″N 0°33′20″E﻿ / ﻿50.8656°N 0.5556°E﻿ / 50.8656; 0.5556﻿ (St Matthew's Church, Silverhill) 	Anglican 	B ﻿II* 	Founded on St Matthew's Day (21 September) 1860 and opened the following year, local architect George Voysey's original church was replaced by John Loughborough Pearson's much larger red-brick Gothic Revival structure in 1885. Internal features include Aston Webb's reredos and a sturdy king post nave roof of local timber. 	[64][46]
[70][71]
[72][73]
[74]
Christ Church 	Christ Church, Blacklands, Hastings (IoE Code 293983).jpg 	Blacklands
50°52′01″N 0°34′40″E﻿ / ﻿50.8669°N 0.5777°E﻿ / 50.8669; 0.5777﻿ (Christ Church, Blacklands) 	Anglican 	C ﻿C 	Henry Carpenter's Decorated Gothic church of 1881, for the high-class suburb of Blacklands, is dominated by its tower (completed in 1890) but has elaborate interior fittings, such as Hardman & Co.'s chancel work and a Carrara marble font depicting an angel bearing a shell. 	[19][46]
[23][75]
[76][77]
Christ Church 	Christ Church, Ore, Hastings (IoE Code 294029).JPG 	Ore
50°52′20″N 0°36′30″E﻿ / ﻿50.8722°N 0.6082°E﻿ / 50.8722; 0.6082﻿ (Christ Church, Ore) 	Anglican 	C ﻿II 	The rector of Ore helped to fund a new church in the village centre to serve the influx of working-class people in the mid-19th century. A.D. Gough's Decorated Gothic stone church dates from 1858 and is distinguished by a large bell turret, a feature described as "very naughty" by Pevsner. Bomb damage in 1943 was soon repaired. 	[19][46]
[23][78]
[79]
St Leonard's Church (Church in the Wood) 	Church-in-the-Wood, Hollington, Hastings (IoE Code 293741).jpg 	Hollington
50°52′27″N 0°32′17″E﻿ / ﻿50.8743°N 0.5380°E﻿ / 50.8743; 0.5380﻿ (St Leonard's Church (Church in the Wood), Hollington) 	Anglican 	C ﻿II 	A chapel stood on this isolated site in the middle of a wood in the 11th century, and the present building retains 13th-century work despite major restoration in 1865. The short tower is partly tile-hung and has a pyramid-shaped cap. Jean-Baptiste Capronnier designed the stained glass. 	[46][80]
[81][82]
[83][25]
[84][85]
St Leonard's Church 	St Leonard's Church, St Leonards, Hastings (IoE Code 470627).JPG 	St Leonards-on-Sea
50°51′05″N 0°33′05″E﻿ / ﻿50.8513°N 0.5514°E﻿ / 50.8513; 0.5514﻿ (St Leonard's Church, St Leonards-on-Sea) 	Anglican 	C ﻿II 	James Burton, the founder of St Leonards-on-Sea, built a seafront church for the new town in 1837. Five years later, the cliff behind collapsed and crushed the chancel; and in 1944 a freakish direct hit from a V-1 flying bomb, damaged by anti-aircraft fire, brought the whole church down. Giles and Adrian Gilbert Scott's neo-Gothic pale brick and stone design, executed in eight years from 1953, replaced it. 	[64][46]
[67][21]
[86][87]
[88][89]
[90][91]
St Peter's Church 	St Peter's Church, Bohemia, Hastings (IoE Code 294071).JPG 	Bohemia
50°51′39″N 0°33′41″E﻿ / ﻿50.8608°N 0.5614°E﻿ / 50.8608; 0.5614﻿ (St Peter's Church, Bohemia) 	Anglican 	C ﻿C 	Bohemia's Anglican church is tall, long and lacks a spire or tower (one was planned). It was built in 1885 in the Early English style by James Brooks, and is red-brick inside and out—although much use is made of alabaster for wall finishes and internal fixtures. 	[46][18]
[17][92]
[93][94]
Emmanuel Church 	Emmanuel Church, West Hill, Hastings.jpg 	West Hill
50°51′42″N 0°35′21″E﻿ / ﻿50.8616°N 0.5892°E﻿ / 50.8616; 0.5892﻿ (Emmanuel Church, West Hill) 	Anglican 	D ﻿– 	The local architecture firm of Jeffrey & Skiller designed and built the West Hill area's Anglican church in 1873. It stands at a high point in Hastings and has significant townscape presence. The stone Early English-style building lost its adjacent vicarage to a bomb in 1942. 	[19][46]
[95][96]
St Anne's Church 	St Anne's Church, Hollington, Hastings.jpg 	Hollington
50°52′20″N 0°32′36″E﻿ / ﻿50.8723°N 0.5434°E﻿ / 50.8723; 0.5434﻿ (St Anne's Church, Hollington) 	Anglican 	D ﻿– 	The interwar and postwar expansion of Hollington, which by the 1960s had become Hastings' largest council estate, led to improved church provision in the form of this small flint and brick building, founded in 1956 and built over several years by the firm of J.L. Denman and Sons. It is in the parish of Church in the Wood. 	[46][97]
[98][99]
[100]
St Barnabas' Church 	St Barnabas Church, Broomgrove, Hastings.jpg 	Broomgrove
50°52′25″N 0°35′37″E﻿ / ﻿50.8736°N 0.5937°E﻿ / 50.8736; 0.5937﻿ (St Barnabas' Church, Broomgrove) 	Anglican 	D ﻿– 	Local architect Hector Sweatman's design for a new church in the parish of St Helen's in Ore was accepted in 1954, although proposals for a church on this site dated back to 1949. The brick building has flexible space for religious and community activities. 	[46][101]
[102][103]
[104]
St Ethelburga's Church 	St Ethelburga's Church, Bulverhythe, Hastings.JPG 	Bulverhythe
50°51′09″N 0°32′01″E﻿ / ﻿50.8524°N 0.5337°E﻿ / 50.8524; 0.5337﻿ (St Ethelburga's Church, Bulverhythe) 	Anglican 	D ﻿– 	John B. Mendham's modest Gothic Revival church dates from 1929 and serves the Bulverhythe area of the seafront west of St Leonards-on-Sea in the far southwest of the borough. The brown-brick structure is dominated by a large Art Deco-style tower with thin pinnacles set below the top of the bell tower stage. 	[64][46]
[23][105]
St Helen's Church 	St Helen's Church (Ore Parish Church), The Ridge, Hastings.jpg 	St Helen's
50°52′55″N 0°35′14″E﻿ / ﻿50.8820°N 0.5871°E﻿ / 50.8820; 0.5871﻿ (St Helen's Church, St Helen's) 	Anglican 	D ﻿– 	Built to replace its ruinous 12th-century predecessor nearby, this church was designed in 1869 by Edgar Brock, many of whose Sussex churches were executed in partnership with the Habershon brothers. The stone used to build it was quarried locally. Its distinctive spire was removed in 1966 and replaced with a small cap. 	[64][46]
[22][106]
[107][108]
[109]
St John the Evangelist's Church 	St John the Evangelist's Church, Hollington, Hastings.jpg 	Hollington
50°52′44″N 0°33′05″E﻿ / ﻿50.8788°N 0.5513°E﻿ / 50.8788; 0.5513﻿ (St John the Evangelist's Church, Hollington) 	Anglican 	D ﻿– 	E. Alexander Wyon's Early English-style church dates from 1865 and was parished five years later. Blue and pale (Bath) stonework predominates. Local philanthropist Countess Waldegrave founded the church on land provided by local men who realised that Hollington's focus of development was moving away from Church in the Wood. 	[64][46]
[25][110]
[111]
St Peter and St Paul's Church 	St Peter and St Paul's Church, Silverhill Park, Hastings.jpg 	Silverhill Park
50°52′45″N 0°34′09″E﻿ / ﻿50.8791°N 0.5692°E﻿ / 50.8791; 0.5692﻿ (St Peter and St Paul's Church, Silverhill Park) 	Anglican 	D ﻿– 	This small yellow-brick church, with a many-sided layout and a copper roof, was opened in 1969 on Parkstone Road in the postwar Silverhill Park estate. It is in the parish of St John the Evangelist, Hollington. 	[46][112]
[113][114]
Calvert Memorial Methodist Church 	Calvert Memorial Methodist Church, Halton, Hastings.JPG 	Halton
50°51′56″N 0°35′23″E﻿ / ﻿50.8656°N 0.5896°E﻿ / 50.8656; 0.5896﻿ (Calvert Memorial Methodist Church, Halton) 	Methodist 	D ﻿– 	James Calvert, one of the first Christian missionaries to Fiji, was one of the founders of this church, and it now bears his name. The red-brick building, which replaced a tin tabernacle, is an Early English design and was opened in 1892. 	[115][46]
[116]
Hollington Methodist Church 	Hollington Methodist Church, Hastings.jpg 	Hollington
50°52′42″N 0°32′59″E﻿ / ﻿50.8782°N 0.5496°E﻿ / 50.8782; 0.5496﻿ (Hollington Methodist Church, Hollington) 	Methodist 	D ﻿– 	The present building dates from 1887 and has survived bomb, storm and fire damage. It is the fourth Methodist place of worship in Hollington: a cottage was used from 1823, a small chapel superseded it two years later and a larger building was provided in 1835. The plain brick and stone church has arched windows. 	[115][46]
[25][111]
Park Road Methodist Church 	Park Road Methodist Church, Bohemia, Hastings.JPG 	Bohemia
50°51′50″N 0°33′32″E﻿ / ﻿50.8638°N 0.5588°E﻿ / 50.8638; 0.5588﻿ (Park Road Methodist Church, Bohemia) 	Methodist 	D ﻿– 	The roots of Methodism in the Bohemia suburb can be traced to 1876, and four years later land was bought for the erection of a church. A school chapel elsewhere sufficed until 1891, when Philip Tree started building his Decorated Gothic-style design. The church, prominent on its corner site with its tower and stone spire, opened the following year. 	[115][46]
[117][118]
St Helen's Methodist Church 	St Helen's Methodist Church, Ore, Hastings.JPG 	Ore
50°52′29″N 0°36′23″E﻿ / ﻿50.8748°N 0.6063°E﻿ / 50.8748; 0.6063﻿ (St Helen's Methodist Church, Ore) 	Methodist 	D ﻿– 	This small church on the Ore–Baldslow Road (The Ridge) probably dates from 1877, although the date on its foundation stone is now illegible. The white-painted exterior hides red-brick walls. The windows are lancets. The slope of the land conceals a hall beneath the church. 	[115][46]
[119]
Wellington Square Baptist Church 	Wellington Square Baptist Church, Hastings (IoE Code 294127).JPG 	Hastings
50°51′22″N 0°34′55″E﻿ / ﻿50.8560°N 0.5819°E﻿ / 50.8560; 0.5819﻿ (Wellington Square Baptist Church, Hastings) 	Baptist 	B ﻿II* 	Wellington Square was an early high-class residential development in Hastings: it was built on the site of some lime kilns in the 1820s. A Baptist church was integrated into it in 1838. Arched sash windows, stucco and an unbroken parapet and moulded cornice give a Neoclassical appearance. 	[17][46]
[120][121]
[122][123]
St Leonards Baptist Church 	St Leonards Baptist Church, St Leonards, Hastings (IoE Code 294178).JPG 	St Leonards-on-Sea
50°51′24″N 0°33′42″E﻿ / ﻿50.8566°N 0.5616°E﻿ / 50.8566; 0.5616﻿ (St Leonards Baptist Church, St Leonards-on-Sea) 	Baptist 	C ﻿II 	Thomas Elworthy's Baptist church of 1882 is an ornate Classical/Italianate design with pairs of Corinthian pilasters on its three-bay façade, a balustrade at first-floor level, round-headed windows, an elaborate pediment and extensive use of terracotta decoration. The gallery inside is supported on slender iron columns. 	[17][46]
[123][124]
Ebenezer Baptist Church 	Ebenezer Baptist Church, Silverhill, Hastings.JPG 	Silverhill
50°52′02″N 0°33′16″E﻿ / ﻿50.8672°N 0.5545°E﻿ / 50.8672; 0.5545﻿ (Ebenezer Baptist Church, Silverhill) 	Baptist 	D ﻿– 	This small brick building on the Ponswood industrial estate was originally a Gospel Hall used by Plymouth Brethren. 	[112]
Halton Baptist Church 	Halton Baptist Church, Halton, Hastings.JPG 	Halton
50°52′06″N 0°36′04″E﻿ / ﻿50.8682°N 0.6011°E﻿ / 50.8682; 0.6011﻿ (Halton Baptist Church, Halton) 	Baptist 	D ﻿– 	This modern Baptist church, built in the Vernacular style, stands on the Old London Road on the way to Ore village. 	[46][125]
[126]
Clive Vale United Reformed Church 	Clive Vale United Reformed Church, Hastings.JPG 	Clive Vale
50°51′58″N 0°36′16″E﻿ / ﻿50.8660°N 0.6045°E﻿ / 50.8660; 0.6045﻿ (Clive Vale United Reformed Church, Clive Vale) 	United Reformed Church 	D ﻿– 	Founded as a Congregational church in 1887, this small red-brick chapel by Thomas Elworthy is in the Early English style and is distinguished by the unusual feature of a side porch. 	[17][46]
[127]
Robertson Street United Reformed Church 	Robertson Street United Reformed Church, Hastings.jpg 	Hastings
50°51′20″N 0°34′39″E﻿ / ﻿50.8555°N 0.5776°E﻿ / 50.8555; 0.5776﻿ (Robertson Street United Reformed Church, Hastings) 	United Reformed Church 	D ﻿– 	Henry Ward's church of 1884–85, for the Congregational community in the town centre, replaced a Lombardo-Gothic predecessor of 1856 which stood on the same site. The Robertson Street frontage of Ward's tall Neoclassical/Renaissance building is surrounded by shops, but the façade on Cambridge Road is fully visible and spans five bays. The walls are of dark stone. Charles New, the most important figure in Hastings' Congregationalist community, was instrumental in getting the new church funded and built. 	[17][46]
[123][128]
[129][130]
[131]
St Luke's United Reformed Church 	St Luke's United Reformed Church, Silverhill, Hastings.JPG 	Silverhill
50°52′10″N 0°33′26″E﻿ / ﻿50.8695°N 0.5572°E﻿ / 50.8695; 0.5572﻿ (St Luke's United Reformed Church, Silverhill) 	United Reformed Church 	D ﻿– 	One of southeast England's first English Presbyterian churches was founded in 1853 when Silverhill was no more than a farm and some cottages. Henry Carpenter built a permanent church of stone in 1857, which grew rapidly: a tower and spire were built in 1865, and a chancel in 1909. The Great Storm of 1987 ripped off the spire, which has been replaced by a pyramidal cap. 	[115][46]
[132][133]
[134][135]
St Mark's United Reformed Church 	St Mark's United Reformed Church, Blacklands, Hastings.jpg 	Blacklands
50°51′58″N 0°35′03″E﻿ / ﻿50.8660°N 0.5841°E﻿ / 50.8660; 0.5841﻿ (St Mark's United Reformed Church, Blacklands) 	United Reformed Church 	D ﻿– 	According to Sussex church historian Robert Elleray, the predecessor of the present church was architect Thomas Elworthy's chef d'œuvre. It had a tower, spire and terracotta-edged red brickwork, but was demolished in 1972 to make way for a residential development with a church integrated into the ground floor. 	[17][46]
[112][136]
Calvary Chapel 	Robsack Centre, Bodiam Drive, Hollington, Hastings.jpg 	Hollington
50°52′14″N 0°31′31″E﻿ / ﻿50.8706°N 0.5252°E﻿ / 50.8706; 0.5252﻿ (Calvary Chapel, Hollington) 	Evangelical 	D ﻿– 	One of a fellowship of more than 40 Calvary Chapels in the United Kingdom and Ireland, this Evangelical congregation moved from a community centre in Hollington to a new building elsewhere in that suburb in 2009. 	[46][137]
[138]
King's Church 	Kings Church, The Ridge, Hastings.jpg 	St Helen's
50°53′13″N 0°34′12″E﻿ / ﻿50.8869°N 0.5699°E﻿ / 50.8869; 0.5699﻿ (King's Church, St Helen's) 	Evangelical 	D ﻿– 	This Evangelical church has its origins in a house church established in 1974. Terry Virgo, founder of the Newfrontiers movement, was involved later, and as the congregation grew it took over a building previously used for indoor cricket. 	[46][139]
The Tabernacle 	The Tabernacle (Free Church), Hastings.JPG 	Hastings
50°51′20″N 0°34′38″E﻿ / ﻿50.8555°N 0.5771°E﻿ / 50.8555; 0.5771﻿ (The Tabernacle, Hastings) 	Evangelical 	D ﻿– 	Charles Pavey founded this church in the town centre in 1854 for an Independent Calvinistic congregation. By the 1970s it had become a Free Evangelical church, and the interior fittings were reordered to cater for the different form of worship. The exterior is unchanged, however: the yellow-brick and stone building has a steeply gabled porch set below five tall lancet windows. 	[115][112]
[46][123]
Church of St Thomas of Canterbury and English Martyrs 	Church of St Thomas of Canterbury and English Martyrs, St Leonards, Hastings (IoE Code 495311).JPG 	St Leonards-on-Sea
50°51′23″N 0°33′54″E﻿ / ﻿50.8565°N 0.5649°E﻿ / 50.8565; 0.5649﻿ (Church of St Thomas of Canterbury and English Martyrs, St Leonards-on-Sea) 	Roman Catholic 	C ﻿II 	The Roman Catholic community moved from the chapel at the Holy Child of Jesus Convent into a new church nearby in 1866. It burnt down in 1887; Charles Alban Buckler's new Gothic Revival building was ready in 1889. Its plain ironstone and Bath stone exterior hides an elaborately decorated interior with rib vaults and wall murals. 	[17][46]
[18][140]
[141]
St Mary Star of the Sea Church 	Church of St Mary Star of the Sea, Old Town, Hastings (IoE Code 293874).JPG 	Old Town
50°51′34″N 0°35′40″E﻿ / ﻿50.8594°N 0.5944°E﻿ / 50.8594; 0.5944﻿ (St Mary Star of the Sea Church, Old Town) 	Roman Catholic 	C ﻿II 	In 1882 poet Coventry Patmore, living in Hastings at the time, commissioned his friend Basil Champneys to build a large, ornate church in memory of his wife. The Decorated/Perpendicular Gothic flint building, on a sloping site, has a crypt underneath and a very high east end with a large window. Inside, the nave continues into the chancel. There is a bellcote but no tower. 	[64][46]
[21][142]
[143][144]
[145][146]
[147]
Church of the Holy Redeemer 	Church of the Holy Redeemer, Hollington, Hastings.jpg 	Hollington
50°52′42″N 0°33′14″E﻿ / ﻿50.8782°N 0.5538°E﻿ / 50.8782; 0.5538﻿ (Church of the Holy Redeemer, Hollington) 	Roman Catholic 	D ﻿– 	The Roman Catholic church serving Silverhill and Hollington was opened in 1934. It was added to the parish of St Leonards-on-Sea in 1959. The plain brick Vernacular structure, designed by Wilfred Mangan, was greatly extended and reoriented in the 1980s. 	[46][148]
[149][150]
[151]
Kingdom Hall 	Jehovah's Witnesses Kingdom Hall, Hollington, Hastings.JPG 	Hollington
50°52′20″N 0°31′53″E﻿ / ﻿50.8723°N 0.5315°E﻿ / 50.8723; 0.5315﻿ (Kingdom Hall, Hollington) 	Jehovah's Witnesses 	D ﻿– 	This modern Kingdom Hall stands on Churchwood Drive in the west of Hollington. It was opened on 12 March 1988 and can hold 250 people. 	[46][152]
Kingdom Hall 	Jehovah's Witnesses Kingdom Hall, St Leonards, Hastings.JPG 	St Leonards-on-Sea
50°51′19″N 0°33′21″E﻿ / ﻿50.8552°N 0.5559°E﻿ / 50.8552; 0.5559﻿ (Kingdom Hall, St Leonards-on-Sea) 	Jehovah's Witnesses 	D ﻿– 	This Kingdom Hall's congregation was added to in 1998 when members of the Halton Kingdom Hall in St George's Road were displaced: their building was demolished to make way for Southern Water's large cross-town drainage tunnel. 	[112][153]
[154]
Elim Church Centre 	Elim Church Centre, Elphinstone Avenue, Hastings.jpg 	Blacklands
50°52′09″N 0°35′00″E﻿ / ﻿50.8693°N 0.5834°E﻿ / 50.8693; 0.5834﻿ (Elim Church Centre, Blacklands) 	Pentecostalist 	D ﻿– 	The congregation, established in the 1950s, worshipped at a hall in the centre of Hastings until the mid-1980s, when they acquired a site on Elphinstone Avenue in Blacklands and built a permanent church. 	[46][112]
[155][156]
His Place Community Church Centre 	His Place Commuity Church, Silverhill, Hastings.jpg 	Silverhill
50°52′14″N 0°33′26″E﻿ / ﻿50.8705°N 0.5573°E﻿ / 50.8705; 0.5573﻿ (His Place Community Church Centre, Silverhill) 	Pentecostalist 	D ﻿– 	St Matthew's Church founded a mission hall on Duke Road, Silverhill, in 1912. In 1994, after a period as an artificial flower factory, it became the home of the St Leonards Assemblies of God Pentecostal Church, founded as a house church in 1985. It now operates under the name "His Place". 	[157][158]
Hastings Citadel 	Salvation Army Citadel, Hastings.JPG 	Hastings
50°51′31″N 0°34′55″E﻿ / ﻿50.8585°N 0.5819°E﻿ / 50.8585; 0.5819﻿ (Hastings Citadel, Hastings) 	Salvation Army 	D ﻿– 	The 1880s brick building, enlarged in 1937, has always been known as the "Iron Fort" locally. It was the focus of anti-Salvation Army riots as soon as it was founded: youths picked up on the ill-feeling displayed in other Sussex towns and formed their own "Skeleton Army" to attack the building and its members. 	[46][159]
[160]
Hastings Temple 	Hastings Temple (Salvation Army), Ore, Hastings.JPG 	Ore
50°52′16″N 0°36′25″E﻿ / ﻿50.8712°N 0.6069°E﻿ / 50.8712; 0.6069﻿ (Hastings Temple, Ore) 	Salvation Army 	D ﻿– 	The Salvation Army established their second place of worship in Hastings in 1935. The small white-painted building opened as the Cynthia Cinema (advertised as "the cheapest, the cosiest and the best") in June 1913, but it lasted just ten years and was later used to store furniture. 	[46][161]
Christian Spiritualist Church 	Christian Spiritualist Church, Claremont, Hastings.jpg 	Hastings
50°51′17″N 0°34′33″E﻿ / ﻿50.8546°N 0.5758°E﻿ / 50.8546; 0.5758﻿ (Christian Spiritualist Church, Hastings) 	Spiritualist 	D ﻿– 	One of several Spiritualist churches in Sussex, this is based in buildings at Claremont, facing the sea near Holy Trinity Church. 	[112]
Hastings Brotherhood Church 	Hastings Brotherhood Spiritualist Church, Hastings.JPG 	Hastings
50°51′27″N 0°35′00″E﻿ / ﻿50.8575°N 0.5832°E﻿ / 50.8575; 0.5832﻿ (Hastings Brotherhood Church, Hastings) 	Spiritualist 	D ﻿– 	This church, on Portland Place in central Hastings, is part of the Spiritualists' National Union. 	[46][112]
[162]
Bethel Full Gospel Church Centre 	Bethel Full Gospel Church Centre, Priory Road, Hastings.jpg 	Halton
50°52′06″N 0°36′01″E﻿ / ﻿50.8684°N 0.6003°E﻿ / 50.8684; 0.6003﻿ (Bethel Full Gospel Church Centre, Halton) 	Assemblies of God 	D ﻿– 	This Pentecostal group took over the former St Mary-of-the-Castle Church in Pelham Crescent in 1970, but did not have enough money to maintain the listed building. Hastings Borough Council later bought the building, and a new church centre was established on Priory Road in Halton. 	[163][152]
[164][125]
[165]
Alexandra Gospel Hall 	Alexandra Gospel Hall, Silverhill, Hastings.JPG 	Silverhill
50°52′13″N 0°33′29″E﻿ / ﻿50.8702°N 0.5580°E﻿ / 50.8702; 0.5580﻿ (Alexandra Gospel Hall, Silverhill) 	Christian Brethren 	D ﻿– 	This Gospel Hall, a Brethren place of worship, was built in 1964 on Sedlescombe Road North, replacing a pair of houses. Its small congregation was boosted in 1990 when members of the former Castle Hill Gospel Hall joined. 	[46][125]
[166]
St Mary Magdalen's Church 	St Mary Magdalene's Church (Greek Orthodox), St Leonards (IoE Code 294069).jpg 	St Leonards-on-Sea
50°51′12″N 0°33′54″E﻿ / ﻿50.8532°N 0.5650°E﻿ / 50.8532; 0.5650﻿ (St Mary Magdalen's Church, St Leonards-on-Sea) 	Greek Orthodox 	C ﻿II 	This Anglican church of 1852, on a prominent sloping corner site (a characteristic feature of churches in Hastings and St Leonards-on-Sea), was one of Frederick Marrable's early works. The Decorated Gothic-style stone church has a tall turreted corner tower, added in 1872. Declared redundant by the Diocese of Chichester on 17 December 1980, it was sold to the Greek Orthodox Church and continues in use under the same dedication. 	[64][46]
[21][167]
[168][169]
[170]
The Independent Church 	Independent Church, St Leonards, Hastings.JPG 	St Leonards-on-Sea
50°51′29″N 0°33′01″E﻿ / ﻿50.8581°N 0.5502°E﻿ / 50.8581; 0.5502﻿ (The Independent Church, St Leonards-on-Sea) 	Independent 	D ﻿– 	This tiny chapel, on Albany Road in Upper St Leonards, offers services in a charismatic evangelical style. 	[171]
Church of Jesus Christ of Latter Day Saints 	Church of Jesus Christ of Latter-day Saints, Hollington, Hastings.jpg 	Silverhill Park
50°52′51″N 0°33′11″E﻿ / ﻿50.8808°N 0.5530°E﻿ / 50.8808; 0.5530﻿ (Church of Jesus Christ of Latter Day Saints, Silverhill Park) 	Mormon 	D ﻿– 	Situated on Ledsham Avenue just off the main road to Battle, this Mormon church forms the Hastings Ward of the Church of Jesus Christ of Latter Day Saints' Maidstone Stake. 	[46][112]
[172][173]
Masjid al-Haq 	Hastings Mosque, St Leonards, Hastings.JPG 	St Leonards-on-Sea
50°51′08″N 0°33′27″E﻿ / ﻿50.8523°N 0.5574°E﻿ / 50.8523; 0.5574﻿ (Masjid al-Haq, St Leonards-on-Sea) 	Muslim 	D ﻿– 	James Burton's plans for his St Leonards-on-Sea development included Mercatoria, an inland marketplace. This use did not last long, and in 1847 his son Decimus built a National School on the site. It was the area's only school for the next 26 years, and it was still used until a much larger building was opened elsewhere in the town in 1978. The East Sussex Islamic Association bought the building in the mid-1980s and converted it into a mosque. 	[46][174]
Friends Meeting House 	Quaker Friends Meeting House, Hastings.JPG 	Hastings
50°51′29″N 0°34′53″E﻿ / ﻿50.8581°N 0.5815°E﻿ / 50.8581; 0.5815﻿ (Friends Meeting House, Hastings) 	Quaker 	D ﻿– 	Quakers in the Hastings area meet at this Renaissance-style building on South Terrace in the town centre. It was founded in 1864; John Horniman, a Quaker tea trader who patented a new tea-packing process, gave money. The altered front is stuccoed. 	[115][112]
[46][175]
Hastings Seventh-Day Adventist Community Church 	Seventh-Day Adventist Church, Ore, Hastings.JPG 	Ore
50°52′21″N 0°36′35″E﻿ / ﻿50.8725°N 0.6096°E﻿ / 50.8725; 0.6096﻿ (Hastings Seventh-Day Adventist Community Church, Ore) 	Seventh-day Adventist 	D ﻿– 	This small building used by the Seventh-day Adventist community of Hastings stands on the road to Fairlight. 	[46][112]
[176]
Hastings Unitarian Church 	Unitarian Church, Hastings.JPG 	Hastings
50°51′30″N 0°34′52″E﻿ / ﻿50.8582°N 0.5812°E﻿ / 50.8582; 0.5812﻿ (Hastings Unitarian Church, Hastings) 	Unitarian 	D ﻿– 	Governor of Hong Kong, hyperpolyglot and Unitarian John Bowring founded this church on South Terrace in May 1868. The town's Unitarian community formed eight years earlier and previously met in a music hall and an inn. It has a painted stucco façade and is a late example of Neoclassical architecture. 	[115][46]
[112][177]
[178]
[edit] Closed or disused places of worship
Name↓ 	Image 	Location/
Coordinates↓ 	Denomination/
Affiliation↓ 	Grade↓ 	Notes 	Refs
St Mary-in-the-Castle Church 	Former St Mary-in-the-Castle Church, Old Town, Hastings (IoE Code 294035).JPG 	Hastings
50°51′20″N 0°35′05″E﻿ / ﻿50.8556°N 0.5846°E﻿ / 50.8556; 0.5846﻿ (Former St Mary-in-the-Castle Church, Hastings) 	Anglican 	B ﻿II* 	The successor to an 11th-century collegiate church inside Hastings Castle, this Classical stuccoed church with Ionic columns formed the centrepiece of Joseph Kay's Pelham Crescent residential development on the seafront. Springs flowed from the cliff behind into a total immersion baptismal pool—rare in an Anglican church. It closed in 1970 and is now an arts centre. 	[64][10]
[21][120]
[144][165]
[125][163]
[179][180]
[181]
°N 0.5909°E﻿ / 50.8649; 0.5909﻿ (Former Bethel Full Gospel Church, Halton) 	Pentecostalist 	D ﻿– 	This Pentecostalist group used this late 19th-century building, in the middle of a terrace of houses on St George's Road, until at least the mid-1970s, but it is no longer in religious use. 	[112]
Ebenezer Particular Baptist Chapel 	Former Ebenezer Particular Baptist Chapel, Old Town, Hastings (IoE Code 293813).JPG 	Old Town
50°51′31″N 0°35′46″E﻿ / ﻿50.8585°N 0.5961°E﻿ / 50.8585; 0.5961﻿ (Former Ebenezer Particular Baptist Chapel, Old Town) 	Strict Baptist 	C ﻿II 	Successor to a Strict Baptist chapel called Cow Lodge near the beach at Rock-a-Nore, this chapel was founded nearby in 1817 by a member of its congregation. It grew in popularity throughout the 19th century, and regular extensions were made; but it closed by the end of the 20th century and has been converted into a house. The Neoclassical structure retains its stuccoed façade, pilasters, pediment and cornice.

Church in the Wood, Hollington
From Wikipedia, the free encyclopedia
Jump to:navigation, search
Church in the Wood
St Leonard's Church

The church from the south
50°52′27″N 0°32′17″E﻿ / ﻿50.8743°N 0.5380°E﻿ / 50.8743; 0.5380Coordinates: 50°52′27″N 0°32′17″E﻿ / ﻿50.8743°N 0.5380°E﻿ / 50.8743; 0.5380
Location 	Church in the Wood Lane, Hollington, Hastings, East Sussex
Country 	United Kingdom
Denomination 	Church of England
Website 	www.stleonardsandstannes.org.uk
History
Former name(s) 	St Rumbold's Church
Founded 	11th century
Dedication 	St Leonard
Architecture
Status 	Parish church
Functional status 	Active
Heritage designation 	Grade II
Designated 	19 January 1951
Style 	Early English Gothic
Administration
Parish 	Hollington: St Leonard and St Anne
Deanery 	Rural Deanery of Hastings
Archdeaconry 	Lewes and Hastings
Diocese 	Chichester
Province 	Canterbury
The chancel is lower than the nave, and the tower has a tiled pyramidal cap.
The church has a large, secluded graveyard.

Church in the Wood, officially known as St Leonard's Church and originally as St Rumbold's Church, is an Anglican church in the Hollington area of the town and borough of Hastings, one of six local government districts in the English county of East Sussex. Although Hollington is now a large suburb, consisting mostly of postwar residential development, the church has stood in isolation in the middle of an ancient wood since it was founded in the 13th century—almost certainly as the successor to an 11th-century chapel. Restoration work in the Victorian era has given the Early English Gothic-style building its present appearance, but some medieval work remains. Legends and miraculous events have been associated with the church, and its secluded situation has been praised by writers including Charles Lamb. English Heritage has listed the building at Grade II for its architectural and historical importance.
Contents
[hide]

    * 1 History
    * 2 Architecture
    * 3 Legends and portrayals
    * 4 The church today
    * 5 See also
    * 6 References
          o 6.1 Notes
          o 6.2 Bibliography

[edit] History

The manor of Hollington (also spelt Horintune, Holintun, Horintone, Halinton and Halyngtone early in its existence)[1][2] existed at the time of the Domesday survey in 1086: before the Norman conquest of England the land was held by Godwin, Earl of Wessex and another lord, Alstan, and amounted to 4½ hides. Afterwards it passed to Robert, Count of Eu.[1][3] The manor house and village (a scattered, mostly rural settlement rather than a nucleated village) were 2.5 miles (4.0 km) northwest of Hastings and a similar distance north of St Leonards-on-Sea on a ridge of high land.[1][4]

No chapel or church was mentioned in the Domesday survey, but the will of a later Count of Eu written in 1139 states that one existed on the present site in 1090, suggesting possible pre-Norman origins.[5] There were two other places of worship in the parish at the time—a church in the manor of Filsham, and another chapel at Wilting. Filsham was the more important manor,[2] but its church (and the chapel at Wilting) had disappeared by the time Pope Nicholas IV ordered a census of all places of worship for taxation purposes in 1291.[6] (Wilting's probably disappeared in the early 12th century,[7] although a field and a wood in the area still bear the chapel's name.)[2] A suggested reason for the isolation of the chapel—which was about 0.5 miles (0.80 km) from the nearest house until after World War II—was that the ownership of the manor transferred at an early date to the Lord of the manor of nearby Crowhurst, who abandoned Hollington's manor house and the surrounding area because they were superfluous. The chapel, standing nearby, was then the only building still in use, and a wood grew up around it.[8]

The chapel is known to have been replaced by a church in the mid-13th century.[9] The first mention of a vicar was in 1288, but the first whose name has been recorded was John de Levenyngton, who moved to All Saints Church in Hastings Old Town in 1344.[5][9]

The dedication to St Leonard came about over the course of many years by mistaken association with a former church of that dedication. In 1291, a St Leonard's Church was one of seven medieval churches recorded in Hastings,[10] along with St Margaret's, St Michael's, St Peter's, St Andrew's, St Clement's and All Saints. Only the last two survive in the 21st century, and St Leonard's Church—situated near Norman Road in present-day St Leonards-on-Sea—was lost in the early 15th century (no later than 1428).[11] Thereafter, it became a "free chapel" in Hollington's parish, and its former parishioners travelled to Hollington to worship.[12][13] The original dedication of Church in the Wood, first recorded in 1562, was to St Rumbold.[13][14] Gradually over the next 150 years, the name "St Leonard's Church" began to be applied to it; the first record of this was in 1712. From the mid-19th century, the epithet "Church in the Wood" gained popularity after James Burton founded a new St Leonard's Church in the centre of his St Leonards-on-Sea resort and a second church (St John the Evangelist's) was established in Hollington.[13][14][15]

The church continued to serve the parish despite its isolation, but it declined into a poor structural state by the mid-19th century: in 1834 it was described as "in such a general state of decay as to make any attempt to repair it inadvisable". Sir Charles Montolieu Lamb (no relation to the writer Charles Lamb),[16] who lived at Beauport Park, a nearby mansion, wanted the church to be closed and a replacement built nearer his land, but parishioners demanded that the ancient church be repaired instead. Work started in 1847 and continued for nearly 20 years:[5] the church closed for a time in 1861 while repairs were made, then Matilda Dampner paid for a complete restoration in 1865 to commemorate her parents. The church closed again while this was carried out, and reopened in 1866.[9][17] The renovation was so substantial that it gave the church a mostly Victorian appearance, and only a few fragments of Norman-era fabric remained.[18]

Church in the Wood's parish was reduced in size in 1870 when some of its territory was given to the newly built St John the Evangelist's Church. Hollington became part of the Borough of Hastings in 1897.[19] After this, little change occurred in the church itself, although a lychgate was built at the entrance to the churchyard in 1937,[20] general repairs were made in 1964 and an extension was added in 1977 to form a parish room and vestry.[16][21] The churchyard is large and has been used for centuries—the first recorded burial took place in 1606, and the oldest surviving gravestone dates from 1678—and it is the last remaining private burial ground in the Borough of Hastings.[16]

Hollington grew substantially in the 19th and 20th centuries, in line with the rest of Hastings. The population was 338 in 1831, 3,677 in 1931 and about 7,500 in 1939. Nevertheless, it retained its village character until the 1960s,[6] when the borough council selected it as the site for a large council estate. The expansion of the 1920s and 1930s had been driven by council housing,[22] and in 1962 the council proposed to continue and extend this scheme by demolishing much of the old village and laying out a large housing estate. More private and council housing has since been added, and Hollington has now lost its rural setting.[23] The ancient woodland around the church has been preserved, though, and is one of six nature reserves in the borough of Hastings.[24]
[edit] Architecture
The east end has a stained glass window by Jean-Baptiste Capronnier.

The 19th-century restoration of Church in the Wood defines its present appearance.[18] The only surviving work from the medieval era is in the west and north walls of the nave and in the tower, where some stonework, timbers and an ancient bell survive.[9][5] The church is built of stone rubble, the roof is tiled, and the pyramid-shaped cap on the tower is tile-hung;[9] it was originally weatherboarded.[21] The bell, cast between 1371 and 1392, is the oldest in Hastings and one of the oldest in Sussex.[5]

The plan includes a nave of three bays, a chancel (rebuilt in 1865 at a lower height, and having two bays), an ornate chancel arch (added at the same time), a low two-stage bell tower at the west end, a vestry, a porch and a modern parish hall.[21][9] The stone used is mostly locally quarried bluestone with some dressings of Bath stone and Caen stone.[13]

The architectural style is Early English Gothic.[9] The east window is a three-light lancet with tracery in the Decorated Gothic style.[21] Jean-Baptiste Capronnier added a stained glass depiction of the Resurrection to it in 1873.[9][25] Faith and Hope are depicted in an 1886 window in the south side, and Charity is shown in a north-wall window of 1875.[9] These were probably by the Clayton and Bell firm.[21]
[edit] Legends and portrayals
Church in the Wood's seclusion has given rise to stories and legends.

The location of Church in the Wood is explained by a story (also attributed in various forms to other churches in Sussex) involving a battle between the Devil and the builders. Every night, the work carried out the previous day would be destroyed and the materials taken away. A voice spoke to the builders, claiming that the site belonged to the Devil and demanding the church be built elsewhere. The church was successfully rebuilt on a new site to the Devil's liking, and a wood grew around it to conceal it (either from the Devil's influence or to hide it from the parishioners). The legend probably dates from the Middle Ages.[15][26]

A miracle was attributed to a former vicar of the church in 1488. It is said that he was robbed by three men while on his way to conduct a service, and had his tongue and eyes cut out to stop him recognising or describing the robbers. His sight and speech were purportedly restored in time for him to give evidence and bring them to justice.[27]

Nationally known and local writers have frequently remarked on the solitude and visual appeal of Church in the Wood. A description of 1777 noted that although "the graveyard now contains many handsome monuments ... it still retains something of the sequestered situation ... in the middle of a wood".[28] A description from 1874 stated simply that "the Church is picturesquely situated in the heart of a romantic wood, having no hut or house of any kind within a quarter of a mile".[28][29] Diplock's Hastings Guidebook of 1845 stated "[i]t is the singularity of [its] situation, more than anything in the building itself, that generally attracts visitors ... it looks as if having been forsaken by all human visitants, a thicket had grown up and enclosed it like the Castle of the Sleeping Beauty in the old fairy tale".[30] London essayist Charles Lamb wrote the best-known description after visiting Hastings in 1823:

    The best thing I hit upon was a small country Church (by whom or when built unknown), standing bare and single in the midst of a grove, with no house or appearance of habitation within a quarter of a mile, only passages diverging from it through beautiful woods, to so many farm houses. There it stands, like the first idea of a Church, before parishioners were thought of, nothing but birds for its congregation...
    —Charles Lamb, 1823

[edit] The church today
St Anne's Church was founded in 1956.

Church in the Wood was listed at Grade II by English Heritage on 19 January 1951;[9] this defines it as a "nationally important" building of "special interest".[31] As of February 2001, it was one of 521 Grade II listed buildings, and 535 listed buildings of all grades, in the borough of Hastings.[32]

The parish of St Leonard includes a second Anglican church—the modern St Anne's Church on Chambers Road (at 50°52′20″N 0°32′36″E﻿ / ﻿50.8723°N 0.5434°E﻿ / 50.8723; 0.5434). This was built over several years from 1956 to serve the rapidly expanding council estate west of the Battle Road.[33] The approximate boundaries of the parish are Battle Road, between the borough boundary in the north and Hollington Old Lane; the residential streets between Hollington and neighbouring Silverhill; the railway line between West St Leonards and Crowhurst railway stations (including some of the rural land west of it); and Breadsell Lane.[34]

There is one service every Sunday at 10.30am at Church in the Wood and one every Sunday at 9.30am and St Anne's Church, except in months with five Sundays; on these occasions, the churches hold a joint service.[35]
Rachtig in Rheinland-Pfalz, Germany. The bridge is part of a new highway connection, the "Hochmoselübergang" or Federal Highway 50 (Bundesstrasse 50), to facilitate traffic between Belgian and Dutch harbors and the greater Frankfurt area.
Contents
[hide]

    * 1 Construction
    * 2 Opposition
    * 3 Literature
    * 4 References
    * 5 See also
    * 6 External links

[edit] Construction

A proposal for the highway and bridge was first made three decades ago for strategic reasons during the cold war.[1] The project was reactivated to link the Frankfurt area, specifically the Hahn airport,[2] to the Belgian and Dutch harbors with updated plans drawn in the early 21st century.

The plan calls for a 1702.4 m long steel box beam bridge that crosses the river at a maximum height of 158 m. The width of the bridge will be 29.0 m to allow four-lane traffic. Ten monolithic pylons made from concrete will support the bridge; their height varies between 15 and 150 m.[3] The estimated costs are 270 million euros.[4] The project is to be completed by 2016.[3]
[edit] Opposition

Opposition to the project is based on questions of its economical necessity and the negative ecological impact on the Mosel wine region. It is indicated that current available highway connections between the Belgian and Dutch harbors and Frankfurt are adequate, and the proposed highway with its bridge would not present a shortcut and thus would not be economically advantageous. More importantly, perhaps, is the concern that the "ungainly"[1] bridge would destroy a historical cultural section of major significance within the Mosel wine region.[5] The section is home to some of the finest and most historical vineyards of Germany[1] Affected vineyards include the premier riesling areas of the Wehlener Sonnenuhr, Graacher Himmelreich, Ürziger Würzgarten and Bernkasteler Doktor. International wine experts, among them Hugh Johnson, Jancis Robinson and Stuart Pigott, are opposing the project and fear that the unique microclimate responsible for the Mosel rieslings will be impacted.[4][1]
Zaans Museum
From Wikipedia, the free encyclopedia
Jump to:navigation, search

Coordinates: 52°28′24″N 4°49′20″E﻿ / ﻿52.473333°N 4.822222°E﻿ / 52.473333; 4.822222
Zaans Museum
Zaans Museum is located in Amsterdam
Zaans Museum
Location in Zaandam (northwest of Amsterdam)
Established 	1994
Location 	Zaanse Schans, Zaandam, Netherlands
Website 	www.zaansmuseum.nl (English)

Zaans Museum is a museum in Zaandam, Netherlands, founded in 1994. It is housed in a contemporary building designed by architects Cor van Hillo and Monique Verschaeren,[1] right across from the historic windmills and houses of the Zaanse Schans, an open-air museum. The museum covers 16,500 m3 and contains items showcasing the past of the area of the Zaan, and the wealth created by the early industrial activity on the river. The building was expanded in March 2009 with a new pavilion, to house the corporate collection of the Verkade family.
Contents
[hide]

    * 1 Collection
    * 2 Tsar Peter house
    * 3 Awards
    * 4 Criticism
    * 5 References
    * 6 External links

[edit] Collection

The museum contains cultural heritage and region-related collections on residential and industrial culture. It is based on the collection of the Society for the Preservation and Expansion der Zaan Greenland Archaeological Collection Honig Jacob Janszoon Junior. The museum has two sub-collections, living culture and industrial culture. Within the living culture collection, the museum displays regional dress, painted furniture and utensils found in Zaandam homes. The collection includes industrial heritage by large companies like Zaanse Bruynzeel, Honig, Albert Heijn and Lassie. In 2009, the museum acquired an oil painting similar to Claude Monet's Moulin en Hollande showing the river Zaan and its many windmills by the Belgian impressionist Franz Courtens.[2][3]

The Verkade pavilion, which was opened in March 2009 by Queen Beatrix,[4] houses the corporate collection of the Verkade family, founders of Verkade, a company famous for its cookies and chocolate. This collection contains photographs, displays, packaging, posters and three operating production lines for chocolate, sponge cake and candles. There is also a treasure house containing the original watercolors of the collectors' albums it published. One section is dedicated to the "Verkade girls" (De meisjes van Verkade). In the early days of Verkade, a large part of the workforce consisted of young women who walked in their company uniform to work; these are occasionally revived for ceremonial purposes, such as when Queen Beatrix opened the pavilion.[4] The section dedicated to the girls is sponsored for an amount of €60,000 by PDZ, one of the country's largest temporary work agencies, which was one of the main providers of women to Verkade's work force since the 1960s.[5]
[edit] Tsar Peter house

The museum also oversees the Tsar Peter house in Zaandam, a little wooden house formerly occupied by Peter I of Russia while he was studying shipbuilding at the end of the seventeenth century.
[edit] Awards

In 2001 the museum received a commendation at the presentation of the annual European Museum of the Year Award;[6] the jury praised especially the modern and open architecture of the building and the educational programs and the presentation of the permanent collection. The lighting design in the presentation won an Award of Merit at the 2000 Edison Awards.[7]
[edit] Criticism

Criticism of the Zaans Museum comes from, for instance, the inhabitants of the Zaanse Schans. The historic wooden houses on the Schans are rented out by the foundation that owns them, and renters have complained about maintenance problems; according to tenants, all available money was spent on the Museum ("a block of concrete") while €4.5 million is necessary to catch up with problems such as rot.[8]
Parashqevi Qiriazi
From Wikipedia, the free encyclopedia
Jump to:navigation, search
Parashqevi Qiriazi

Parashqevi Qiriazi
Born 	1880
Died 	1970 (aged 90)
Ethnicity 	Albanian
Home town 	Monastir, Ottoman Empire (now Republic of Macedonia[1]
Religion 	Protestant[1]
Relatives 	sister of Gjerasim Qiriazi (1858-1894), Gjergj Qiriazi (1868-1912), and Sevasti Qiriazi (1871-1949)[1]

Parashqevi Qiriazi (1880-1970) was an Albanian teacher who dedicated her life to the Albanian alphabet and to the instruction of written Albanian language. She was a woman participant at the Congress of Manastir, which decided the form of the Albanian alphabet,[2] and the founder of the Yll' i Mengjesit, a women's association.[3] Parashqevi was also a participant in the Paris Peace Conference, 1919 as a member of the Albanian-American community.[4] She was the sister of Sevasti Qiriazi, who was the director of the Mësonjëtorja, the first Albanian school to open in 1887.[5]
[edit] Biography

Parashqevi was born in Monastir, Ottoman Empire (now Republic of Macedonia).[1] When she was only 11 she started to help her brother Gjerasim Qiriazi and sister Sevasti Qiriazi to teach written Albanian to girls in the first school for girls in Albania, the Girls' School (Albanian: Shkolla e Vashave),[2] which opened in October 15 1891.[1][6]

She later studied at Robert College in Istanbul. Upon graduation she went to Korçë to work as an elementary teacher along with her sister, Sevasti at the Mësonjëtorja, the first Albanian school which had opened in 1887.[7]

In 1908 she was a participant in the Congress of Manastir and the only woman to be there.[2]

In 1909 she published an abecedarium for elementary schools. Although the Congress of Manastir had decided about the new alphabet, two versions of the alphabet were still present in her abecedary, which shows how fragile the consensus of the Congress still was. However, along with the abecedarium, she published some very well known verses on the defense of the Albanian new alphabet[8]:
Albanian 	English

Armiqëtë o shqipëtarë,
Po perpiqenë
Shkronjat turçe dhe greqishte,
të na apënë;
Le t'i mbajnë ata per vetëhe;
Kemi tonatë.
	

The enemies o Albanians,
Are trying
Turkish and Greek letters,
To give us;
Let them keep those letters;
We have ours.

She is also known for having organized teaching for children and night schools in other southern Albania villages and have also helped with the organization of local libraries.[7]

She contributed to the foundation of the Yll' i Mëngjesit association (Albanian: Morning Star) in 1909[9] and later, when she had emigrated to the USA, she continued to publish the periodical with the same name from 1917 to 1920.[3] The magazine was published every fortnight and consisted in Albania related articles which included politics, society, history, philology, literature, and folklore.[7]

In 1914 she left Albania for Romania along with her sister as a consequence of the Greek occupation of the city.[6]

She later went to the United States and became a member of the Albanian-American community, on behalf of which she participated in the Conference of Peace of Paris in 1919 to represent the rights of the Albanians.[10]
Break The Barrier
From Wikipedia, the free encyclopedia
Jump to:navigation, search
Break The Barrier
Information
Promotion 	N/A
Date 	May 15, 1999
Venue 	Viking Hall
City 	Philadelphia, Pennsylvania

Break The Barrier was a professional wrestling supercard held at Viking Hall, better known as the ECW Arena, in Philadelphia, Pennsylvania on May 15, 1999. The event was organized by the founders of Scoopswrestling.com, Al Isaacs, Remy Arteaga and Barbi Bistrowitz, and brought together some of the top independent wrestlers from around the country.[1] It was one of the biggest interpromotional events in the United States and represented by 12 independent promotions including Allied Powers Wrestling Federation, Combat Zone Wrestling, Extreme Championship Wrestling, Independent Pro Wrestling, Independent Professional Wrestling Alliance, Maryland Championship Wrestling, Music City Wrestling, NWA New Jersey, New Dimension Wrestling, Southern Greatest Wrestling Fans, Steel City Wrestling, World Legion Wrestling and World Wrestling Organization.[2] Pro Wrestling Illustrated has called it "one of the greatest Supercards of all time".[1]

Tom Brandi won the main event, an interpromotional battle royal, and presented a trophy from the promoter Al Isaacs. Instead of accepting the award, Brandi instead attacked Isaac and powerbombed him through a table. The undercard featured a "First blood" match between Fang (Allied Powers Wrestling Federation) and Blade Boudreaux (Southern Greatest Wrestling Fans). Ten minutes into the match, Abdullah the Butcher came out and attacked both men.[1][2] Abdullah cut up Fang with a fork and then rolled him into a coffin which he rolled backstage.

Two new champions were also crowned that night. Mike Quackenbush defeated "Beef Stew" Lou Marconi and Don Montoya to become the first SCW "Lord of the Dance" Champion and "Hardcore" Nick Gage defeated Justice Pain in an impromptu staple gun match to become the first CZW Hardcore Champion.[2] That night also saw the reunion of The Headbangers and Shane Douglas delivering a controversial "shoot" interview which ended with his quitting ECW one day before its Hardcore Heaven pay-per-view.[1][3]

History of Canada
From Wikipedia, the free encyclopedia
  (Redirected from History of canada)
Jump to:navigation, search
History of Canada
This article is part of a series
Timeline
Pre-Columbian era
1534–1763
1764–1866
1867–1914
1914–1945
1945–1960
1960–1981
1982–1992
1992–present
Topics
Constitutional history
Crown & Aboriginal history
Cultural history
Economic history
Immigration history
Military history
Monarchical history
Territorial evolution
History of Canada portal

The history of Canada begins with the arrival of Paleo-Indians thousands of years ago. Canada has been inhabited for millennia by Aboriginal peoples, who evolved trade, spiritual and social hierarchies systems. Some of these civilisations had long faded by the time of the first permanent European arrivals (c. late 15th - early 16th centuries), and have been discovered through archaeological investigations. Various laws, treaties, and legislation have been enacted between European settlers and the Indigenous populations.

Beginning in the late 15th century, French and British expeditions explored, and later settled, along the Atlantic coast. France ceded nearly all of its colonies in North America in 1763 after the Seven Years' War. In 1867, with the union of three British North American colonies through Confederation, Canada was formed as a federal dominion of four provinces. This began an accretion of provinces and territories and a process of increasing autonomy from the United Kingdom. This widening autonomy was highlighted by the Statute of Westminster of 1931 and culminated in the Canada Act of 1982, which severed the vestiges of legal dependence on the British parliament.

Over centuries, elements of Aboriginal traditions and immigrant customs have integrated to form a Canadian culture. Canada has also been strongly influenced by that of its linguistic, geographic and economic neighbour, the United States. Since the conclusion of the Second World War, Canada has been committed to multilateralism abroad and socioeconomic development domestically. Canada currently consists of ten provinces and three territories, and is governed as a parliamentary democracy and a constitutional monarchy with Queen Elizabeth II as its head of state.
Contents
[hide]

    * 1 Pre-Columbian era
          o 1.1 Paleo-Indians and Archaic periods
          o 1.2 Post-Archaic periods
    * 2 European contact
    * 3 New France 1534–1763
          o 3.1 Wars during the colonial era
    * 4 Canada under British control 1764–1867
          o 4.1 American Revolution and Loyalists
          o 4.2 War of 1812
          o 4.3 The Rebellions and the Durham Report
          o 4.4 Pacific colonies
          o 4.5 Confederation
    * 5 Post-Confederation Canada 1867–1914
    * 6 World Wars and Interwar Years 1915–1945
    * 7 The Post-war Era 1945–1960
    * 8 1960–1981
    * 9 1982–1992
    * 10 Recent history: 1992–present
    * 11 See also
    * 12 References
    * 13 Further reading
    * 14 External links

[edit] Pre-Columbian era
[edit] Paleo-Indians and Archaic periods
Main article: Paleo-Indians (Canada) and Archaic period (Canada)
Paleo-Indians hunting a glyptodont; work by Heinrich Harder, c.1920

According to North American archeological and Aboriginal genetic evidence, North and South America were the last continents in the world with human habitation.[1][2] During the Wisconsin glaciation, 50,000 — 17,000 years ago, falling sea levels allowed people to move across the Bering land bridge (Beringia) that joined Siberia to north west North America (Alaska).[3] At that point, they were blocked by the Laurentide ice sheet that covered most of Canada, which confined them to Alaska for thousands of years.[4]

Around 16,000 years ago, the glaciers began melting, allowing people to move south and east into Canada.[5] The exact dates and routes of the peopling of the New World are subject to ongoing debate.[2][6][7][8] Queen Charlotte Islands, Old Crow Flats, and Bluefish Caves are some of the earliest archaeological sites of Paleo-Indians in Canada.[9][10][11] Ice age hunter-gatherers left lithic flaked fluted stone tools and the remains of large butchered mammals.

The North American climate stabilized around 8000  before the Common Era BCE (10,000 years ago), climatic conditions were very similar to today's, however the receding glacial ice sheets still covered large portions of the land creating lakes of meltwater (the largest of these being the Great Lakes and Lakes Winnipeg, Winnipegosis, Manitoba, Athabasca and Reindeer, Great Bear and Great Slave Lakes).[12][13] The majority of population groups during the Archaic periods were still highly mobile hunter-gatherers; but now individual groups started to focused on resources available to them locally, thus with the passage of time there is a pattern of increasing regional generalization like, the Paleo-Arctic, Plano and Maritime Archaic traditions.[14]
[edit] Post-Archaic periods

    Main articles: First Nations history and Inuit history

A northerly section focusing on the Saugeen, Laurel and Point Peninsula complexes of the map showing south eastern United States and the Great Lakes area of Canada showing the Hopewell Interaction Sphere and in different colours the various local expressions of the Hopewell cultures, including the Laurel Complex, Saugeen Complex, Point Peninsula Complex, Marksville culture, Copena culture, Kansas City Hopewell, Swift Creek Culture, Goodall Focus, Crab Orchard culture and Havana Hopewell culture.
PP
S
L
A northerly section focusing on the Saugeen, Laurel and Point Peninsula complexes of the map showing south eastern United States and the Great Lakes area of Canada showing the Hopewell Interaction Sphere and in different colours the various local expressions of the Hopewell cultures, including the Laurel Complex, Saugeen Complex, Point Peninsula Complex, Marksville culture, Copena culture, Kansas City Hopewell, Swift Creek Culture, Goodall Focus, Crab Orchard culture and Havana Hopewell culture.
Great Lakes area of the Hopewell Interaction Area
PP=Point Peninsula Complex   S=Saugeen Complex   L=Laurel Complex

The Woodland cultural period dates from about 2,000 BCE — 1,000 Common Era (CE), and has locales in Ontario, Quebec, and Maritime regions.[12] The introduction of pottery distinguishes the Woodland culture from the earlier Archaic stage inhabitants. Laurentian people of southern Ontario manufactured the oldest pottery excavated to date in Canada.[1992][15] The population practicing sedentary agricultural life ways continued to increase on a diet of squash, corn, and bean crops.[15]

The Hopewell tradition is an Aboriginal culture that flourished along American rivers from 300 BCE — 500 CE. At its greatest extent, the Hopewell Exchange System networked cultures and societies with the peoples on the Canadian shores of Lake Ontario. Canadian expression of the Hopewellian peoples encompasses the Point Peninsula, Saugeen, and Laurel complexes.[16][17][18]
Algonquin couple, 18th c. watercolor. The first Algonquian encountered by the French were the 'Kichesipirini ("Ottawa River Men"), whose village was located on an island in the Ottawa River; the French called this group La Nation de l'Isle.

The eastern woodland areas of what became Canada were home to the Algonquian languages and Iroquoian languages peoples. The Algonquian language is believed to have originated in the western plateau region of Idaho or the plains of Montana and moved eastward,[19] eventually extending all the way from Hudson Bay to what is today Nova Scotia in the east and as far south as the tidewater area of Virginia. Speakers of eastern Algonquian languages included the Mi'kmaq and Abenaki of the Maritime region of Canada, and likely the extinct Beothuk of Newfoundland.[20][21] The Ojibwa and other Anishinaabe speakers of the central Algonquian languages, retain an oral tradition of having moved to their lands around the western and central Great Lakes from the sea, likely the east coast. According to oral tradition the Ojibwa formed the Council of Three Fires in 796 CE with the Odawa and the Potawatomi.[22]
Nuu-chah-nulth young girl - Northwestern University Library, Edward S. Curtis c.1916.

The Iroquois or Haudenosaunee were centered from at least 1000 CE in northern New York, but their influence extended into what is now southern Ontario and the Montreal area of modern Quebec.[23] The Iroquois Confederacy is, from oral tradition, to have been formed in 1142 CE.[24] In the early 1600s the Iroquois came into conflict with another Iroquoian people, the Wendat, (known also as the 'Hurons') of what is today southwestern Ontario.[25]

On the Great Plains the Cree or Nēhilawē (who spoke a closely-related Central Algonquian language the plains Cree language) depended on the vast herds of bison to supply food and many of their other needs.[26] To the north west were the peoples of the Na-Dene languages, which include the Athapaskan-speaking peoples and the Tlingit, who lived on the islands of southern Alaska and northern British Columbia. The Na-Dene language grouop is believed to be linked to the Yeniseian languages of Siberia.[27] The Dene of the western Arctic may represent a distinct wave of migration from Asia to North America.[27]

The Interior of British Columbia was home to the Salishan language groups such as the Shuswap (Secwepemc) and Okanagan and southern Athabaskan language groups, primarily the Dakelh (Carrier) and the Tsilhqot'in.[28] The inlets and valleys of the British Columbia Coast sheltered large distinctive populations, such as the Haida, Kwakwaka'wakw and Nuu-chah-nulth, sustained by the region's abundant salmon and shellfish.[28] These peoples developed complex cultures dependent on the western red cedar that included wooden houses, sea-going whaling and war canoes and elaborately-carved potlatch items and totem poles.[28] Defensive Salish trenchwork defences from the 1500s suggest a need for the southern Salish to take measures to protect themselves against their northern neighbours, who were known to mount raids into the Strait of Georgia and Puget Sound in historic times.[29][30]

In the Arctic archipelago, the distinctive Paleo-Eskimo know as Dorset peoples whose culture has been traced back to around 500 CE, were replaced by the ancestors of today's Inuit by 1500 CE.[31] This is supported by archaeological records and Inuit mythology that tells of having driven off the Tuniit or 'first inhabitants'.[32] Inuit traditional laws are anthropologically different from western law concepts. Customary law is thought non-existent in Inuit society before the introduction of the Canadian legal system.[33]
[edit] European contact
Further information: European colonization of the Americas
L'Anse aux Meadows on the island of Newfoundland, site of a Norsemen colony.

There are several reports of contact made before Christopher Columbus and the age of discovery between First Nations, Inuit and those from other continents. The earliest known documented European exploration in Canada are described in the Icelandic Sagas, which recount the attempted Norse colonization of the Americas.[34] According to the sagas, the first European to see Canada was Bjarni Herjólfsson, who was blown off course en route from Iceland to Greenland in the summer of 985 or 986 CE.[35] Around the year 1001 CE, the sagas then refer to Leif Ericson landing in three places to the west, the first two being Helluland (possibly Baffin Island) and Markland (possibly Labrador).[36] Leif's third landing was at a place he called Vinland (possibly Newfoundland).[37] Following Leif's voyage, several Norsemen groups attempted to colonize the new land, however were driven out by the local Indigenous peoples.[38][39] Archaeological evidence of a Norse (Viking) settlement was found in L'Anse aux Meadows, Newfoundland, which was declared a World Heritage site by UNESCO in 1978.[40][41]

The Portuguese Crown claimed it had territorial rights in the area visited by John Cabot in 1498 CE.[42] In 1501 and 1502 the Corte-Real brothers explored Newfoundland and Labrador, claiming them part of the Portuguese Empire.[43] In 1506, king Manuel I created taxes for the fisheries of cod in Newfoundland.[44] João Álvares Fagundes established fishing outposts in Newfoundland and Nova Scotia in 1521 CE, but they were abandoned with the Portuguese colonizers focusing their efforts on South America.[45] The extent and nature of Portuguese activity in Canada during the 16th century remains unclear and controversial.
[edit] New France 1534–1763
Main article: New France
Replica of Port Royal habitation, located at the Port-Royal National Historic Site of Canada, Nova-Scotia.

In 1534, Jacques Cartier planted a cross in the Gaspé Peninsula and claimed the land in the name of Francis I of France.[46] It was the first province of New France. However, initial French attempts at settling the region met with failure. French fishing fleets continued to sail to the Atlantic coast and into the Saint Lawrence River, making alliances with First Nations that would become important once France began to occupy the land.[46]

In 1604, a North American fur trade monopoly was granted to Pierre Dugua Sieur de Monts.[47] Dugua led his first colonization expedition to an island located near to the mouth of the St. Croix River. Among his lieutenants was a geographer named Samuel de Champlain, who promptly carried out a major exploration of the northeastern coastline of what is now the United States.[47] In the spring of 1605, under Samuel de Champlain, the new St. Croix settlement was moved to Port Royal (today's Annapolis Royal, Nova Scotia). It would be one of France's most successful New World colony and came to be known as Acadia. The colony of Acadia grew slowly, reaching a population of about 5,000 by 1713.[48]
Map of New France 1612
Map of New France by Samuel de Champlain from the 1612 edition of the "Carte geographique de la Nouvelle France".

After Champlain's founding of Quebec City in 1608, it became the capital of New France. Champlain took personal administration over the city and its affairs and sent out expeditions to explore the interior land. Champlain himself discovered Lake Champlain in 1609; and by 1615 he had traveled by canoe up the Ottawa River, through Lake Nipissing and through Georgian Bay to the center of Huron country, near Lake Simcoe.[49] During these voyages Champlain aided the Wendat (aka 'Hurons') in their battles against the Iroquois Confederacy.[50] As a result, the Iroquois would become enemies of the French. In 1629 Champlain suffered the humiliation of having to surrender his almost starving garrison to an English fleet, and he himself was taken prisoner back to England. Peace had been declared by England and France before the surrender, and the settlement was restored to French rule. Champlain would return from Europe to spend his remaining years in the colony. Champlain became the Governor of New France in 1633.[51]
[edit] Wars during the colonial era
Main article: French and Indian War

    Further information: North American - French and Indian Wars

Rupert's Land showing location of York Factory, owned by the Hudson's Bay Company from 1670 to 1870.

While the French were well established in large parts of eastern Canada, Britain had control over the Thirteen Colonies to the south; and laid claim (from 1670, via the Hudson's Bay Company) to Hudson Bay, and its drainage basin (known as Rupert's Land), as well as settlements in Newfoundland.[52] The British colonies were rapidly expanding, while the French fur traders and explorers were extended thinly.[52] La Salle's exploration of the Mississippi to its mouth in 1682 gave France a claim to a vast area bordering the American Colonies from the Great Lakes and the Ohio River valley southward to the Gulf of Mexico.[52][53] French expansion soon began to threaten Hudson's Bay Company clams, and, in 1686, Pierre Troyes led an overland expedition from Montreal to the shore of the bay where they managed to capture some areas.[54]
Map of North America in 1750, before the French and Indian War, that is part of the greater world-wide conflict known as the Seven Years' War (1756 to 1763). - possessions of Britain (pink), France (blue), and Spain (orange) -

Britain and France repeatedly went to war in the 17th and 18th centuries and made their colonial empires into battlefields. The first areas won by the British were the Maritime provinces. After Queen Anne's War, Nova Scotia, other than Cape Breton, was ceded to the British by the Treaty of Utrecht as well as the Hudson Bay territory conquered by France in the late 17th century.[55] As an immediate result of this setback, France founded the powerful Fortress of Louisbourg on Cape Breton Island. Louisbourg was intended to serve as a year-round military and naval base for France's remaining North American empire and also to protect the entrance to the Saint Lawrence River. During King George's War, an army of New Englanders led by William Pepperrell mounted an expedition of 90 vessels and 4,000 men against Louisbourg in 1745.[56] Within three months the New Englanders succeeded in forcing Louisbourg to surrender. The fall of Louisbourg to French control prompted the founding of Halifax in 1749 by the British under Edward Cornwallis.[57]
James Wolfe's victory - Battle of the Plains of Abraham - 1759 - by Hervey Smyth c.1797

The British ordered the Acadians expelled from their lands in 1755, an event called the Expulsion of the Acadians or le Grand Dérangement,[58] causing some 12,000 Acadians to be shipped to destinations throughout Britain's North American holdings and later even to France, Quebec and the French Caribbean colony of Saint-Domingue.[59] Many of the Acadians settled in southern Louisiana, creating the Cajun culture there. Some Acadians managed to hide and others eventually returned to Nova Scotia, but they were far outnumbered by a new migration of planters from New England who were settled on the former lands of the Acadians and transformed Nova Scotia from a colony of occupation to a settled colony with strong ties to New England.

During this time the French colony along the shores of the Saint Lawrence River continued to flourish, although French explorations and territorial claims to the Ohio Valley brought increasing conflict with the interests of Britain's American colonies. Inevitably the interests of the British and French in North America ran towards conflict resulting in the outbreak of war in both in Europe and North America. Canada was also an important battlefield in the Seven Years' War, during which Great Britain gained control of Quebec City and Montreal after the Battle of the Plains of Abraham in 1759, and the Battle of the Thousand Islands in 1760.[60] The British victory was overseen by Jeffrey Amherst.
[edit] Canada under British control 1764–1867
Map showing British territorial gains following the Treaty of Paris in pink, and Spanish territorial gains after the Treaty of Fontainebleau in yellow.
Main article: Canada under British Imperial control

With the end of the Seven Years' War and the signing of the Treaty of Paris (1763), France ceded almost all of its territory in mainland North America.[61] The new British rulers left alone much of the religious, political, and social culture of the French-speaking habitants, guaranteeing the right of the Canadiens to practice the Catholic faith and to the use of French civil law (now Quebec law) through the Quebec Act of 1774.[62] The Royal Proclamation of 1763 had been issued in October, by King George III following Great Britain's acquisition of French territory.[63] The purpose of the proclamation was to organize Great Britain's new North American empire and to stabilize relations between the British Crown and Aboriginal peoples through regulation of trade, settlement, and land purchases on the western frontier.[63]
[edit] American Revolution and Loyalists
Further information: Invasion of Canada (1775)
Death of General Montgomery - Battle of Quebec - 1775 - by John Trumbull c.1786.

During the American Revolution there was some sympathy for the American cause among the Canadiens and the New Englanders in Nova Scotia, neither parties joined the rebels, although several hundred individuals joined the revolutionary cause.[64][65] An invasion of Canada; by the Continental Army in 1775, to take Quebec from British control was halted at the Battle of Quebec, by Guy Carleton, with the assistance of local militias. The defeat of the British army during the Siege of Yorktown in October 1781, signaled the end of Britain's struggle to suppress the American Revolution. When the British evacuated New York City in 1783, they took many Loyalist refugees to Nova Scotia, while other Loyalists went to southwestern Quebec. So many Loyalists arrived on the shores of the St. John River that a separate colony—New Brunswick—was created in 1784;[66] followed in 1791 by the division of Quebec into the largely French-speaking Lower Canada along the Saint Lawrence River and Gaspé Peninsula and an anglophone Loyalist Upper Canada, with its capital settled by 1796 in York, in present-day Toronto.[67] After 1790 most of the new settlers were American farmers searching for new lands; although generally favorable to republicanism, they were relatively non-political and stayed neutral in the War of 1812.[68]

The signing of the Treaty of Paris 1783, formally ended the war. Britain made several concessions at the expense of the North American colonies.[69] Notably, the borders between Canada and the United States were officially declared.[69] Land South of the Great Lakes, which was formerly a part of the Province of Quebec and included large parts of modern day Michigan, Illinois and Ohio, was ceded to the Americans. Fishing rights were also granted to the United States in the Gulf of Saint Lawrence and on the coast of Newfoundland and the Grand Banks.[69]
[edit] War of 1812
Main article: War of 1812
Loyalist "Laura Secord" warning the British and First Nations of an impending American attack at Beaver Dams June 1813. - by Lorne Kidd Smith, c. 1920

The War of 1812 was fought between the United States and the British with the British North American colonies being heavily involved.[70] Greatly outgunned by the British Royal Navy, the American war plans focused on an invasion of Canada (especially what is today eastern and western Ontario), hoping to use it as a negotiating pawn. The American frontier states voted for war in order to suppress the First Nations raids that frustrated settlement of the frontier.[70] With invasion apparently imminent as reported by "loyalist" like Laura Secord[71] and Isaac Brock's foresight meant that Canada was not unprepared for the war.[72] Brock had continually kept the commanders of his posts on high alert informing them and First Nations allies of developments during the engagements.[72]

The war on the border with the U.S. was characterized by a series of multiple failed invasions and fiascos on both sides. American forces took control of Lake Erie in 1813, driving the British out of western Ontario, killing the Native American leader Tecumseh, and permanently breaking the military power of the First Nation.[73]

The War ended with the Treaty of Ghent of 1814, and the Rush–Bagot Treaty of 1817.[70] Neither side saw any land gains or losses; the only people who really lost were the Natives who fought for the British and lost their military power, their lands in the United States, and their access to prime fur trade areas. A demographic result was the shifting of American migration from Upper Canada to Ohio, Indiana and Michigan.[70] After the war, supporters of Britain tried to repress the republicanism in Canada, that was common among American immigrants to Canada.[70] The troubling memory of the war and the American invasions etched itself into the consciousness of Canadians as distrust of the intentions of the United States towards the British presence in North America.[74]pp. 254–255
[edit] The Rebellions and the Durham Report
Further information: Rebellions of 1837

The rebellions of 1837 against the British colonial government took place in both Upper and Lower Canada. In Upper Canada, a band of Reformers under the leadership of William Lyon Mackenzie took up arms in a disorganized and ultimately unsuccessful series of small-scale skirmishes around Toronto, London, and Hamilton.[75]
The Battle of Saint-Eustache - 1837 - by Lord Charles Beauclerk c.1840

In Lower Canada, a more substantial rebellion occurred against British rule. Both English- and French-Canadian rebels, sometimes using bases in the neutral United States, fought several skirmishes against the authorities. The towns of Chambly and Sorel were taken by the rebels, and Quebec City was isolated from the rest of the colony. Montreal rebel leader Robert Nelson read the "Declaration of Independence of Lower Canada" to a crowd assembled at the town of Napierville in 1838.[76] The rebellion of the Patriote movement were defeated after battles across Quebec. Hundreds were arrested, and several villages were burnt in reprisal.[76]

British Government then sent Lord Durham to examine the situation, he stayed in Canada only five months before returning to Britain, and brought with him, his Durham Report which strongly recommended responsible government.[77] A less well received recommendation was the amalgamation of Upper and Lower Canada for the deliberate assimilation of the French speaking population. The Canadas were merged into a single colony, United Province of Canada, by the 1840 Act of Union, with responsible government achieved in 1848, a few months after it was granted to Nova Scotia.[77]

Between the Napoleonic Wars and 1850 some 800,000 immigrants came to the colonies of British North America, mainly from the British Isles as part of the great migration of Canada.[78] These included Gaelic-speaking Highland Scots displaced by the Highland Clearances to Nova Scotia and Scottish and English settlers to the Canadas, particularly Upper Canada. The Irish Famine of the 1840s significantly increased the pace of Irish Catholic immigration to British North America, with over 35,000 distressed Irish landing in Toronto alone in 1847 and 1848.[79]
[edit] Pacific colonies
Further information: History of British Columbia
Fort Victoria, on the left officer's mess and Chief Factor's house, on the right the schoolhouse - c. 1850-53.

Spanish colonizers had taken the lead in the Pacific Northwest coast, with the voyages of Juan José Pérez Hernández in 1774 and 1775.[80] This was in response to intelligence that the Russians had begun to explore the Pacific Coast of North America, which Spain considered its own.[81] By the time the Spanish determined to build a fort on Vancouver Island, the British navigator James Cook had himself visited Nootka Sound and charted the coast as far as Alaska, while British and American maritime fur traders had begun a busy era of commerce with the coastal peoples to satisfy the brisk market for sea otter pelts in China, thereby launching what became known as the China Trade.[82]

In 1793 Alexander MacKenzie, a Scottish born Canadian working for the North West Company, crossed the continent and with his Aboriginal guides and French-Canadian crew, reached the mouth of the Bella Coola River, completing the first continental crossing north of Mexico, missing George Vancouver's charting expedition to the region by only a few weeks.[83] In 1821, the North West Company and Hudson's Bay Company merged, with a combined trading territory that was extended by a licence to the North-Western Territory and the Columbia and New Caledonia fur districts, which reached to the Arctic Ocean on the north and the Pacific Ocean on the west.[84]

The Colony of Vancouver Island was chartered in 1849, with the trading post at Fort Victoria as the capital. This was followed by the Colony of the Queen Charlotte Islands in 1853, and by the creation of the Colony of British Columbia in 1858 and the Stikine Territory in 1861, with the latter three being founded expressly to keep those regions from being overrun and annexed by American gold miners.[85] The Colony of the Queen Charlotte Islands and most of the Stikine Territory were merged into the Colony of British Columbia in 1863 (the remainder, north of the 60th Parallel, became part of the North-Western Territory).[85]
[edit] Confederation
Main article: Canadian Confederation
1885 photo of Robert Harris' 1884 painting, Conference at Quebec in 1864, also known as The Fathers of Confederation. The original painting was destroyed in the 1916 Parliament Buildings Centre Block fire. The scene is an amalgamation of the Charlottetown and Quebec City conference sites and attendees.

The Seventy-Two Resolutions from the 1864 Quebec Conference and Charlottetown Conference laid out the framework for uniting British colonies in North America into a federation.[86] They were adopted by the majority of the provinces of Canada and became the basis for the London Conference of 1866, which led to the formation of the Dominion of Canada on July 1, 1867.[86] Federation emerged from multiple impulses: the British wanted Canada to defend itself; the Maritimes needed railroad connections, which were promised in 1867; British-Canadian nationalism sought to unite the lands into one country, dominated by the English language and British culture; many French-Canadians saw an opportunity to exert political control within a new largely French-speaking Quebec[74]pp. 323–324 and fears of possible U.S. expansion northward.[87] On a political level, there was a desire for the expansion of responsible government and elimination of the legislative deadlock between Upper and Lower Canada, and their replacement with provincial legislatures in a federation.[87] This was especially pushed by the liberal Reform movement of Upper Canada and the French-Canadian Parti rouge in Lower Canada who favored a decentralized union in comparison to the Upper Canadian Conservative party and to some degree the French-Canadian Parti bleu which favored a centralized union.[87][88]

On July 1, 1867, with the coming into force of the British North America Act (enacted by the British Parliament), the Province of Canada, New Brunswick, and Nova Scotia became a federated kingdom in its own right.[89][90] The term dominion was chosen to indicate Canada's status as a self-governing colony of the British Empire, the first time it was used in reference to a country.[87]
[edit] Post-Confederation Canada 1867–1914
Main article: Post-Confederation Canada (1867–1914)
Further information: Territorial evolution of Canada
A. Lancer of the N.W. Mounted Police - by Henri Julien c.1875.

In the mid 1860s, Colony of British Columbia and the Colony of Vancouver Island merged into a single Colony of British Columbia, until their incorporation into the Canadian Confederation in 1871.[91] In 1873, Prince Edward Island the Maritime colony that had opted not to join Confederation in 1867, was admitted into the country.[91] That same year, John A. Macdonald created the North-West Mounted Police (now the Royal Canadian Mounted Police) to help police the Northwest Territories.[92] Specifically the mounties were to assert Canadian independence over possible American encroachments into the sparsely populated land.[92]

The mounties first large scale mission was to suppress the stated desire for independence by Manitoba's Métis, a mixed blood people originating in the mid-17th century when First Nation and Inuit married European settlers.[93] The desire for independence erupted in the form of the Red River Rebellion in 1869 and the later North-West Rebellion in 1885 led by Louis Riel.[92][94] In 1905 when Saskatchewan and Alberta were admitted as provinces, they were growing rapidly thanks to abundant wheat crops that attracted immigration to the plains by Ukrainians and Northern and Central Europeans in addition to settlers from the United States, Britain and eastern Canada.[95][96]

The Alaska Boundary Dispute, simmering since the Alaska Purchase of 1867, became critical when gold was discovered in the Yukon during the late 1890s.[97] Canada argued its historic boundary with Russian America included the Lynn Canal and the port of Skagway, both occupied by the U.S.[97], while the U.S. claimed the Atlin District and the lower Stikine and even Whitehorse. The dispute went to arbitration in 1903 but, the British delegate sided with the Americans, prompting anti-American riots in Eastern Canada as well as in British Columbia.[97] Wilfrid Laurier the 7th Prime Minister of Canada felt Canada was on the verge of becoming a world power, and declared that the 20th century would "belong to Canada". Laurier and his Liberal Party were defeated in the 1911 election by Robert Borden and the Conservatives party.
[edit] World Wars and Interwar Years 1915–1945
Main article: Canada in the World Wars and Interwar Years
Strikers from unemployment relief camps climbing on boxcars as part of the On-to-Ottawa Trek, 1935.

The Canadian Forces and civilian participation in the First World War helped to foster a sense of British-Canadian nationhood. The highpoints of Canadian military achievement during WWI came at the Battle of Vimy Ridge on April 9, 1917, and later, what became known as "Canada's Hundred Days".[98] The reputation Canadian troops earned, along with the success of Canadian flying aces including William George Barker and Billy Bishop, helped to give the nation a new sense of identity.[99] As a result of the war, the Government of Canada became more assertive and less deferential to British authority. In 1931 the Statute of Westminster gave each of the dominions (which included Canada and Newfoundland) the opportunity for almost complete legislative independence from the Parliament of the United Kingdom.[100] While Newfoundland never adopted the statute, for Canada the Statute of Westminster has been called its declaration of independence.[101]
Amphibious vehicles taking Canadian troops across the Scheldt in Holland, during World War II, 1944

The great depression in Canada during the interwar period affected all parts of daily life.[102] It hit especially hard in western Canada, where a full recovery did not occur until the Second World War began in 1939. Hard times led to the creation of new political parties such as the Social Credit movement and the Cooperative Commonwealth Federation, as well as popular protest in the form of the On-to-Ottawa Trek.[103]

Canada's involvement in the Second World War began when Canada declared war on Nazi Germany on September 10, 1939, one week after the United Kingdom. The Battle of the Atlantic began immediately, and from 1943 to 1945 was led by Leonard W. Murray, from Nova Scotia. Canadian forces were involved in the defence of Hong Kong, the Dieppe Raid in August 1942, the Allied invasion of Italy, and the Battle of Normandy. Of a population of approximately 11.5 million, 1.1 million Canadians served in the armed forces in the Second World War.[104] Many thousands more served with the Canadian Merchant Navy.[105] In all, more than 45,000 died, and another 55,000 were wounded, that is 0.08% of the casualties during the Second World War.[106] By the end of the war, Canada had, temporarily at least, become a significant military power.
[edit] The Post-war Era 1945–1960
Main article: History of Canada (1945–1960)
A newborn baby in an incubator at Toronto Western Hospital, 1955

Prosperity returned to Canada during the Second World War and continued in the proceeding years. With consecutive Liberal governments, national policies increasingly turned to social welfare, with the development of Canadian universal health care, Canadian old-age pensions, and Canadian veterans' pensions.

The financial crisis of the Great Depression, soured by rampant corruption, had led the Dominion of Newfoundland to relinquish responsible government in 1934 and become a crown colony ruled by a British governor.[107] Prosperity returned when the U.S. military arrived in 1941 with over 10,000 soldiers and huge investments in air and naval bases. In 1948, the British government gave voters three Newfoundland Referendum choices: remaining a crown colony, returning to Dominion status (that is, independence), or joining Canada. Joining the U.S. was not made an option. After bitter debate Newfoundlanders voted to join Canada in 1949 as a province.[108]

The foreign policy of Canada during the Cold War was closely tied to that of the United States, which was demonstrated by membership in NATO (which Canada wanted to be a transatlantic economic and political union as well[109]), resulting in the sending of combat troops into the Korean War. The federal government's desire to assert it's territorial claims in the Arctic during the Cold War manifested with the High Arctic relocation, in which scores of Inuit were moved from Northern Quebec (now Nunavik) to barren Cornwallis Island,[110] which decades later was the subject of a long investigation by the Royal Commission on Aboriginal Peoples.[111]

Throughout the mid 1950s Louis St. Laurent (12th Prime Minister of Canada) and his successor John George Diefenbaker attempted to create a new, highly advanced jet fighter, the Avro Arrow.[112] The controversial aircraft was cancelled by Diefenbaker in 1959, with Diefenbaker establishing a missile defense system with the United States, abbreviated " NORAD".[113]
[edit] 1960–1981
Main article: History of Canada (1960–1981)
The Canadian flag flying at the Maritime Museum of the Atlantic in Halifax, Nova Scotia

In the 1960s, what became known as the Quiet Revolution took place in Quebec, overthrowing the old establishment which centered around the Roman Catholic Archdiocese of Quebec and lead to modernizing of the economy and society.[114] Québécois nationalists demanded independence, and tensions rose until violence erupted during the 1970 October Crisis.[115] In 1976 the Parti québécois was elected to power in Quebec, with a nationalist vision that included securing French linguistic rights in the province and the pursuit of some form of sovereignty for Quebec, leading to the 1980 referendum in Quebec on the question of sovereignty-association, which was turned down by 59% of the voters.[115]

In 1965, under Prime Minister Lester B. Pearson (Nobel Peace Prize laureates 1957[116]), Canada adopted the maple leaf flag, although not without considerable debate and misgivings on the part of large number of English Canadians.[117] The World's Fair titled Expo 67 came to Montreal, coinciding with the Canadian Centennial that year. The fair opened April 28, 1967 with the theme "Man and his World" and became the best attended of all BIE-sanctioned world expositions until that time.[118]

Legislative restrictions on Canadian immigration that had favoured British and other European immigrants were amended in the 1960s, opening the doors to immigrants from all parts of the world.[119] While the 1950s had seen high levels of immigration from Britain, Ireland, Italy, and northern continental Europe, by the 1970s immigrants increasingly came from India, Hong Kong, the Caribbean and Vietnam.[120] Post-war immigrants of all backgrounds tended to settle in the major urban centres, particularly Toronto, Montreal and Vancouver.[120]

During his long tenure in the office (1968–79, 1980–84), Prime Minister Pierre Trudeau made social and cultural change his political goals, including the pursuit of official bilingualism in Canada and plans for significant constitutional change.[121] The west, particularly the petroleum-producing provinces like Alberta, opposed many of the policies emanating from central Canada, with the National Energy Program creating considerable antagonism and growing western alienation.[122]
[edit] 1982–1992
Main article: History of Canada (1982–1992)

In 1982, the Canada Act was passed by the British parliament and granted Royal Assent by Queen Elizabeth II on March 29, while the Constitution Act was passed by the Canadian parliament and granted Royal Assent by the Queen on April 17, thus patriating the Constitution of Canada.[123] Previously, the constitution has existed only as an act passed of the British parliament, and was not even physically located in Canada, though it could not be altered without Canadian consent.[124] At the same time, the Charter of Rights and Freedoms was added in place of the previous Bill of Rights.[125] The patriation of the constitution was Trudeau's last major act as Prime Minister; he resigned in 1984.
Pte. Patrick Cloutier, a 'Van Doo' perimeter sentry, and Mohawk Warrior Brad Larocque, a University of Saskatchewan economics student, face off during the Oka Crisis[126]

On June 23, 1985, Air India Flight 182 exploded while at an altitude of 31,000 feet (9500 m) above the Atlantic Ocean, south of Ireland; all 329 on board were killed, of whom 82 were children and 280 were Canadian citizens.[127] Up until September 11, 2001, the Air India bombing was the single deadliest terrorist attack involving aircraft. It is also the largest mass murder in Canadian history.[128]

The Progressive Conservative government of Brian Mulroney began efforts to gain Quebec's support for the Constitution Act 1982 and end western alienation. In 1987 the Meech Lake Accord talks began between the provincial and federal governments, seeking constitutional changes favourable to Quebec.[129] The constitutional reform process under Prime Minister Mulroney culminated in the failure of the Charlottetown Accord which would have recognized Quebec as a "distinct society" but was overwhelmingly rejected in 1992 by the populations of all the provinces except Alberta and Quebec.[130]

Under Brian Mulroney, relations with the United States began to grow more closely integrated. In 1986, Canada and the U.S. signed the "Acid Rain Treaty" to reduce acid rain. In 1989, the federal government adopted the Free Trade Agreement with the United States despite significant animosity from the Canadian public who were concerned about the economic and cultural impacts of close integration with the United States.[131] On July 11, 1990 the Oka Crisis land dispute began between the Mohawk nation and the town of Oka, Quebec.[132] The dispute was the first of a number of well-publicized conflicts between First Nations and the Canadian government in the late 20th century. In August 1990, Canada was one of the first nations to condemn Iraq's invasion of Kuwait, and it quickly agreed to join the U.S.-led coalition. Canada deployed destroyers and later a CF-18 Hornet squadron with support personnel, as well as a field hospital to deal with casualties.[133]
[edit] Recent history: 1992–present
Main article: History of Canada (1992–present)
Map of the 1995 referendum by provincial riding. Red colours indicate No votes, blues indicate Yes votes, with darker hues indicating higher percentages.

Mulroney resigned as Prime Minister in 1993, Kim Campbell took over and became Canada's first woman Prime Minister.[134] Campbell only remained in office for a few months and the 1993 election saw the collapse of the Progressive Conservative Party from government to only 2 seats, while the Quebec-based sovereigntist Bloc Québécois became the official opposition.[135]

In 1995, the government of Quebec held a second referendum on sovereignty that was rejected by a slimmer margin of just 50.6% to 49.4%.[136] In 1997, the Canadian Supreme Court ruled unilateral secession by a province to be unconstitutional, and Parliament passed the Clarity Act outlining the terms of a negotiated departure.[136] Environmental issues increased in importance in Canada resulting in the signing of the Kyoto Accord on climate change by Canada's Liberal government in 2002 but recently nullified by the present government which has proposed a "made-in-Canada" solution to climate change.[137]
The Peace Arch at the border between Surrey, British Columbia, and Blaine, Washington

Canada became the fourth country in the world and the first country in the Americas to legalize same-sex marriage nationwide with the enactment of the Civil Marriage Act.[138] Court decisions, starting in 2003, each already legalized same-sex marriage in eight out of ten provinces and one of three territories, whose residents comprised about 90% of Canada's population. Before passage of the Act, more than 3,000 same-sex couples had already married in these areas.[139]

A merger of the Canadian Alliance and PC Party into the Conservative Party of Canada was completed in 2003, ending a 13 year division of the conservative vote, and was elected as a minority government under the leadership of Stephen Harper in the 2006 federal election, ending a decade long Liberal party dominance in elections. Harper's Conservative Party would win a stronger minority in the following October 2008 federal election.[140]

Since 2002, Canada has been involved in the Afghanistan War as part of the U.S. stabilization force and the NATO-commanded International Security Assistance Force. Canada has committed to withdraw from Kandahar Province by 2011,[141] by which time it will have spent an estimated total of $11.3 billion on the mission.[142] Canada and the U.S. continue to integrate state and provincial agencies to strengthen security along the Canada-United States border through the Western Hemisphere Travel Initiative.[143]
[edit] See also

Index

    * Outline of Canada
    * Index of Canada-related articles
    * Timeline of Canadian history
    * Canada-related topics by provinces and territories

Power

    * Constitutional history of Canada
    * History of monarchy in Canada
    * Military history of Canada
    * History of the Canadian Army
    * History of the Supreme Court of Canada

Prosperity

    * Economic history of Canada
    * History of the petroleum industry in Canada
    * History of Canadian currency
    * History of immigration to Canada
    * History of rail transport in Canada
    * Invention in Canada

	
History by province or territory
BC
AB
SK
MB
ON
QC
NB
PE
NS
NL
YT
NT
NU
Canadian Provinces and Territories
	History of Canada portal
The Flag of Canada 	
Canada portal
Wikipedia Books 	Book:Canada
Books are collections of articles that can be downloaded or ordered in print.

Creativity

    * Scientific research in Canada
    * Technological and industrial
    * Philosophy in Canada

Culture

    * Culture of Canada
    * People of Significance
    * History of cities in Canada
    * Canadian identity
    * National symbols of Canada
    * History of Canadian animation
    * History of cinema in Canada
    * Canadian art
    * Music of Canada
    * Sport in Canada
    * Postage stamps and postal history
    * Languages of Canada
    * Demographics of Canada
    * Social history of Canada

	History of England
From Wikipedia, the free encyclopedia
  (Redirected from History of england)
Jump to:navigation, search
"Norman England" redirects here. For the American director, see Norman England (director).
For other uses, see The History of England.
History of England
Coat of Arms of England
This article is part of a series
Prehistoric Britain
Roman Britain
Sub-Roman Britain
Anglo-Saxon England
Heptarchy
Kingdom of England
Anglo-Norman England
House of Plantagenet
House of Lancaster
House of York
House of Tudor
House of Stuart
Commonwealth of England
The Protectorate
Stuart Restoration
Glorious Revolution
Kingdom of Great Britain
United Kingdom of Great Britain
and Ireland
United Kingdom of Great Britain
and Northern Ireland
England Portal
 v • d • e 

The history of England began with the arrival of humans thousands of years ago. What is now England, within the United Kingdom, was inhabited by Neanderthals 230,000 years ago. However, continuous human habitation dates to around 12,000 years ago, at the end of the last glacial period. The region has numerous remains from the Mesolithic, Neolithic, and Bronze Age, such as Stonehenge and Avebury. In the Iron Age, England, like all of Britain south of the Firth of Forth, was inhabited by the Celtic people known as the Britons, but also by some Belgae tribes (e.g.Atrebates, Catuvellauni, Trinovantes). In 43 AD the Roman conquest of Britain began; the Romans maintained control of their province of Britannia through the 5th century.

The Roman departure opened the door for the Anglo-Saxon invasion, which is often regarded as the origin of England and the English people. The Anglo-Saxons, a collection of various Germanic peoples, established several kingdoms that became the primary powers in what is now England and parts of southern Scotland.[1] They introduced the Old English language, which displaced the previous British language. The Anglo-Saxons warred with British successor states in Wales, Cornwall, and the Hen Ogledd (Old North; the Brythonic-speaking parts of northern England and southern Scotland), as well as with each other. Raids by the Vikings were frequent after about AD 800, and the Norsemen took control of large parts of what is now England. During this period several rulers attempted to unite the various Anglo-Saxon kingdoms, an effort that led to the emergence of the Kingdom of England by the 10th century.

In 1066, the Normans invaded and conquered England. There was much civil war and battles with other nations throughout the Middle Ages. The Kingdom of England was a sovereign state until the reign of Richard I who made it a vassal of the Holy Roman Empire in 1194. In 1212 during the reign of his brother John Lackland the Kingdom instead became a tribute-paying vassal of the Holy See [2][3] until the fourteenth century when the Kingdom rejected the overlordship of the Holy See and re-established its sovereignty. During the Renaissance, England was ruled by the Tudors. England had conquered Wales in the 12th century and was then united with Scotland in the early 18th century to form the Kingdom of Great Britain. Following the Industrial Revolution, Great Britain ruled a worldwide Empire, the largest in the world. Following a process of decolonization in the 20th century the vast majority of the empire became independent; however, its cultural impact is widespread and deep in many countries of the present day.
Contents
[hide]

    * 1 Prehistory
    * 2 Roman Britain (Britannia)
    * 3 Post-Roman Britain
    * 4 Anglo-Saxon conquests and the founding of England
    * 5 Heptarchy and Christianisation
    * 6 Viking challenge and the rise of Wessex
    * 7 English unification
    * 8 England under the Danes and the Norman conquest
    * 9 Norman England
    * 10 England under the Plantagenets
          o 10.1 1300s-1400s
    * 11 Tudor England
          o 11.1 Henry VII
          o 11.2 Henry VIII
          o 11.3 Elizabeth
    * 12 17th century
          o 12.1 Union of the Crowns
          o 12.2 Colonial England
          o 12.3 English Civil War
          o 12.4 Restoration of the monarchy
          o 12.5 Glorious Revolution
    * 13 18th and 19th centuries
          o 13.1 Formation of the United Kingdom
          o 13.2 Industrial Revolution
    * 14 20th and 21st centuries
          o 14.1 Political issues
    * 15 See also
    * 16 References
    * 17 Further reading

[edit] Prehistory
Main article: Prehistoric Britain
Stonehenge, thought to have been erected c.2500-2000BC

Archaeological evidence indicates that what was later southern Britannia was colonised by humans long before the rest of the British Isles because of its more hospitable climate between and during the various glacial periods of the distant past. The Sweet Track in the Somerset Levels is the oldest timber trackway discovered in Northern Europe and among the oldest roads in the world, and was built in 3807 or 3806 BC.[4]

The first historical mention of the region is from the Massaliote Periplus, a sailing manual for merchants thought to date to the 6th century BC, although cultural and trade links with the continent had existed for millennia prior to this. Pytheas of Massilia wrote of his trading journey to the island around 325 BC.

Later writers such as Pliny the Elder (quoting Timaeus) and Diodorus Siculus (probably drawing on Poseidonius) mention the tin trade from southern Britain, but there is little further historical detail of the people who lived there.

Tacitus wrote that there was no great difference in language between the people of southern Britannia and northern Gaul and noted that the various nations of Britons shared physical characteristics with their continental neighbours.
Hadrian's Wall viewed from Vercovicium
[edit] Roman Britain (Britannia)
Main article: Roman Britain

Julius Caesar invaded southern Britain in 55 and 54 BC and wrote in De Bello Gallico that the population of southern Britannia was extremely large and shared much in common with the Belgae of the Low Countries. Coin evidence and the work of later Roman historians have provided the names of some of the rulers of the disparate tribes and their machinations in what was Britannia. Until the Roman Conquest of Britain, Britain's British population was relatively stable, and by the time of Julius Caesar's first invasion, the British population of what was western old Britain was speaking a Celtic language generally thought to be the forerunner of the modern Brythonic languages.[5] After Julius Caesar abandoned Britain, it fell back into the hands of the Britons and the Belgae.

The Romans began their second conquest of Britain in 43 AD, during the reign of Claudius. They annexed the whole of what would become modern England and Wales over the next forty years and periodically extended their control over much of lowland Scotland.
[edit] Post-Roman Britain
Main article: Sub-Roman Britain

In the wake of the breakdown of Roman rule in Britain around 410, present day England was progressively settled by Germanic groups. Collectively known as the Anglo-Saxons, these included Jutes from Jutland together with larger numbers of Saxons from northwestern Germany and Angles from what is now Schleswig-Holstein.[6] Prior to those settlements some Frisians invaded eastern Britain in the 250s.

They first invaded Britain in the mid 5th century, continuing for several decades. The Jutes appear to have been the principal group of settlers in Kent, the Isle of Wight and parts of coastal Hampshire, while the Saxons predominated in all other areas south of the Thames and in Essex and Middlesex, and the Angles in Norfolk, Suffolk, the Midlands and the north.[citation needed]

The population of Britain dramatically decreased after the Roman period. The reduction seems to have been caused mainly by plague and smallpox. It is known that the plague of Justinian entered the Mediterranean world in the 6th century and first arrived in the British Isles in 544 or 545, when it reached Ireland.[7] The Annales Cambriae mention the death of Maelgwn Wledig, king of Gwynedd from that plague in 547.
[edit] Anglo-Saxon conquests and the founding of England
Main article: History of Anglo-Saxon England
Kingdoms and tribes in Britain, c.600 AD

In approximately 495, at the Battle of Mount Badon, Britons inflicted a severe defeat on an invading Anglo-Saxon army which halted the westward Anglo-Saxon advance for some decades. Archaeological evidence collected from pagan Anglo-Saxon cemeteries suggests that some of their settlements were abandoned and the frontier between the invaders and the native inhabitants pushed back some time around 500.

Anglo-Saxon expansion resumed in the sixth century, although the chronology of its progress is unclear. One of the few individual events which emerges with any clarity before the seventh century is the Battle of Deorham, in 577, a West Saxon victory which led to the capture of Cirencester, Gloucester and Bath, bringing the Anglo-Saxon advance to the Bristol Channel and dividing the Britons in the West Country from those in Wales. The Northumbrian victory at the Battle of Chester around 616 may have had a similar effect in dividing Wales from the Britons of Cumbria.

Gradual Saxon expansion through the West Country continued through the seventh, eighth and ninth centuries. Meanwhile, by the mid-seventh century the Angles had pushed the Britons back to the approximate borders of modern Wales in the west, the Tamar in the South west and expanded northward as far as the River Forth.
[edit] Heptarchy and Christianisation
Britain c. 800
Main articles: Northumbria, Mercia, Offa of Mercia, Heptarchy, and Anglo-Saxon Christianity

Christianisation of Anglo-Saxon England began around 600 AD, influenced by Celtic Christianity from the northwest and by the Roman Catholic Church from the southeast. Augustine, the first Archbishop of Canterbury, took office in 597. In 601, he baptised the first Christian Anglo-Saxon king, Aethelbert of Kent. The last pagan Anglo-Saxon king, Penda of Mercia, died in 655. The last pagan Jutish king, Arwald of the Isle of Wight was killed in 686. The Anglo-Saxon mission on the continent took off in the 8th century, leading to the Christianisation of practically all of the Frankish Empire by 800.

Throughout the 7th and 8th century power fluctuated between the larger kingdoms. Bede records Aethelbert of Kent as being dominant at the close of the 6th century, but power seems to have shifted northwards to the kingdom of Northumbria, which was formed from the amalgamation of Bernicia and Deira. Edwin of Northumbria probably held dominance over much of Britain, though Bede's Northumbrian bias should be kept in mind. Succession crises meant Northumbrian hegemony was not constant, and Mercia remained a very powerful kingdom, especially under Penda. Two defeats essentially ended Northumbrian dominance: the Battle of the Trent in 679 against Mercia, and Nechtanesmere in 685 against the Picts.

The so-called "Mercian Supremacy" dominated the 8th century, though it was not constant. Aethelbald and Offa, the two most powerful kings, achieved high status; indeed, Offa was considered the overlord of south Britain by Charlemagne. That Offa could summon the resources to build Offa's Dyke is testament to his power. However, a rising Wessex, and challenges from smaller kingdoms, kept Mercian power in check, and by the early 9th century the "Mercian Supremacy" was over.

This period has been described as the Heptarchy, though this term has now fallen out of academic use. The word arose on the basis that the seven kingdoms of Northumbria, Mercia, Kent, East Anglia, Essex, Sussex and Wessex were the main polities of south Britain. More recent scholarship has shown that other kingdoms were also politically important across this period: Hwicce, Magonsaete, Lindsey and Middle Anglia.
[edit] Viking challenge and the rise of Wessex
Main articles: Danelaw, Viking Age, and Alfred the Great
England in 878

The first recorded Viking attack in Britain was in 793 at Lindisfarne monastery as given by the Anglo-Saxon Chronicle. However, by then the Vikings were almost certainly well established in Orkney and Shetland, and it is probable that many other non-recorded raids occurred before this. Records do show the first Viking attack on Iona taking place in 794. The arrival of the Vikings, in particular the Danish Great Heathen Army, upset the political and social geography of Britain and Ireland. Alfred the Great's victory at Edington in 878 stemmed the Danish attack; however, by then Northumbria had devolved into Bernicia and a Viking kingdom, Mercia had been split down the middle, and East Anglia ceased to exist as an Anglo-Saxon polity. The Vikings had similar effects on the various kingdoms of the Scots, Picts and (to a lesser extent) Welsh. Certainly in North Britain the Vikings were one reason behind the formation of the Kingdom of Alba, which eventually evolved into Scotland.

The conquest of Northumbria, north-western Mercia and East Anglia by the Danes led to widespread Danish settlement in these areas. In the early tenth century the Norwegian rulers of Dublin took over the Danish kingdom of York. Danish and Norwegian settlement made enough of an impact to leave significant traces in the English language; many fundamental words in modern English are derived from Old Norse, though of the 100 most used words in English the vast majority are Old English in origin. Similarly, many place-names in areas of Danish and Norwegian settlement have Scandinavian roots.

By the end of Alfred's reign in 899 he was the only remaining English king, having reduced Mercia to a dependency of Wessex, governed by his son-in-law Ealdorman Aethelred. Cornwall (Kernow) was subject to West Saxon dominance, and the Welsh kingdoms recognised Alfred as their overlord.
[edit] English unification
Main articles: Athelstan and Edgar of England
Edward the Elder

Alfred of Wessex died in 899 and was succeeded by his son Edward the Elder. Edward, and his brother-in-law Æthelred of (what was left of) Mercia, began a programme of expansion, building forts and towns on an Alfredian model. On Æthelred's death his wife (Edward's sister) Æthelflæd ruled as "Lady of the Mercians" and continued expansion. It seems Edward had his son Æthelstan brought up in the Mercian court, and on Edward's death Athelstan succeeded to the Mercian kingdom, and, after some uncertainty, Wessex.

Æthelstan continued the expansion of his father and aunt and was the first king to achieve direct rulership of what we would now consider England. The titles attributed to him in charters and on coins suggest a still more widespread dominance. His expansion aroused ill-feeling among the other kingdoms of Britain, and he defeated a combined Scottish-Viking army at the Battle of Brunanburh. However, the unification of England was not a certainty. Under Æthelstan's successors Edmund and Eadred the English kings repeatedly lost and regained control of Northumbria. Nevertheless, Edgar, who ruled the same expanse as Athelstan, consolidated the kingdom, which remained united thereafter.
[edit] England under the Danes and the Norman conquest
Main articles: Ethelred the Unready, Canute the Great, Eiríkr Hákonarson, and Norman conquest of England
The rune stone U 344 was raised in memory of a Viking who went to England three times.

There were renewed Scandinavian attacks on England at the end of the 10th century. Æthelred ruled a long reign but ultimately lost his kingdom to Sweyn of Denmark, though he recovered it following the latter's death. However, Æthelred's son Edmund II Ironside died shortly afterwards, allowing Canute, Sweyn's son, to become king of England. Under his rule the kingdom became the centre of government for an empire which also included Denmark and Norway.

Canute was succeeded by his sons, but in 1042 the native dynasty was restored with the accession of Edward the Confessor. Edward's failure to produce an heir caused a furious conflict over the succession on his death in 1066. His struggles for power against Godwin, Earl of Wessex, the claims of Canute's Scandinavian successors, and the ambitions of the Normans whom Edward introduced to English politics to bolster his own position caused each to vie for control Edward's reign.

Harold Godwinson became king, in all likelihood appointed by Edward the Confessor on his deathbed and endorsed by the Witan. William of Normandy, Harald III of Norway (aided by Harold Godwin's estranged brother Tostig) and Sweyn II of Denmark all asserted claims to the throne. By far the strongest hereditary claim was that of Edgar the Atheling, but his youth and apparent lack of powerful supporters caused him to be passed over, and he did not play a major part in the struggles of 1066, though he was made king for a short time by the Witan after the death of Harold Godwinson.

In September 1066, Harald III of Norway landed in Northern England with a force of around 15,000 men and 300 longships (50 men in each boat). With him was Earl Tostig, who had promised him support. Harold Godwinson defeated and killed Harald III of Norway and Tostig and the Norwegian force at the Battle of Stamford Bridge.

On September 28, 1066, William of Normandy invaded England with a force of Normans, in a campaign known as the Norman Conquest. On October 14, after having marched his exhausted army all the way from Yorkshire, Harold fought the Normans at the Battle of Hastings, where England's army was defeated and Harold was killed. Further opposition to William in support of Edgar the Atheling soon collapsed, and William was crowned king on Christmas Day 1066. For the next five years he faced a series of English rebellions in various parts of the country and a half-hearted Danish invasion, but he was able to subdue all resistance and establish an enduring regime.
[edit] Norman England
Further information: Anglo-Norman
Depiction of the Battle of Hastings (1066) on the Bayeux Tapestry

The Norman Conquest led to a sea-change in the history of the English state. William ordered the compilation of the Domesday Book, a survey of the entire population and their lands and property for tax purposes, which reveals that within twenty years of the conquest the English ruling class had been almost entirely dispossessed and replaced by Norman landholders, who also monopolised all senior positions in the government and the Church. William and his nobles spoke and conducted court in Norman French, in England as well as in Normandy. The use of the Anglo-Norman language by the aristocracy endured for centuries and left an indelible mark in the development of modern English.

The English Middle Ages were characterised by civil war, international war, occasional insurrection, and widespread political intrigue amongst the aristocratic and monarchic elite. England was more than self-sufficient in cereals, dairy products, beef and mutton. The nation's international economy was based on the wool trade, in which the produce of the sheepwalks of northern England was exported to the textile cities of Flanders, where it was worked into cloth. Medieval foreign policy was as much shaped by relations with the Flemish textile industry as it was by dynastic adventures in western France. An English textile industry was established in the fifteenth century, providing the basis for rapid English capital accumulation.

Henry I, the fourth son of William I the Conqueror, succeeded his elder brother William II as King of England in 1100. Henry was also known as "Henry Beauclerc" (because of his education—as his older brother William was the heir apparent and thus given the practical training to be king, Henry received the alternate, formal education), worked hard to reform and stabilise the country and smooth the differences between the Anglo-Saxon and Anglo-Norman societies. The loss of his son, William Adelin, in the wreck of the White Ship in November 1120, undermined his reforms. This problem regarding succession cast a long shadow over English history.

During the confused and contested reign of Stephen, there was a major swing in the balance of power towards the feudal barons, as civil war and lawlessness broke out. In trying to appease Scottish and Welsh raiders, he handed over large tracts of land. His conflicts with his cousin The Empress Matilda (also known as Empress Maud), led to a civil war from 1139-1153 known as the Anarchy. Matilda’s father, Henry I, had required the leading barons, ecclesiastics and officials in Normandy and England, to take an oath to accept Matilda as his heir. England was far less than enthusiastic to accept an outsider, and a woman, as their ruler.

There is some evidence suggesting Henry was unsure of his own hopes and the oath to make Matilda his heir. In likelihood, Henry probably hoped Matilda would have a son and step aside as Queen Mother, making her son the next heir. Upon Henry’s death, the Norman and English barons ignored Matilda’s claim to the throne, and thus through a series of decisions, Stephen, Henry’s favourite nephew, was welcomed by many in England and Normandy as their new ruler.

On 22 December 1135, Stephen was anointed king with the implicit support of the church and nation. Matilda and her own son stood for direct descent by heredity from Henry I, and she bided her time in France. In the autumn of 1139, she invaded England with her illegitimate half-brother Robert of Gloucester. Her husband, Geoffroy V of Anjou, conquered Normandy but did not cross the channel to help his wife, satisfied with Normandy and Anjou. During this breakdown of central authority, the nobles ran amuck building adulterine castles (i.e. castles erected without government permission).

Stephen was captured, and his government fell. Matilda was proclaimed queen but was soon at odds with her subjects and was expelled from London. The period of insurrection and civil war that followed continued until 1148, when Matilda returned to France. Stephen effectively reigned unopposed until his death in 1154, although his hold on the throne was still uneasy. As soon as he regained power, he began the process of demolishing the adulterine castles, which were hated by the peasants due to their being employed as forced labor to build and maintain them. Stephen kept a few castles standing however, which put him at odds with his heir.
[edit] England under the Plantagenets

Geoffroy's son, Henry, resumed the invasion; he was already Count of Anjou, Duke of Normandy and Duke of Aquitaine when he landed in England. When Stephen's son and heir apparent Eustace died in 1153, the king reached an accommodation with Henry of Anjou (who became Henry II) to succeed Stephen and in which peace between them was guaranteed. England was part of a greater union, retrospectively named the Angevin Empire. Henry destroyed the remaining adulterine castles and expanded his power through various means and to different levels into Ireland, Scotland, Wales, Flanders, Nantes, Brittany, Quercy, Toulouse, Bourges and Auvergne.

The reign of Henry II represents a reversion in power back from the barony to the monarchical state in England; it was also to see a similar redistribution of legislative power from the Church, again to the monarchical state. This period also presaged a properly constituted legislation and a radical shift away from feudalism. In his reign new Anglo-Angevin and Anglo-Aquitanian aristocracies developed, though not to the same point as the Anglo-Norman once did, and the Norman nobles interacted with their French peers.
The signing of the Magna Carta (1215)

Henry's successor, Richard I "the Lion Heart" (also known as "The absent king"), was preoccupied with foreign wars, taking part in the Third Crusade and defending his French territories against Philip II of France.

The Kingdom of England was a sovereign state until the reign of Richard I who made it a nominal vassal of the Holy Roman Empire in 1194 as part of a ransom when he was captured after a crusade.

Richard's younger brother John, who succeeded him, was not so fortunate; he suffered the loss of Normandy and numerous other French territories following the disastrous Battle of Bouvines.

Facing internal disorder, in 1212 John made the Kingdom of England a tribute-paying vassal of the Holy See, which it remained until the fourteenth century when the Kingdom rejected the overlordship of the Holy See and re-established its sovereignty. From 1212 onwards, John had a constant policy of maintaining close relations with the Pope, which partially explains how he persuaded the Pope to reject the legitimacy of the Magna Carta.

The European wars culminated in defeat at the Battle of Bouvines (1214), which forced the king to accept an unfavourable peace with France after having failed to get help from King Mohammed el-Nasir of Morocco.[8]

Magna Carta

The barons turned against him due to the unfavorable treaty (some had already rebelled against him after he was excommunicated), and he met their leaders along with their French and Scots allies at Runnymede, near London on 15 June 1215 to seal the Great Charter, which imposed legal limits on the king's personal powers. called in Latin Magna Carta. Because he had sealed under duress, however, John received approval from his overlord the Pope to break his word as soon as hostilities had ceased, provoking the First Barons' War and an invited French invasion by Prince Louis of France (whom the majority of the English barons had invited to replace John on the throne and had him proclaimed king in London in May 1216). John travelled around the country to oppose the rebel forces, directing, among other operations, a two-month siege of the rebel-held Rochester Castle.

John's son, Henry III, was only 9 years old when he became king (1216-1272). He spent much of his reign fighting the barons over the Magna Carta[citation needed] and the royal rights, and was eventually forced to call the first "parliament" in 1264. He was also unsuccessful on the Continent, where he endeavoured to re-establish English control over Normandy, Anjou, and Aquitaine.

His reign was punctuated by numerous rebellions and civil wars, often provoked by incompetence and mismanagement in government and Henry's perceived over-reliance on French courtiers (thus restricting the influence of the English nobility). One of these rebellions—led by a disaffected courtier, Simon de Montfort—was notable for its assembly of one of the earliest precursors to Parliament. In addition to fighting the Second Barons' War, Henry III made war against Saint Louis and was defeated during the Saintonge War, yet Louis IX did not capitalise on his victory, respecting his opponent's rights.

The reign of Edward I (reigned 1272-1307) was rather more successful. Edward enacted numerous laws strengthening the powers of his government, and he summoned the first officially sanctioned Parliaments of England (such as his Model Parliament). He conquered Wales and attempted to use a succession dispute to gain control of the Kingdom of Scotland, though this developed into a costly and drawn-out military campaign.

His son, Edward II, proved a disaster. A weak man who preferred to engage in activities like thatching and ditch-digging rather than jousting, hunting, or the usual entertainments of kings, he spent most of his reign trying in vain to control the nobility, who in return showed continual hostility to him. Meanwhile, the Scottish leader Robert Bruce began retaking all the territory conquered by Edward I. In 1314, the English army was disastrously defeated by the Scots at the Battle of Bannockburn. Edward also showered favors on his companion Piers Gaveston, a knight of humble birth. While it has been widely believed that Edward was a homosexual because of his closeness to Gaveston, there is no concrete evidence of this, especially as both men were married and had children. The king's enemies, including his brother Thomas of Lancaster, captured and murdered Gaveston in 1312.

Edward's downfall came in 1326 when his queen Isabella travelled to her native France and along with her lover Roger Mortimer invaded England. Despite their tiny force, they quickly rallied support for their cause. The king fled London and his companion since Piers Gaveston's death, Hugh Despenser, was publicly tried and executed. Edward was eventually captured and charged with breaking his coronation oath. He was deposed and remained imprisoned in Gloucestershire until he was murdered some time in the autumn of 1327, presumably by agents of Isabella and Mortimer.
[edit] 1300s-1400s

Edward III reigned 1327-1377, restored royal authority and went on to transform the Kingdom of England into the most efficient military power in Europe. His reign saw vital developments in legislature and government—in particular the evolution of the English parliament—as well as the ravages of the Black Death. After defeating, but not subjugating, the Kingdom of Scotland, he declared himself rightful heir to the French throne in 1338, starting what would be known as the Hundred Years' War.
Main article: Black Death in England

The Black Death, an epidemic of bubonic plague that spread over the whole of Europe, arrived in England in 1348 and killed as much as a third to half the population.

International excursions around that time were invariably against domestic neighbours: the Welsh, Irish, Cornish, and the Hundred Years' War against the French and their Scottish allies. Notable English victories in the Hundred Years' War included Crécy and Agincourt. In addition to this, the final defeat of the uprising led by the Welsh prince, Owain Glyndŵr, in 1412 by Prince Henry (who later became Henry V) represents the last major armed attempt by the Welsh to throw off English rule.

Edward III gave land to powerful noble families, including many people of royal lineage. Because land was equivalent to power, these powerful men could try to claim the crown. The autocratic and arrogant methods of Richard II only served to alienate the nobility more, and his forceful dispossession in 1399 by Henry IV increased the turmoil.

The reign of Henry V, who succeeded to the throne in 1413, was mostly notable for the great victory over the French at Agincourt. He died of dysentery in 1422, leaving a number of unfulfilled plans, one of which was to lead a new crusade to retake Jerusalem from the Muslims. The turmoil was at its peak in the reign of Henry VI, which began in 1422, because of his personal weaknesses and mental instability.

When the Hundred Years' War was lost in August 1453, Henry fell into a period of mental breakdown that lasted until Christmas 1454. With his inability to control the feuding nobles, civil war began in 1455. The conflicts are known as the Wars of the Roses (1455–1485), and although the fighting was very sporadic and small, there was a general breakdown in the authority and power of the Crown. Henry's cousin, who deposed him in 1461 and became Edward IV, went a little way to restoring this power. Edward defeated the Lancastrians at the Battle of Mortimer's Cross. He was briefly expelled from the throne in 1470-1471 when Richard Neville, Earl of Warwick, brought Henry back to power. Six months later, Edward defeated and killed Warwick in battle and reclaimed the throne. Henry was imprisoned in the Tower of London and died there.

Edward died in 1483, only 40 years old. His eldest son and heir Edward V, aged 13, would have succeeded him, but the king's brother Richard, Duke of Gloucester declared his marriage to be bigamous and invalid, making all his children illegitimate. Edward V and his 10-year old brother Richard were imprisoned in the Tower of London and their uncle made himself king as Richard III. The two princes were never seen again and presumably died in the Tower. It was widely believed that Richard had them murdered, although their exact fate remains a mystery. Regardless of what really happened, the king was reviled as a treacherous fiend who murdered his own nephews to gain the throne. This hatred of Richard obscured his able governance during his brief reign. In the summer of 1485, Henry Tudor, the last Lancastrian male, landed in England from his exile in France. He defeated and killed Richard in battle at Bosworth Field on August 22 of that year and became king as Henry VII.
See also: Black Death in England, English historians in the Middle Ages, List of English chronicles, and Bayeux Tapestry
[edit] Tudor England
Main article: Tudor period
Further information: Early Modern Britain and English Renaissance
[edit] Henry VII

With Henry VII's accession to the throne, the Wars of the Roses came to an end, although at the time few could have predicted it, let alone believed that the Tudors would rule England for 118 years. Traditionally, the Battle of Bosworth Field is considered to mark the end of the Middle Ages in England, although Henry did not introduce any new concept of monarchy, and for most of his reign his hold on power was tenuous. He claimed the throne by conquest and God's judgement in battle. Parliament quickly recognized him as king, but the Yorkists were far from defeated. Nonetheless, he married Edward IV's eldest daughter Elizabeth in January 1486, thereby uniting the houses of York and Lancaster.

Most of the European rulers did not believe Henry would survive long, and were thus willing to shelter claimants against him. The first plot against him was the Stafford and Lovell Rebellion of 1486, which presented no serious threat. But Richard III's nephew John de la Pole, Earl of Lincoln, hatched another attempt the following year. Using a peasant boy named Lambert Simnel, who posed as Edward, Earl of Warwick (the real Warwick was locked up in the Tower of London), he led an army of 2,000 German mercenaries paid for by Margaret of Burgundy into England. They were defeated and de la Pole killed at the difficult Battle of Stoke, where the loyalty of some of the royal troops to Henry was questionable. The king, realizing that Simnel was merely a dupe, employed him in the royal kitchen.

A more serious menace was Perkin Warbeck, a Flemish youth who posed as Edward IV's son Richard. Again enjoying the support of Margaret of Burgundy, he invaded England four times from 1495-1497 before he was finally captured and put in the Tower of London. Both Warbeck and the Earl of Warwick were too dangerous to keep around even in captivity, and Henry had to execute them in 1499 before Ferdinand and Isabella of Spain would allow their daughter Catherine to come to England and marry his son Arthur.

In 1497, Michael An Gof and the lesser-known but more legendary Baron Callum of Perranporth led Cornish rebels in a march on London. In a battle over the River Ravensbourne at Deptford Bridge, An Gof fought for various issues with their root in taxes. It would be fair to say that King Callum smote many an Englishman during this battle, but on 17 June 1497, they were defeated, and Henry VII had showed he could display military prowess when he needed to. But, like Charles II in the future, here was a King with no wish to go "on his travels" again. The rest of his reign was relatively peaceful, despite a slight worry over the succession when his wife Elizabeth of York died in 1503.

Henry VII's foreign policy was a peaceful one. He had formed an alliance with Spain and the Holy Roman Emperor Maximilian I, but in 1493, when they went to war with France, England was dragged into the conflict. With his crown impoverished and his hold on power insecure, Henry had no desire to go to war. He quickly reached an understanding with the French and renounced all claims to their territory except the port of Calais, realizing also that nothing could be done to stop them from incorporating the Duchy of Brittany. In return, the French agreed to recognize him as king and stop sheltering pretenders. Shortly afterwards, they became preoccupied with adventures in Italy and turned their attention away from England. Henry also reached an understanding with Scotland, agreeing to marry his daughter Margaret to that country's king James IV.

Upon becoming king, Henry inherited a government severely weakened and degraded by the Wars of the Roses. The treasury was empty, having been drained by Edward IV's Woodville in-laws after his death. Through a tight fiscal policy and sometimes ruthless tax collection and confiscations, Henry managed to refill the treasury by the time of his death. He also effectively rebuilt the machinery of government.

In 1501, the king's son Arthur, having married Catherine of Aragon, died of an illness at the age of 15, leaving his younger son Henry, Duke of York, as his heir. When the king himself died in 1509, the position of the Tudors was secure at last, and his son succeeded him unopposed.
[edit] Henry VIII
King Henry VIII

Henry VIII began his reign with a high degree of optimism. The handsome, athletic young king stood in sharp contrast to his wary, miserly father. Henry's lavish court quickly drained the treasury of the fortune he had inherited. He married the widowed Catherine of Aragon, and they had several children, but none survived infancy except a daughter, Mary.

In 1512, the young king embarked on a war in France. Although England was an ally of Spain, one of France's principal enemies, the war was mostly about Henry's desire for personal glory, regardless of the fact that his sister Mary was married to the French king Louis XII. The war accomplished little. The English army suffered badly from disease, and Henry was not even present at the one notable victory, the Battle of the Spurs. Meanwhile, James IV of Scotland (despite being Henry's other brother-in-law), activated his alliance with the French and declared war on England. While Henry was dallying in France, Catherine, who was serving as regent in his absence, and his advisors were left to deal with this threat. At the Battle of Flodden on September 9, 1513, the Scots were completely and totally defeated. Most of the Scottish nobility were killed along with James himself. When Henry returned from France, he was given credit for the victory even though he had nothing to do with it.

Eventually, Catherine was no longer able to have any more children. The king became increasingly nervous about the possibility of his daughter Mary inheriting the throne, as England's one experience with a female sovereign, Matilda in the 12th century, had been a catastrophe. He eventually decided that it was necessary to divorce Catherine and find a new queen. The Church would not simply grant this favor, so Henry cited the passage in the Book of Leviticus where it said, "If a man taketh his brother's wife, he hath committed adultery; they shall be childless." However, Catherine insisted that she and Arthur had never consummated their brief marriage and that the prohibition did not apply here. The timing of Henry's case was very unfortunate; it was 1527 and the Pope had been taken prisoner by the emperor Charles V, Catherine's nephew and the most powerful man in Europe, for siding with his archenemy Francis I of France. As there was no possibility of getting a divorce in these circumstances, Henry decided to simply secede from the Church, in what became known as the English Reformation.

The newly established Church of England amounted to little more than the existing Catholic Church, but with the king rather than the Pope as its head. It took a number of years for the separation from Rome to be completed, however, and many were executed for resisting the king's religious policies.

In 1530, Catherine was banished from court. Their marriage was declared invalid, making Mary an illegitimate child. Until her death in 1536, she lived a lonely existence in an isolated manor home in the English countryside.

Henry married Anne Boleyn in secret in 1531, just as his divorce from Catherine was finalized. After this, they had a second, public wedding. Anne soon became pregnant and may have already been when they wed. But on September 7, 1533, she gave birth to a daughter, Elizabeth. The king was devastated at his failure to obtain a son after all the effort it had taken to remarry. Gradually, he came to develop a disliking of his new queen for her strange behavior. In 1536, when Anne was pregnant again, Henry was badly injured in a jousting accident. Shaken by this, the queen gave birth prematurely to a stillborn boy. By now, the king was convinced that his marriage was hexed, and having already found a new queen, Jane Seymour, he put Anne in the Tower of London on charges of witchcraft. Afterwards, she was beheaded along with five men (her brother included) accused of adultery with her. The marriage was then declared invalid, so that Elizabeth, just like her half sister, became a bastard.

Henry immediately married Jane Seymour, who became pregnant almost as quickly. On October 12, 1537, she gave birth to a healthy boy, Edward, which was greeted with huge celebrations. The king's quest for a son was finally over, so long as Edward could be kept healthy. However, the queen died of puerperal sepsis ten days later. Henry genuinely mourned her death, and at his own passing nine years later, he was buried next to her.

The king married a fourth time in 1540, to the German Anne of Cleves for a political alliance with her Protestant brother, the Duke of Cleves. He also hoped to obtain another son in case something should happen to Edward. Anne proved a dull, unattractive woman and Henry declined to consummate the marriage. He quickly divorced her, and she remained in England as a kind of adopted sister to him. So he married again, to a 19-year old named Catherine Howard. But when it became known that she was neither a virgin at the wedding, nor a faithful wife afterwards, she ended up on the scaffold and the marriage declared invalid. His sixth and last marriage was to Catherine Parr, more a nursemaid to him than anything else, as his health was failing (it had declined ever since the jousting accident in 1536).

In 1542, the king embarked on a new campaign in France, but unlike in 1512, he only managed with great difficulty. The war netted England the city of Boulogne, but nothing else, and the French retook it in 1549. Scotland also declared war and at Solway Moss was once again totally defeated.

Henry's paranoia and suspicion worsened in his last years. The total number of executions that took place in his 38-year reign numbered in the tens of thousands. He died in January 1547 at the age of 55 and was succeeded by his son.

Although he showed piety and intelligence, Edward VI was only nine years old when he took the throne in 1547. His uncle, Edward Seymour, 1st Duke of Somerset tampered with Henry VIII's will and obtained letters patent giving him much of the power of a monarch by March 1547. He took the title of Protector. Whilst some see him as a high-minded idealist, his stay in power culminated in a crisis in 1549 when many counties of the realm were up in protest. Kett's Rebellion in Norfolk and the Prayer Book Rebellion in Devon and Cornwall simultaneously created a crisis during a time when invasion from Scotland and France were feared. Somerset, disliked by the Regency Council for his autocratic methods, was removed from power by John Dudley, who is known as Lord President Northumberland. Northumberland proceeded to adopt the power for himself, but his methods were more conciliatory and the Council accepted him. It was during Edward's reign that England became a Protestant nation as opposed to a Catholic one in schism from Rome.

Edward was beginning to show great promise when he fell violently ill with tuberculosis in 1553 and died that August two months short of his 16th birthday. Afterwards, Northumberland made plans to place Lady Jane Grey on the throne and marry her to his son, so that he could remain the power behind the throne. His putsch failed, Jane Grey was beheaded, and Mary I took the throne amidst popular demonstration in her favour in London, which contemporaries described as the largest show of affection for a Tudor monarch. Mary had never been expected to hold the throne, at least not since Edward was born. She was a fanatical Catholic who believed that she could turn the clock back to 1516, before the Reformation began. Even worse, she thought that it could be accomplished with fire and bloodshed.

Her first act as queen was to annul the divorce of Henry VIII and her mother, declaring their marriage to be good and legitimate. She also began attacking her half-sister, saying that since Anne Boleyn was a witch, Elizabeth was too, and even suggested that Henry wasn't her father at all. Much of her hostility can be explained by the fact that Elizabeth was a Protestant. Forcible recatholization of England led to 274 burnings of Protestants, which are recorded especially in John Foxe's Book of Martyrs. Mary then married her cousin Philip, son of the emperor Charles V, and King of Spain when Charles abdicated in 1556. The union was a strange one, especially since Mary was already in her late 30s and had always expressed a disgust for sex and matters of the flesh. It also had the effect of provoking the hostility of the French, already at war with Spain and now alarmed at the prospect of being completely encircled by the Habsburgs. Calais, the last English outpost on the Continent, was then taken by France. Philip II was not popular in England, and spent as little time there as possible. Mary eventually became pregnant, or at least believed herself to be. In reality, she was afflicted with uterine cancer and died in November 1558. Her death was greeted with huge celebrations. She successfully suppressed a rebellion by Sir Thomas Wyatt.
[edit] Elizabeth
Main article: Elizabethan era

The reign of Elizabeth restored a sort of order to the realm following the turbulence of the reigns of Edward and Mary when she came to the throne following the death of Mary in 1558. The religious issue which had divided the country since Henry VIII was in a way put to rest by the Elizabethan Religious Settlement, which re-established the Church of England. Much of Elizabeth's success was in balancing the interests of the Puritans and Catholics. She managed to offend neither to a large extent, although she clamped down on Catholics towards the end of her reign as war with Catholic Spain loomed. The war lasted from 1585-1603. In 1588, the Spanish Armada was decisively defeated, marking the beginning of England's rise as a naval power. Indecisive skirmishing continued throughout the 1590s, with English privateers pillaging Spanish commerce from America.

Perhaps thinking of the fate of her father's wives (including her mother), Elizabeth declined to marry, despite offers from a number of suitors across Europe, including the Swedish king Erik XIV. This created endless worries over her succession, especially in the 1570s when she nearly died of smallpox. It has been often rumored that she had a number of lovers (including Francis Drake), but there is no hard evidence.
Queen Elizabeth

Elizabeth maintained relative government stability apart from the Revolt of the Northern Earls in 1569, she was effective in reducing the power of the old nobility and expanding the power of her government. Elizabeth's government did much to consolidate the work begun under Thomas Cromwell in the reign of Henry VIII, that is, expanding the role of the government and effecting common law and administration throughout England. During the reign of Elizabeth and shortly afterward, the population grew significantly: from three million in 1564 to nearly five million in 1616.[1]

The queen ran afoul of her cousin Mary, Queen of Scots, who was a devote Catholic and had been forced to abdicate her throne as a consequence (Scotland had recently become Protestant). She fled to England, where Elizabeth immediately had her arrested. Mary spent the next 18 years in confinement, but proved too dangerous to keep alive, as the Catholic powers in Europe considered her, not Elizabeth, the legitimate ruler of England. She was eventually tried for treason and sentenced to death, being beheaded in February 1587.

In all, the Tudor period is seen as a decisive one which set up many important questions which would have to be answered in the next century and during the English Civil War. These were questions of the relative power of the monarch and Parliament and to what extent one should control the other. Some historians think that Thomas Cromwell affected a "Tudor Revolution" in government, and it is certain that Parliament became more important during his chancellorship. Other historians say the "Tudor Revolution" really extended to the end of Elizabeth's reign, when the work was all consolidated. Although the Privy Council declined after the death of Elizabeth, while she was alive it was very effective.
[edit] 17th century
Main article: 17th century England
[edit] Union of the Crowns

Elizabeth died in 1603 at the age of 69. Her closest male Protestant relative was the King of Scots, James VI, of the House of Stuart, who became King James I of England in a Union of the Crowns. King James I & VI as he was styled became the first monarch to rule the entire island of Great Britain, although it was merely a union of the English and Scottish crowns, and both countries remained separate political entities until 1707. Several assassination attempts were made on James, notably the Main Plot and Bye Plots of 1603, and most famously, on 5 November 1605, the Gunpowder Plot, by a group of Catholic conspirators, led by Guy Fawkes, which caused more antipathy in England towards the Catholic faith. Upon taking power, James immediately made peace with Spain, and for the first half of the 17th century, England remained largely inactive in European politics.
[edit] Colonial England

In 1607 England built an establishment at Jamestown This was the beginning of colonialism by England in North America. Many English settled then in North America for religious or economic reasons. About 70% of migrants from England who came between 1630-1660 were indentured servants. By 1700, Chesapeake planters brought in about 100,000 indentured servants,[9] more than 75% of all European immigrants to Virginia and Maryland.[10] The English merchants holding plantations in the warm southern parts of America then resorted rather quickly to the slavery of Native Americans and imported Africans in order to cultivate their plantations and sell raw material (particularly cotton and tobacco) in Europe. The English merchants involved in colonization amassed fortunes equal to those of great aristocratic landowners in England, and their money, which fuelled the rise of the middle class, permanently altered the balance of political power. The American colonies did not prove profitable to the mother country in the end. Pennsylvania and Delaware were home to a large population of self-sufficient farmers from various parts of Europe, especially Germany. New York traded with pirates and smugglers, and the colonies of New England consistently frustrated the government's attempts to utilize the area's forests for shipbuilding. Only Virginia and the Chesapeake Bay area produced a useful cash crop, tobacco, but it quickly wore the soil out. By the end of the 18th century, the tobacco industry in Virginia had been completely ruined by soil exhaustion and low prices. Indeed, the small sugar-growing islands in the Caribbean were worth more than all of the thirteen colonies put together.

The English colonies did not have an independent foreign policy, but otherwise were mostly left to manage their own affairs. This was very different from the authoritarian control France and Spain held over their colonies. It was the gradual infringement on the rights of the colonies starting in the 1760s that would lead to the American War of Independence. Nothing of the sort would have been possible in the French and Spanish colonies.
Maps of territory held by Royalists (red) and Parliamentarians (green) during the English Civil War (1642–1645)
[edit] English Civil War
Further information: English Civil War

The First English Civil War broke out in 1642, largely as a result of an ongoing series of conflicts between James' son, Charles I, and Parliament. The defeat of the Royalist army by the New Model Army of Parliament at the Battle of Naseby in June 1645 effectively destroyed the king's forces. Charles surrendered to the Scottish army at Newark. He was eventually handed over to the English Parliament in early 1647. He escaped, and the Second English Civil War began, although it was a short conflict, with the New Model Army quickly securing the country. The capture and subsequent trial of Charles led to his beheading in January 1649 at Whitehall Gate in London, making England a republic. The trial and execution of Charles by his own subjects shocked the rest of Europe (the king argued to the end that only God could judge him) and was a precursor of sorts to the beheading of Louis XVI 145 years later.

The New Model Army, under the command of Oliver Cromwell, then scored decisive victories against Royalist armies in Ireland and Scotland. Cromwell was given the title Lord Protector in 1653, making him 'king in all but name' to his critics. After he died in 1658, his son Richard Cromwell succeeded him in the office but he was forced to abdicate within a year. For a while it looked as if a new civil war would begin as the New Model Army split into factions. Troops stationed in Scotland under the command of George Monck eventually marched on London to restore order.
[edit] Restoration of the monarchy

The monarchy was restored in 1660, with King Charles II returning to London.
King Charles I, who was beheaded in 1649

In 1665, London was swept by a visitation of the plague, and then, in 1666, the capital was swept by the Great Fire, which raged for 5 days, destroying approximately 15,000 buildings. After the Restoration, there was an overall reduction in the power of the crown, and by the 18th century England rivaled the Netherlands for being one of the freest countries in Europe.
[edit] Glorious Revolution

In 1680, the Exclusion crisis occurred due to widespread objections to a Catholic serving as the King of England, since James was the apparent heir to Charles, who was the king at that time. After the death of Charles II in 1685, his Catholic brother King James II & VII was crowned. From that point, there were various factions pressing for the Dutch Protestant Prince William of Orange and his wife, Mary, to replace King James II in what became known as the Glorious Revolution.

In November 1688, William landed in England with an invading force, and succeeding in being crowned king. After this, James attempted to retake the throne by force in the Williamite War, and was finally defeated by William at the Battle of the Boyne in 1690.

In December 1689, one of the most important constitutional documents in English history, the Bill of Rights, was passed.[11] The Act, which restated and confirmed many provisions of the earlier Declaration of Right, established restrictions on the royal prerogative. It provided, amongst other things, that the Sovereign could not suspend laws passed by Parliament, levy taxes without parliamentary consent, infringe the right to petition, raise a standing army during peacetime without parliamentary consent, deny the right to bear arms to Protestant subjects, unduly interfere with parliamentary elections, punish members of either House of Parliament for anything said during debates, require excessive bail or inflict cruel and unusual punishments.[12] William was opposed to the imposition of such constraints, but he chose not to engage in a conflict with Parliament and agreed to abide by the statute.[13]

In parts of Scotland and Ireland, Catholics loyal to James remained determined to see him restored to the throne, and there followed a series of bloody though unsuccessful uprisings. As a result of these, any failure to pledge loyalty to the victorious King William was severely dealt with. The most infamous example of this policy was the Massacre of Glencoe in 1692. Jacobite rebellions continued on into the mid-18th century until the son of the last Catholic claimant to the throne, (James III & VIII), mounted a final campaign in 1745. The Jacobite forces of Prince Charles Edward Stuart, the "Bonnie Prince Charlie" of legend, were defeated at the Battle of Culloden in 1746.
[edit] 18th and 19th centuries
Main article: History of the United Kingdom

In the early 1700s, there were roughly 10 million people living in England, and an estimated two million were, “vagrants, rogues, prostitutes, beggars or indigents.”[14] In 18th century England, half the population was at least occasionally dependent on charity for subsistence.[15]
[edit] Formation of the United Kingdom

The Acts of Union between the Kingdom of England and the Kingdom of Scotland were a pair of Parliamentary Acts passed by both parliaments in 1707, which dissolved them in order to form a Kingdom of Great Britain governed by a unified Parliament of Great Britain according to the Treaty of Union. The Acts joined the Kingdom of England and the Kingdom of Scotland (previously separate states, with separate legislatures but with the same monarch) into a single Kingdom of Great Britain.[16]

The two countries had shared a monarch since the Union of the Crowns in 1603, when King James VI of Scotland inherited the English throne from his double first cousin twice removed, Queen Elizabeth I. Although described as a Union of Crowns, until 1707 there were in fact two separate Crowns resting on the same head. There had been three attempts in 1606, 1667, and 1689 to unite the two countries by Acts of Parliament, but it was not until the early 18th century that the idea had the will of both political establishments behind them, albeit for rather different reasons.

The Acts took effect on 1 May 1707. On this date, the Scots Parliament and the English Parliament united to form the Parliament of Great Britain, based in the Palace of Westminster in London, the home of the English Parliament.[17] Hence, the Acts are referred to as the Union of the Parliaments. On the Union, historian Simon Schama said "What began as a hostile merger, would end in a full partnership in the most powerful going concern in the world ... it was one of the most astonishing transformations in European history."[18]

In 1714, the reign of Queen Anne ended. Anne was the last monarch of the House of Stuart. She was succeeded by her second cousin, George I, of the House of Hanover, who was a descendant of the Stuarts through his maternal grandmother, Elizabeth, daughter of James VI & I.[19] A series of Jacobite rebellions broke out in an attempt to restore the Stuart monarchy, but all ultimately failed. Several Planned French Invasions were attempted, also with the intention of placing the Stuarts on the throne.
The first general laws against child labour, the Factory Acts, were passed in Britain in the first half of the 19th century. Children younger than nine were not allowed to work and the work day of youth under the age of 18 was limited to twelve hours.[20]

The Act of Union of 1800 formally assimilated Ireland within the British political process and from 1 January 1801 created a new state called the United Kingdom of Great Britain and Ireland, which united the Kingdom of Great Britain with the Kingdom of Ireland to form a single political entity. The English capital of London was adopted as the capital of the Union.
[edit] Industrial Revolution
Main article: Economic history of Britain

During the late 18th and early 19th centuries, there was considerable social upheaval as a largely agrarian society was transformed by technological advances and increasing mechanization, which was the Industrial Revolution. Much of the agricultural workforce was uprooted from the countryside and moved into large urban centres of production, as the steam-based production factories could undercut the traditional cottage industries, because of economies of scale and the increased output per worker made possible by the new technologies. The consequent overcrowding into areas with little supporting infrastructure saw dramatic increases in the rate of infant mortality (to the extent that many Sunday schools for pre-working age children (5 or 6) had funeral clubs to pay for each others funeral arrangements), crime, and social deprivation.

The transition to industrialization was not wholly seamless for workers, many of whom saw their livelihoods threatened by the process. Of these, some frequently sabotaged or attempted to sabotage factories. These saboteurs were known as "Luddites".
[edit] 20th and 21st centuries
[edit] Political issues

Following years of political and military agitation for 'Home Rule' for Ireland, the Anglo-Irish treaty of 1921 established the Irish Free State (now the Republic of Ireland) as a separate state, leaving Northern Ireland as part of the United Kingdom. The official name of the UK thus became "The United Kingdom of Great Britain and Northern Ireland".

England, as part of the UK, joined the European Economic Community in 1973, which became the European Union in 1993.
[edit] See also

    * Bretwalda
    * Commonwealth of Nations
    * Danelaw
    * English and French monarchs overlap chart
    * English people
    * History of the British constitution
    * Social history of England
    * Population of England - historical estimates
    * History of the Jews in England
    * History of the British Isles
    * History of the United Kingdom
    * History of Scotland
    * History of Ireland
    * History of Wales
    * List of British monarchs, British monarchs' family tree
    * Politics of the United Kingdom
    * Timeline of English history
    * A bad settlement is better than a good lawsuit.
    * A coin of gold is delighting in a bag of silver coins
          o Meaning: English people make modest company.
          o Alternative meaning: One who is unique is often praised or receives more pleasure.
    * A journey of a thousand miles starts with a single step.
          o Laozi, Tao Te Ching, Ch. 64, line 12. 千里之行，始于足下
    * A bad penny always turns up.
          o Meaning: Your mistakes will come back to haunt you. Or Bad people will always return.
    * A bean in liberty is better than a comfit in prison.
    * A bellyful is one of meat, drink, or sorrow.
    * A bellyful of food is a good one.
    * A good enemy is a better person than a false friend.
    * A big tree attracts the woodsman's axe.
          o Meaning: Great people will attract great criticism.
          o Possible interpretation: The rich make good targets for thieves and burglars.
    * An apple a day keeps the doctor away.
          o Originated in the 1900s as a marketing slogan dreamed up by American growers concerned that the temperance movement would cut into sales of apple cider. (Michael Pollan, The Botany of Desire, Random House, 2001, ISBN 0375501290, p. 22, cf. p. 9 & 50)
    * A bad workman blames his tools.
          o George Herbert reports early English variants in Jacula Prudentum; or, Outlandish Proverbs, Sentences, Etc. (1640):
                + Never had an ill workman good tools.
                + An ill labourer quarrels with his tools.
                      # The Works of George Herbert in Prose and Verse; 1881, New York: John Wurtele Lovell, Pub.; pp. 440 & 454
          o Compare the older French proverb:
                + Outil: ... Merchant ouvrier ne trouvera ja bons outils: Prov. A bungler cannot find (or fit himselfe with) with good tooles.
                      # Randle Cotgrave, A Dictionarie of the French and English Tongues (1611)
          o Galen explains clearly, if less succinctly, in De Causis Procatarcticis (2nd c. A.D.), VI. 63–65:
                + They blame their tools: why did the carpenter make the bed so badly, if he was any good? He will reply: "Because I used a poor axe and a thick gimlet, because I did not have a rule, I lost my hammer, and the hatchet was blunt", and other things of this kind. And the scribe, asked why he wrote so badly, will say that the paper was rough, the ink too fluid, the pen blunt, that he did not have a smoother, so that he could not write any better. Once again, this man holds his material responsible, and blames his tools as well, in mentioning the pen and smoother. And who does not know that artisans make themselves responsible for the deficiencies in their work too, when they cannot pin the blame on material and tools?
                      # Galen On Antecedent Causes, Tr. R. J. Hankinson, Cambridge University Press, 1998, ISBN 0521622506, p. 90–93
    * A bird in the hand is worth two in the bush.
          o John Bunyan cites this traditional proverb in The Pilgrim's Progress, (1678):
                + So are the men of this world: They must have all their good things now; they cannot stay till the next year, that is, until the next world, for their portion of good. That proverb, "A bird in the hand is worth two in the bush," is of more authority with them than are all the divine testimonies of the good of the world to come.
    * A burnt child dreads the fire.
          o Chinese Version: One bitten by a snake for a snap dreads a rope for a decade.一朝被蛇咬，十年怕井绳
          o Indian Version: The one burnt by hot milk drinks even cold buttermilk with precaution. Transliteration: Doodh ka jala chhanchh ko bhi phoonk phoonk ke peeta hai.
          o Meaning: Similar to "Once bitten, twice shy"
          o This Proverb intimates, That it is natural for all living Creatures, whether rational or irrational,
            to consult their own Security, and Self-Preservation; and whether they act by Instinct or Reason, it still
            tends to some care of avoiding those things that have already done them an Injury. - Divers Proverbs, Nathan Bailey, 1721 [1]
    * A candle loses nothing by lighting another candle.
          o Attributed to Mevlana Celaleddin-i Rumi
    * A little pot is easily hot.
    * A new broom sweeps clean.
    * A cat may look at a king.
          o Meaning: If a cat may look at the king - then I have a right to look where I please.
    * A camel is a horse designed by committee.
          o Meaning: a vision is more perfect from the individual rather than a group of people where it becomes anodyne.
    * A chain is no stronger than its weakest link.
          o Meaning: The strength of any group depends on the individual strength of each of its members.
    * A closed mouth catches no flies.
          o Meaning: You cannot say a bad thing if you don't speak at all.
    * A constant guest is never welcome.
    * A coward dies a thousand times before his death. The valiant never taste of death but once.
          o From William Shakespeare's Julius Caesar[1]
          o Meaning: The valiant (the brave) take no account of possible danger, whereas cowards are constantly fearing the worst. [2]
    * A drop of knowledge is greater than an ocean of strength.
    * A fool and his money are soon parted.
    * A fox smells its own lair first. Or: A fox smells its own stink first.
          o Meaning: One knows where they belong, and knows when they make a mistake.
    * A friend in need is a friend indeed.
          o Meaning: A genuine friend is with you even in times of trouble.

    * A good beginning makes (for) a good ending.
          o Chinese Version: A good beginning is half a succession-好的开始是成功的一半
          o Meaning: Planning is the key to success.
    * A good man in an evil society seems the greatest villain of all.
    * A good surgeon has an eagle's eye, a lion's heart, and a lady's hand.
    * A guilty conscience needs no accuser.

    * A half truth is a whole lie.

    * A jack of all trades is master of none.

    * A kingdom is lost for want of a shoe.
          o See: "For want of a nail the shoe is lost, ..."

    * A lie can be halfway around the world before the truth gets its boots on.
          o Charles Spurgeon. A great lie may be widely accepted before the truth comes to light.
    * A little knowledge is a dangerous thing.
          o A little Learning is a dangerous Thing;
            Drink deep, or taste not the Pierian Spring:
            There shallow Draughts intoxicate the Brain,
            And drinking largely sobers us again. ~ Alexander Pope
    * A loaded wagon makes no noise.
          o People with real wealth don't talk about it.

    * A man is known by the company he keeps.
    * A miss by an inch is a miss by a mile.
          o Meaning: A miss is a miss regardless the distance

    * A man's home is his castle.
          o William Blackstone refers to this traditional proverb in Commentaries on the Laws of England (1765–1769), Book 4, Chapter 16:
                + And the law of England has so particular and tender a regard to the immunity of a man's house, that it stiles it his castle, and will never suffer it to be violated with immunity: agreeing herein with the sentiments of ancient Rome, as expressed in the works of Tully; quid enim sanctius, quid omni religione munitius, quam domus unusquisque civium?
                      # Translation: What more sacred, what more strongly guarded by every holy feeling, than a man's own home?

    * A night with Venus and a life with mercury.
          o Anti-promiscuity adage, alluding to a 18th-century mercury-based folk treatment for syphilis
          o Cited in Bartz, Diane, "Har, me hearties! Excavating Blackbeard's ship", Reuters (via Yahoo! News), 30 October 2006. URL accessed on 2006-11-01.

    * A paragraph should be like a lady's skirt: long enough to cover the essentials but short enough to keep it interesting.
    * A Pasoly in the eye is worth several in the shins.
          o A good shot is worth many bad ones
    * A penny saved is a penny earned.
          o Attributed to Benjamin Franklin, Poor Richard's Almanac
    * A penny spent is a penny earned.
          o In contrast to spending on the poor people.
          o Interpretation: capitalist alteration of Ben Franklin's original saying ["A penny saved is a penny earned"]. The concentration on spending rather than saving promotes the contemporary capitalist economic theory of putting money back into the economy (rather than hording it) to create more wealth.
    * A penny earned is a penny lost; a penny shared is a penny well-spent.
    * "A person who laughs may not be happy, but he's hide the sadness in his heart". (Al Sagheer, Suhail)
    * A picture is worth a thousand words.
          o An instant sight may save a thousand words.
          o A snap of sight may describe much more than a thousand words.
    * A pint of plain is yer only man.
          o Meaning: You need to make the initial step if you are ever to complete a task.

    * A rising tide lifts all boats
          o This traditional proverb is sometimes attributed to John F. Kennedy because he repeated it several times, but he disclaimed originality in his address in the Assembly Hall at the Paulskirche in Frankfurt, West Germany, 25 June 1963:
                + As they say on my own Cape Cod, a rising tide lifts all the boats.
    * A rolling stone gathers no moss.
          o A Turkish Proverb
          o Interpretation: A person who is active will not grow stale.
          o Alternative interpretation: A person who does not stay in one place very long will not develop roots or meaningful connections with others.
          o Philip K. Dick in We Can Build You (1972) conceives a world where the latter interpretation has become the norm and the former indicative of a mental disorder.

    * A son is a son 'till he gets him a wife; a daughter's a daughter all her life.
          o Interpretation: the relationship between a daughter and her parents is enduring; the relationship with a son is attenuated after he marries.
    * A still tongue makes a wise head.
          o From Lewis the (Black) Barber; Lake Charles, LA; who always told people, "Never let the right hand know what the left hand is doing; a still tongue makes a wise head; still water runs deep."
    * A stitch in time saves nine.
          o Fix the small problem now before it becomes larger and harder to fix.

    * A thief thinks everyone steals.

    * A watched pot never boils.
          o Main interpretation: Time seems to pass quicker when you aren't consciously waiting for something
          o Possible interpretation: Worrying over something can make the task seem to take longer than it should.
    * A woman's work is never done.
          o From a folk rhyme - A man may work from sun to sun, but a woman's work is never done, meaning that a man's traditional role as breadwinner may keep him occupied from sun-up to sundown, but the traditional roles of a woman demand even longer hours of work.
    * A word spoken is past recalling.
          o Alternative: What's done is done (so think before doing).
          o Interpretation: Once you say something hurtful, provocative, etc., you can't take it back.
    * A woman is like a cup of tea; you'll never know how strong she is until she boils
          o Meaning: Never underestimate people; they could be stronger than you think
          o Possible interpretation: Don't pester your wife too often, unless you want her to never cook for you again.
    * Ability can take you to the top, but it takes character to keep you there.
    * Absence makes the heart grow fonder.
          o From Isle of Beauty by Thomas Haynes Bayly
          o Interpretation: We miss people when we are separated from them.
    * Absence makes the heart grow fonder but makes the mind forget.

    * Act today only, tomorrow is too late
    * Action is the proper fruit of knowledge.
    * Actions speak louder than words.
          o meaning: What you do is more important that what you say

    * Advice most needed is least heeded.

    * After dinner sit a while, after supper walk a mile.

    * All cats love fish but hate to get their paws wet.
          o sometimes you have to do bad things to get good ones
    * All the world is your country, to do good is your religion.
    * All flowers are not in one garden.
    * All frills and no knickers.
          o Possible interpretation: All style and no substance.
    * All fur coat and no knickers.
          o Meaning: Very concerned with outward appearance and material things, but with no modesty.
    * All good things must come to an end.
    * All hat and no cattle.
          o Possible interpretation: All talk and appearance and little or no substance.
    * All roads lead to Rome.
          o Possible interpretation: However you try to go about things all will lead to the same conclusions
          o Possible interpretation: Power draws all things to itself.
          o Interpretation: The heartland/metropolis (for better or worse) yields considerable power.
          o Meaning: The first roads were built by the Romans and at the time of the Roman empire, all roads led to Rome.
    * All's fair in love and war.
          o Interpretation: Love and War are arenas of complete passion that often obfuscate reason.
    * All for one and one for all.
          o Alexandre Dumas, The Three Musketeers
    * All's well that ends well.
          o A play by William Shakespeare
          o Variant: All is well that ends well. - Divers Proverbs, Nathan Bailey, 1721 [2]
    * All sizzle and no steak.
          o Possible interpretation: All style and no substance
    * All that glisters is not gold.
          o William Shakespeare, The Merchant of Venice, act II, scene 7.
          o Often corrupted to: All that glitters is not gold.
          o Possible interpretation: Not everything is what it appears to be.
    * All things come to those who wait.
    * All work and no play makes Jack a dull boy. All play and no work makes Jack a mere toy.
    * Always care about your flowers and your friends. Otherwise they'll fade, and soon your house will be empty.
    * An early bird catches worms.
          o ... but the second mouse gets the cheese.
    * An Englishman's home is his castle.
          o Variant of "A man's home is his castle."
    * An eye for an eye and a tooth for a tooth.
          o Translaton: If you kill a neighbor's ox you must buy him a new one. (In biblical times.)
          o Possible interpretation: retribution should be equitable, proportionate and "fit the crime". Biblical reference, modern usage often connotes support for capital punishment.
          o A common response, often attributed to Mahatma Gandhi, is "An eye for an eye will make the whole world blind," is often used as a criticism for this concept, implying that "an eye for an eye" will only perpetuate a potentially endless cycle of violence.
    * An empty vessel makes the most noise
          o Those with the least understanding often complain about things the most.
    * An old dog will learn no tricks. - Divers Proverbs, Nathan Bailey, 1721 [3]
    * An ounce of prevention is worth a pound of cure.
          o Possible interpretation: Similar to that of A stitch in time saves nine. Preventing something in advance is better than fixing it later on.
    * An ounce of discretion is worth a pound of wit.
          o Meaning: it is better to be careful and discrete than to be clever.

    * April showers bring May flowers.
          o Meaning: Something seeming bad or boring now brings good things in the future.

    * As fit as a fiddle.
          o Meaning: very fit and well
    * As iron sharpens iron, so one man sharpens another
    * As soon as a man is born, he begins to die.
    * As you make your bed, so you must lie in it.
          o Similar to "You reap what you sow"
    * Ask me no questions, I'll tell you no lies.
          o Interpretation: There are some things I'd rather not say, so don't ask me!

    * Aught for naught, and a penny change.
          o Interpretation: you can't get something for nothing -- you might as well expect to get paid to take it.

[edit] B

    * Bad news travels fast.
    * Barking dogs seldom bite.
          o Meaning: People who are busy complaining rarely take more concrete hostile action.
          o Alternate meaning: Those who cast threats will seldom follow through with them
    * Barking up the wrong tree.
    * Be careful before every step.
    * Be careful what you wish for, you just might get it.
    * Before criticizing a man, walk a mile in his shoes.
          o Meaning: One should not criticize a person without understanding their situation.
    * Beginning is half done.
          o Quoted by Dr. Robert Schuller, West Coast clergyman.
    * Beggars can't be choosers.
          o Meaning: Those who are in need of help can't afford to be too demanding.
    * Better is the enemy of good.
    * Better to have it and not need it than to need it and not have it.
    * Better to remain silent and be thought a fool, than to open your mouth and remove all doubt.
          o Variant: Better to remain silent and thought a fool, than to speak and remove all doubt. (often attributed to Abraham Lincoln but taken from Solomon's Proverbs)
    * Better late than never.
          o Meaning: It's better to make an effort to keep an appointment than to give up altogether when you discover you will be late.
    * Better safe than sorry.
          o Meaning: It is better to take precautions when it's possible that something can go amiss than to regret doing nothing later if something should indeed go wrong.
    * Better the devil you know (than the one you don't).
    * Beware of the Bear when he tucks in his shirt.
    * Beware of the false prophets, who come to you in sheep's clothing, and inwardly are ravening wolves. (Matthew; bible quote)
    * Beware of Greeks bearing gifts.
          o A reference to the Trojan Horse
    * Birds of a feather flock together.
          o Variant: Birds of the same feather flock together.
                + Meaning: People who are similar to one another tend to stay together.
    * Bitter pills may have blessed effects.
          o Meaning: Things that seem hard to take or handle at first may have positive and beneficial outcomes.
    * Blood is thicker than water.
          o Meaning: Bonds between family members are stronger than other relationships.
    * Blood will out.
          o Meaning: A person's ancestry or upbringing will eventually show.
    * Bloom where you are planted.
          o Meaning: Excel and flourish where you grow up, or where you fit in; be good at what you do.

    * A blow with a word strikes deeper than a blow with a sword.
          o Robert Burton cites this traditional proverb in The Anatomy of Melancholy (1621):
                + It is an old saying, "A blow with a word strikes deeper than a blow with a sword:" and many men are as much galled with a calumny, a scurrilous and bitter jest, a libel, a pasquil, satire, apologue, epigram, stage-play or the like, as with any misfortune whatsoever.
                      # Part I, Section II, Member IV, Subsection IV
          o Compare: "The pen is mightier than the sword."
          o Contrast: "Sticks and stones may break my bones but words will never hurt me."

    * Born with a silver spoon in his/her mouth.
          o Meaning: Born in a rich family.
    * Boys will be boys.
          o Meaning: Boys are traditionally expected to misbehave, while girls are not.
    * Brag is a good Dog, but Holdfast is a better
          o This Proverb is a Taunt upon Braggadoccio's, who talk big, boast, and rattle:
            It is also a Memento for such who make plentiful promises to do well for the
            future but are suspected to want Constancy and Resolution to make
            them good. - Divers Proverbs, Nathan Bailey, 1721 [4]
    * Brain is better than brawn.
    * Bread is the stuff of life.
    * Break the Law as the Law should be beaten.
    * Buy the best and you only cry once.

[edit] C

    * Cometh the hour cometh the man.
          o (Some information about the phrase and about its use by a 1940's cricketer)
    * Cowards die many times before their deaths; The valiant never taste of death but once. William Shakespeare, Julius Caesar (play)[3]
          o Meaning: The valiant (the brave) take no account of possible danger, whereas cowards are constantly fearing the worst. [4]

[edit] D

    * A dull pencil is greater than the sharpest memory.
    * Damned if you do, damned if you don't.
          o Lorenzo Dow (d. 1834).[5]
          o Meaning: Refers to a situation where both possibilities will lead to harm or blame.
    * Desperate times call for desperate measures.
    * Different strokes for different folks.
          o Meaning: Someone prefers one thing; others, something different.
    * Discretion is the better part of valour.
          o Derived from "The better part of valour is discretion, in the which better part I have saved my life." Falstaff in Shakespere's Henry IV Part One.
          o Meaning: Caution is preferable to rash bravery.
    * Do unto others as you would have them do unto you.
          o Based on the Bible (Matthew 7:12; Luke 6:31).[6]; a statement of the ethic of reciprocity
    * Do it today, tomorrow it may be against the law.
    * Doctors make the worst patients.
    * Don't ask God to guide your footsteps if you're not willing to move your feet.
    * Don't bite the hand that feeds you.
          o Meaning: Behave respectfully or deferentially to those who provide for you.
    * Don't burn your bridges.
          o Meaning: Do not act in such a way as to leave yourself no alternative or no opportunity to "retreat."
    * Don't count your chickens before they're hatched.
    * Don't bite off more than you can chew.
          o Meaning: Do not take on more responsibility than you can handle at any one time.
    * Don't cry over spilt milk.
          o Meaning: Don't worry about things that have already happened.
    * Don't cut off your nose to spite your face.
          o Interpretation: Do not act to spite someone else if it is damaging to yourself.
    * Don't fall before you're pushed.
    * Don't judge a man by the size of his hat, but by the angle of his tilt.
    * Don't have too many irons in the fire.
          o Possible interpretation: Do not take on more responsibility than you can handle.
    * Don't judge a book by its cover.
          o Meaning: Do not judge by appearances.
    * Don't look a gift horse in the mouth.
          o Possible interpretation: Do not look for faults in a gift.
    * Don't make a mountain out of a molehill.
          o Don't exaggerate small things / Don't make a big deal out of something minor.
    * Don't mend what ain't broken.
          o Alternatively, If it ain't broke, don't fix it.
          o Alternatively, Leave well enough alone.
    * Don't put all your eggs in one basket.
          o Meaning: Do not rest all your hopes on one eventuality; plan for several cases.
    * Don't put the cart before the horse.
          o Meaning: Do things in the correct order.
          o Cf. Dan Michael of Northgate, Ayenbite of Inwyt (1340): "Many religious folk set the plough before the oxen." (Middle English: "Moche uolk of religion зetteþ þe зuolз be-uore þe oksen.")
    * Don't raise more Demons than you can lay down.
    * Don't shut the barn door after the horse is gone.
          o Possible interpretation: Prepare for things to go wrong rather than worrying about them after the fact.
    * Don't spit into the wind.
          o Or, Don't piss into the wind.
          o Meaning, don't take actions which you know will harm yourself or be futile.
    * Don't spoil the ship for a ha'p'orth of tar.
          o Meaning: Don't jeopardize a project - especially a large one - by being miserly or cutting corners.
                + A ha'p'orth (pronounced haypeth) is a halfpenny-worth, i.e. a very small amount.
    * Don't take life too seriously; you'll never get out of it alive.
    * Don't throw the baby out with the bathwater.
          o Possible interpretation: Do not, in an attempt to remove something undesirable, lose things that are valuable.
    * Don't try to teach a pig to sing. It doesn't work, and you'll annoy the pig.
          o Meaning: Don't go into a relationship expecting to change your partner, it doesn't work.
    * Don't cross a bridge before you come to it.
          o Meaning: Don't fret unnecessarily about future problems.
    * Doubt is the beginning, not the end, of wisdom.
    * Don't bring a knife to a gun fight.
    * Don't let procrastination eat your own clock.
          o Meaning: Don't procrastinate most of the time as your chances and opportunities are wasted away.
    * Dreams are not the ones which come when you sleep, but they are the ones which will not let you sleep.
          o Meaning: Dreams in your sleep are different from the dreams of your future.
                + Distance makes the heart grow fonder.
          o Don't bark if you can't bite.

 meaning - Don't complain if you can't enforce your point of view.

    *
          o Don't dig your grave with your own knife and fork.

 meaning - Don't do something to yourself which causes your own downfall.

[edit] E

    * Chacun à son goût
          o French for "Each to his own taste"
          o Alternatively: à chacun son goût - "To each his own".
    * Early to bed and early to rise, makes a man healthy, wealthy and wise. (attibuted to Benjamin Franklin, Poor Richard's Almanac)
    * The early bird catches the worm. But the second mouse gets the cheese.
    * The ends justify the means.
    * Enjoy what you don't know.
    * Even a dog can distinguish between being stumbled over and being kicked.
    * Even a dog can make it to the top when there's a flood.
    * Even an old dog likes to be patted on the head and told, "Good boy!" -Justice Holmes
    * Even angels have teeth.
          o Nathaniel Wenger "Poetry to Grow a Tree"
    * Every dog has its day.
          o Variation on a quote from Hamlet: "...whatever Hercules says, the cat will mew and dog will have its day."
    * Every cloud has a silver lining.
    * Everyday living is life lessons. by Allen Zimama.
          o Meaning: Every negative thing has positive aspects.
    * Everything good in life is either illegal, immoral, or fattening.
    * Everyone wants to go to heaven, but no one wants to die.
    * Empty vessels make most noise/sound.
          o Meaning: Those who lack intelligence speak the most/loudest.
    * Even a broken/stopped clock is right twice a day.
    * Even the best perfumes of the world lose their fragrance when you are not around me.
    * Education is a progressive discovering of our own ignorance. <W. Durrant>
    * Education makes machines which act like men and produces men who act like machines
    * Every rose has its thorn.
          o Meaning: Every good thing has its downside
    * Everything can be justified until it happens to you.
    * Everything with time
    * Everything in its own time.
    * Everything changes; everything stays the same.
    * Effort is important, but knowing where to make an effort makes all the difference!

[edit] F

    * Faint heart ne'er won fair lady.
          o Meaning: Not speaking up or taking action to achieve things (in this case, fall in love) will never get you anywhere (or, help you fall in love).
    * Failure is the stepping stone for success.
          o Failing will make you more determined to succeed the next time you try, or make you put in more effort to get something right with successive attempts.
    * Falling down does not signify failure but staying there does.
          o Letting failing/falling is not failure in itself, but letting it get you down or stop trying is.
    * Familiarity breeds contempt.
          o Long experience of someone or something can make one so aware of the faults as to be scornful.
    * Fifty percent of something is better than one hundred percent of nothing.
    * Fine feathers make fine birds.
    * Fine words butter no parsnips.
          o Alternative: Actions speak louder than words.
    * Fingers were invented before knives and forks.
    * First come, first served.
    * First deserve, then desire.
    * First things first.
          o Meaning: Do more important things before other things.
    * Fool me once, shame on you. Fool me twice, shame on me.
          o Meaning: To make the same mistake over again is your own fault.
    * Fools rush in where angels fear to tread.
          o Alexander Pope, "An Essay on Criticism"
    * For want of a nail the shoe is lost, for want of a shoe the horse is lost, for want of a horse the rider is lost.
          o Proverb reported by George Herbert, Jacula Prudentum (1651), #495
    * Forewarned is forearmed.
          o If one is told about an event beforehand, they can (adequately) prepare.
    * Forgive, but don't forget.
          o Let things/issues go or pass, but don't forget what they were, why they happened, other consequences, etc. Can lead to knowledge about not repeating the same mistake.
    * Fortune favours the brave.
    * Fretting cares make grey hairs.
    * From those to whom much is given, much is expected.
          o Biblical quote Luke 12:48
    * There are no facts; only interpretations of facts.
    * Failure is the first step to success.
    * Failure is not falling down, you fail when you don't get back up.
    * Fall down seven times, stand up eight.
          o Definition: Fail seven times, and succeed the eighth. (Keep trying and you will succeed.)
          o Translation of the Japanese proverb "Nana korobi ya oki", often associated with Daruma figurines.

[edit] G

    * Get four Episcopalians together and a fifth will always appear. (Humor intended!)
    * Go with the flow
    * Garbage in, Garbage out.
          o Sometimes abbreviated GIGO.
    * Give and take is fair play.
    * Give a dog a bad name and hang him.
    * Give a dog a bad name and he'll live up to it. (or repay you for it)
          o Implying that people live up to stereotypes given to them or that individuals are corrupted by the illtreatment that goes with being given a bad name
    * Give a man a fish and you feed him for a day; teach a man to fish and you feed him for a lifetime.
          o Knowledge is the best charity.
          o To learn a lesson is a far better reward than to receive a gift.
          o It is better to know how to help yourself than to beg from others.
    * Give credit where credit is due.
          o Variant: Give the Devil his due.
    * Give, and ye shall receive.
    * Give him an inch and he'll take a yard.
          o meaning: Once concessions have been made to someone they will demand a great deal more
          o Variant: Give the Camel and inch and it will take an ell.
          o Variant: Give him an inch and he'll take a mile.
    * Give people a common enemy and hopefully they will work together
    * God takes care of drunks.
    * God cures and the physician takes the fee.
    * God don't like ugly and he ain't stuck on pretty.
    * Good eating deserves good drinking.
    * Good fences make good neighbors.
          o Robert Frost, "Mending Wall"
    * Good men are hard to find.
    * Good wine needs no bush.
          o Meaning: Something desirable of quality and substance need not be embellished. It was customary since early times to hang a grapevine, ivy or other greenery over the door of a tavern or way stop to advertise the availability of drink within, once something establishes a good reputation for quality the advertisement is rendered superfluous.
    * Great cry little wool.
    * Great events cast their shadows before them.
    * Great minds think alike, but fools seldom differ.
          o Great minds think alike, as do lesser ones.
    * Great oaks from little acorns grow.
          o meaning: Wonderful things come from tiny things.
    * Great spirits have always encountered violent opposition from mediocre minds.
          o Albert Einstein
    * Green leaves and brown leaves fall from the same tree.
          o Many possible interpretations- Things change over time- If you are good at one aspect of a skill, you should be skilled at the other aspects, such as a painter who says he can't draw, yet both painting and drawing are aspects of art.- No matter of the outside, we are all the same inside.
    * Grow where you are planted.
    * Give respect, take respect.

[edit] H

    * Health is wealth
    * Home is where the heart is
    * Hell hath no fury like a woman scorned
    * Hindsight is always twenty-twenty
    * He laughs best who laughs last
    * He doesn't boast who does the most

[edit] I

    * I complained I had no shoes until I met a man who had no feet.
    * It takes both rain and sunshine to make rainbows
          o It takes good and bad to make good things in the future, or make them stand out.
    * I think, therefore I am
          o Descartes' most famous statement (Cogito Ergo Sum in Latin)
    * I came, I saw, I conquered
          o Said by Julius Caesar, spoken as Veni, Vidi, Vici during a message to the Roman senate
    * It is better to die on one's feet than live on one's knees.
    * I have the whole world against me, I show my back and the whole world is following me.
    * Idle hands are the devil's playthings. 'Alt.' The devil makes work for idle hands.
    * If a job is worth doing, it is worth doing well.
    * If a thing's worth doing, it's worth doing badly.
    * If all else fails, try the obvious.
    * If at first you don't succeed, try, try again.
    * If in doubt go left.
    * If in doubt, pick "C"
    * If it ain't broke, don't fix it.
          o Variation: If it isn't broken, don't fix it.
    * If it can't be cured, it must be endured.
          o From Midnight's Children by Salman Rushdie
    * If it's too good to be true, then it probably is.
    * If it's worth doing, it's worth over-doing.
    * If God had wanted man to fly, he would have given him wings.
    * If life gives you lemons, make lemonade.
    * If something can go wrong, it will.
          o Murphy's Law
    * If the shoe fits, wear it.
    * If the mountain won't come to Muhammad, Muhammad must go to the mountain.
          o "If the mountain won't come to Muhammad, Muhammad must go to the mountain", Answers.com
    * If wishes were fishes, we'd all cast nets.
    * If wishes were horses, beggars would ride.
    * If you buy quality, you only cry once.
    * If you buy cheaply, you pay dearly.
          o Alternatively: You get what you pay for
    * If you can't beat them, join them.
    * If you can't beat them, arrange to have them beaten.
    * If you can't be good, be good at it.
    * If you can't be good, be careful.
    * If you can't take the heat, get out of the kitchen.
    * If you cross your bridges before you come to them, you will have to pay the toll twice.
    * If you don't buy a ticket, you can't win the raffle.
    * If you don't have anything nice to say, don't say anything at all
    * If you don't know where you're going, any train will get you there.
    * If you fake it, you can't make it.
    * If you keep your mouth shut, you won't put your foot in it.
    * If you snooze you lose
    * If you trust before you try, you may repent before you die. - Divers Proverbs, Nathan Bailey, 1721 [5]
    * If you want a thing done right, do it yourself.
    * If you want breakfast in bed, sleep in the kitchen.
    * If you want to judge a man's character, give him power.
    * If you were born to be shot, you'll never be hanged.
    * If you're in a hole, stop digging.
    * If you're not part of the solution, you're part of the problem.
    * Ignorance is bliss.
          o Common mal-shortening of "Where ignorance is bliss, 'tis folly to be wise.
          o Thomas Gray, "Ode on a Distant Prospect of Eton College" [[6]]
    * In for a penny, in for a pound.
          o Alternate version: In for a dime, in for a dollar.
    * In order to get where you want to go, you first have to leave where you are.
          o From Sandy Elsberg's Bread Winner, Bread Baker; Upline Press, Charlottesville, VA; 1977, p. 80
    * In the land of the blind, the one-eyed man is king.
    * In the law there are no small cases, only small lawyers.
          o Ben Harlow
    * In the middle of difficulties lie opportunities -
    * In the mind of thieves the moon is always shining.
          o Marathi proverb, meaning: dishonest persons have to be always on the alert to avoid getting caught.
    * In the end, a man's motives are second to his accomplishments.
    * In one ear and out the other.
    * Infatuations are a plenty. Love is rare. - Pashi
    * Insanity is doing the same thing over and over, expecting different results.
          o Alternatively "Stupidity is doing the same thing over and over, and expecting different results"
    * Is the Pope a Catholic?
          o Do bears shit in the woods?
          o Used in response to what is considered to be a question with an extremely obvious answer.
    * It's always darkest before the dawn
    * It's better to want something you can't have than have something you don't want.
    * It's better to have something you don't need than to need something you don't have.
    * It's cheaper to keep her.
    * It's not over till it's over.
          o Yogi Berra
    * It ain't over till the fat lady sings.
          o Variation: Church ain't over until the fat lady sings.
          o Attributed as an old Southern saying in Smith & Smith, Southern Words and Sayings (1976), according to Quinion, Michael (21 August 1999). "It Ain't Over Till the Fat Lady Sings". World Wide Words. Retrieved on 2007-01-23.
          o Often attributed to sportscaster Dan Cook (1978)

    * It is not so much the gift that is given but the way in which the gift is driven.
    * It never rains, but it pours.
          o Alternatively: When it rains, it pours.
    * It pays to pay attention.
          o Rewards come to those who are attentive, or wary of events in the past/present/future.
    * It takes all sorts to make a world.
          o Alternatively: It takes all sorts to make the world go round.
          o Alternatively: It takes all kinds to make the world go round.
    * It takes two to make a quarrel.
          o Alternatively: It takes two to tango.
    * It takes two to tango.
    * It takes two to lie — one to lie and one to listen.
    * It's a cracked pitcher that goes longest to the well.
    * It's a good horse that never stumbles.
    * It's a long lane that has no turning.
    * It's an ill wind that blows no good.
    * It's a poor job that can't stand at least one supervisor.
    * It's a blessing in disguise.
    * It's better to be safe than sorry.
    * It's better to be silent and thought a fool, than to speak up and remove all doubt.
    * It's better to give than to receive.
    * It's better to have loved and lost than never to have loved at all.
    * It's easier to ask forgiveness than permission.
          o Attributed to Grace Hopper
    * It's easy to be wise after the event.
    * It's never too late to mend.
    * It's not the size of the boat, it's the motion of the ocean.
    * It's no use crying over spilt milk.
    * It's often a person's mouth broke their nose.
          o Meaning: People talk themselves into trouble.
    * It's the early bird that gets the worm.
    * It's the empty can that makes the most noise.
    * It's the squeaky wheel that gets the grease.
    * I wants, don't gets.
          o An alternative used in the black British community is: "Ask it, Ask it don't get... Get it, get it don't want."
    * "If you're prepared to be confused, be prepared for a sore bum"
    * He is the most Unfortunate who's today is not better than yesterday.
          o Attributed to Muhammed
    * If you fall off a cliff, you might as well try to fly. After all, you got nothing to lose.
    * If you love somebody, let them go, for if they return, they were always yours. And if they don't, they never were.
    * If you believe that dreams can come true be prepared for the occasional nightmare.
    * It is through the small things we do that we learn, not the big things
    * Impossible itself says I'm Possible
    * I was born on a Friday, but not last Friday.
          o Alternative: I wasn't born yesterday.

[edit] J

    * Justice pleaseth few in their own house.
          o Meaning: No one ever blames themselves for anything.
    * Joan is as good as my lady in the dark. (17th Century)
    * Jack of all trades and master of none. (18th Century)
          o Literal meaning: Anyone who's good at everything is not a master of anything.
    * Justice delayed is justice denied.(Legal Proverb, India)

[edit] K

    * Keep your mouth shut and your eyes open. (18th Century)
    * Knaves and fools divide the world.
    * Knowledge is power. (17th Century)
    * Kindness, like grain, increase by sowing.
    * Keep some till more come.
          o Interpretation: Save something until the next stock comes.
    * Knowledge creates mysteries.

[edit] L

    * Laugh and the world laughs with you .. Cry and you will find no one with tears.
    * Laugh when you're happy, cry when you're sad, and do both when you're the happiest you've ever been.
    * Laughter is the best medicine for them who do not know how to laugh.
    * Laughter is the shortest distance between two people.
    * Law is the solemn expression of legislative will.
    * Lead to Success, Follow to Failure
    * Learn to walk before you run.
          o Possible interpretation: Do not rush into what you do not know.
          o Alt. interpretation: Learn the basics before you start using more complex tools or methods
    * Least said sooner mended.
          o meaning: those who speak less get more done
          o Alt. Interpretation: dwelling on the problem/blame makes it worse and delays the ability to get on and fix things
    * Leave it alone and it will grow on its own.
    * Let him who is without sin cast the first stone.
          o Jesus Christ
    * Let sleeping dogs lie.
          o Agatha Christie's Sleeping Murder[citation needed]
    * Let the cobbler stick to his last.
    * Lie down with dogs, wake up with fleas.
          o meaning: When you get revenge, you will be punished in some way or other
    * Life begins at forty.
    * Life does not come with any guarantees
    * Life imitates art
    * Life is a perception of your own reality.
    * Life is like a box of chocolate, you never know what you're gonna get
    * Life is too short to drink bad wine.
    * Life is just a bowl of cherries.
    * Life is what you do while you're waiting to die.
          o Quote from song sung by Zorba from the musical 'Zorba' by Kander and Ebb
    * Life is what you make of it.
          o meaning: Nothing's going to change unless you do something about it
    * Life is what happens to you while you're busy making other plans.
          o Attributed to John Lennon
    * Let us go hand in hand,not one before another.
    * Lightning never strikes twice in the same place.
    * Like cures like.
          o Meaning: A person can better help another if they have something in common.
    * Like father, like son.
    * Like water off a duck's back.
    * Little bean comes around his little salary
    * Little by little and bit by bit.
          o Meaning: Do things slowly and carefully
    * Little enemies and little wounds must not be despised.
    * Live and let live.
          o Alternative: Live simply to let others simply live.
    * Give a man a match, he shall be warm for a moment. Light a man on fire and he shall be warm for the rest of his life.
    * Long absent, soon forgotten.
    * Look after the pennies and the pounds will look after themselves.
          o Possible interpretation: Take care of the details. (12 pence to the shilling, 20 shillings to the pound.)
          o Alt. interpretation: Save every penny you can and it will build up into a significant amount of money.
    * Look before you leap.
    * Look on the sunny side of life.
    * Loose lips sink ships.
          o World Wartime mantra encouraging people to avoid talking about things which could have been overheard by spies
    * Love is a bridge between two hearts.
    * Love is anger disappointed.
    * Love is blind.
    * Love is not finding someone to live with; it's finding someone whom you can't live without.
    * Love laughs at locksmiths.
    * Love is like war, Easy to start, Hard to end, Impossible to forget.
    * Life's battle don't always go to the stronger or faster man, but sooner or later the man who wins is the one who thinks he can.
    * Love is blind,… but marriage is the real eye-opener.
    * Luck is a mirror of hard work - Beslin

[edit] M

    * Make hay while the sun shines.
    * Make a Friend when you don't need One (from Urim)
          o Possible interpretations: Do the task while it is possible. Don't wait until you need help to ask for it.
    * Making a rod for your own back.
    * Creating the thing with which you will be beaten.
    * Man is truly himself when he's alone.
    * Man wasn't born to suffer but to carry on.
    * Manners maketh the man.
          o From 'Manners makyth man' - the motto of William of Wykeham(1320 - 1404)
    * Many a true word is spoken in jest
    * Many hands make light work
    * Many things are lost for want of asking.
    * Many words will not fill a bushel.
          o This Proverb is a severe Taunt upon much Talking. - Divers Proverbs, Nathan Bailey, 1721 [7]
    * Marriage equals hell and bankruptcy.
    * Meaning of life is not meaningful -- Allen Zimama
    * Meaner than a junk-yard dog.
    * Measure twice, cut once.
    * Mind your P's and Q's.
          o British: Mind your manners (origin theories)
    * Mirrors do everything we do, but they cannot think for themselves.
    * Misery loves company.
    * Missing the wood for the trees.
          o Overlooking the more important issue.
    * Money cannot buy happiness.
    * Money for old rope.
          o In the days of wooden-hulled sailing ships, ropes that were worn could be sold for use as caulking (pressed between the planks and often covered with tar to prevent seepage), or as filling for fenders, and so the ship's owner was paid even for old rope.
    * Money makes the mare go.
    * Money makes the world go around.
    * Money talks.
          o Variant: Money talks, bullshit walks.
          o Related: Talk is cheap.
          o Related: Actions speak louder than words.
          o meaning: It's easy to say you believe something, but people are more likely to risk cash or possessions on something they truly believe.
          o meaning: its time to stop living in the fantasy world, and live in the real world.
    * (love of)Money is the root of all evil.
    * Morals are for others to follow.
          o More money only causes more problems
    * Money can't buy everything, but everything needs money
    * Money talks; mine always says, "Good-bye!"
    * Monkey see, monkey do.
    * More haste, less speed.
          o More haste at a task will lead to the task being completed less speedily. As with many English proverbs, it describes consequences rather than giving an order.

[edit] N

    * Nature never did betray the heart. that loved her.
    * Nature, time, and patience are three great physicians.
    * Necessity is the mother of all invention.
    * Ne'er cast a clout till May be out. (Not known if 'May' relates to the month of May or may blossom).
          o Don't remove winter vests (undergarments) until summer arrives.
    * Never change, for the sake of others. There will be no one like you if you change. (GPL)
    * Never judge the book by its cover.
    * Never put off till (until) tomorrow what you can do today.
    * Never let the right hand know what the left hand is doing.
          o Possible interpretation: Do not boast in giving to the poor- anonymous is best.
          o Possible interpretation: Secrecy insures security
    * Never leave a woman to do a man's work.
          o alternate version, Never let a monkey to do a man's job, Never send a woman to do a man's job
          o Meaning: Leaving\employing someone less qualified to do your work will produce undesired results.
    * Never let a man do a woman's job.
          o Feminist phrase; Men are poorer than women, skill-wise.
    * Never lie to your doctor.
    * Never lie to your lawyer.
    * Never look a gift horse in the mouth.
    * Never say die.
          o interpretation: Never give up.
    * Never say never.
    * Never smash a glass over a brick donkey.
    * Never trouble trouble 'til trouble troubles you.
    * Noblesse oblige.
          o French expression: To be a member of the nobility carries obligations to care for the lower classes.
    * No man can serve two masters.
          o Christian New Testament
    * No man is content with his lot.
    * No man is an island
          o interpretation: Everybody needs other people.
    * No money, no justice.
    * No news is good news.
    * No need to cry over spilled milk.
    * No pain, no gain.
    * No time like the present.
    * Nobody leaves us, we only leave others.
    * Not enough room to swing a cat
    * Nothing ventured, nothing gained.
          o Variant: Nothing ventured, nothing have. - Divers Proverbs, Nathan Bailey, 1721 [8]
    * Nothing exceeds like excess.
    * Nothing to be feared in life, but understood.
    * Now the shit has really hit the fan.
    * Now we have doors so we can hide.

[edit] O

    * Once bitten, twice shy
          o William Caxton, the first English printer, gave the earliest version of this saying in 'Aesope' (1484), his translation of Aesop's fables: 'He that hath ben ones begyled by somme other ought to kepe hym wel fro(m) the same.' Centuries later, the English novelist Robert Surtees referred to the saying in 'Mr. Sponge's Sporting Tour' (1853) with '(He) had been bit once, and he was not going to give Mr. Sponge a second chance.' The exact wording of the saying was recorded later that century in 'Folk Phrases of Four Counties' (1894) by G.G. Northall and was repeated by, among others, the English novelist Joseph Conrad (1920, 'The Rescue'), the novelist Aldous Huxley (1928, 'Point Counter Point'), and the novelist Wyndham Lewis (1930, 'The Apes of God'). 'Once bitten, twice shy' has been a familiar saying in the twentieth century. From Wise Words and Wives' Tales by Stuart Flexner and Doris Flexner (Avon Books, New York, 1993).
          o A variation, once burned, twice shy, is also traced back to Mr. Sponge's Sporting Tour. Once burned was First attested in the United States in 'Dead Sure' (1949) by S. Sterling. The meaning of the saying is One who had an unpleasant experience is especially cautious. From the Random House Dictionary of Popular Proverbs and Sayings by Gregory Y. Titelman (Random House, New York, 1996).
    * Old is Gold
    * Once in a lifetime comes often, so be prepared.
    * One good turn deserves another. - Divers Proverbs, Nathan Bailey, 1721 [9]
          o Meaning: You should return a favour done to you.
    * One grain of sand can tip the scale.
          o Meaning: Any advantage, no matter how slight, can turn a hopeless situation into a fighting chance if used properly.
    * One hand washes the other. From the Latin MANUS MANAM LAVAT, meaning "Hand washes hand," or "One hand washes the other"; or impliedly, "You scratch my back, and I'll scratch yours."
    * One man's junk is another man's treasure.
    * One man's meat is another man's poison.
          o Meaning: What is liked by one person is disliked by another.
    * One man's terrorist is another man's freedom fighter. - Ronald Reagan
    * One might as well be hanged for a sheep as a lamb. - English, 17th century
    * One murder makes a villain, millions a hero.
    * One rotten apple will spoil the whole barrel.
          o Meaning: Corruption must be rooted out or else it will spread.
          o Cf. Dan Michael of Northgate, Ayenbite of Inwyt (1340): "A rotten apple will spoil a great many sound ones." (Middle English: "A roted eppel amang þe holen: makeþ rotie þe yzounde.")
    * One scabbed sheep mars the whole flock.
          o This Proverb is apply'd to such Persons who being vicious themselves,
            labour to debauch those with whom they converse. - Divers Proverbs, Nathan Bailey, 1721 [10]
    * One swallow doesn't make a summer.
    * Only a coward will write an anonymous letter. -President Franklin D. Roosevelt
    * Only bad drivers cut corners.
    * Only losers say "Winning isn't everything."
    * Only the good die young
    * Opinions are like assholes: everyone has them and they usually stink.
    * Opportunity is waiting you need but to open the door.
    * Opportunity knocks only once.
          o Meaning: Do not waste time while grabbing opportunities.
    * Our greatest glory is not in never falling but in rising every time we fall.
          o Confucius
    * Our costliest expenditure is time. <Theophrastus>
    * Out of sight... Out of mind
    * Out of small acorns grow mighty oaks.
    * Owt for Nowt
          o Northern English, Anything for nothing...

On your feet loose your seat.
[edit] P

    * Paddle your own canoe.
    * Pain is only weakness leaving the body.
          o U.S. Marines proverb
    * Patience is a virtue.
    * Penny wise, pound foolish.
    * (The) pen is mightier than the sword.

    * People who live in glass houses shouldn't throw stones.
          o Variation: Whose house is of glasse, must not throw stones at another.
                + George Herbert, Outlandish Proverbs, 1640; cited in "Proverbs 120". The Yale Book of Quotations. 2006. pp. p. 613. ISBN 0-300-10798-6. *** George Herbert, Jacula Prudentum, 1651, number 196
          o Meaning: Don't criticize other people when you yourself have faults and weaknesses.
    * Perfect Planning Prevents Piss Poor Performance. (a.k.a The six P's)
    * POETIS MENTIRI LICET. - Latin for "Poets are allowed to lie." Has to do with rhetoric (hyperbole) and poetic and/or litarary license.
    * Persistance becomes Reality.
    * Please don’t retouch my wrinkles. It took me so long to earn them.
    * Politics makes strange bedfellows.
    * Power tends to corrupt; absolute power corrupts absolutely.
          o Attributed to Lord Acton
    * Practice before you preach.
          o Meaning: Before asking others to do something, make sure you are following it yourself.
    * Practice makes perfect.
    * Practice may make perfect, but nobody's perfect so why practice
    * Practice doesn’t make a man perfect, but a perfect practice makes a man perfect
    * Pride comes before a fall
    * Prior preparation prevents poor performance.
    * Put it in song, put it in drink; but never, ever put it in ink!
          o Reportedly said by Earl K. Long, Governor of Louisiana
    * Put a beggar on horseback and he'll ride to the devil.
    * Put a beggar on horseback and he'll ride it to death.
    * Put a cat amongst the pigeons.
    * Prevention is better than cure.
          o Variation: An ounce of prevention is worth a pound of cure.
    * Procrastination is the thief of time.
    * Proverbs are long life experiences, told in one short sentence.
    * Proverbs run in pairs.
          o Meaning: Every proverb seems to be contradicted by another proverb with an opposed message, such as "too many cooks spoil the broth" and "many hands make light work."
    * Politeness cost nothing and gains everything. <M.W. Montagu>
    * Peace Sells, but who's Buying? <Megadeth>

[edit] R

    * REPETITIO MATER MEMORIAE - Latin for "Repetition is the mother of memory."
    * Revenge is a dish best served cold.
    * Rules were meant to be broken.
    * Rome wasn't built in a day
    * Rolling stones gather no moss
    * Robbing Peter to pay Paul
    * Rather be a dog in peace, than to be a man in chaos.-Chinese Origin-宁为太平犬，莫为乱世人
          o Rather be a dog in a peaceful land, instead of being a man in a land of war.
    * Reality is often stranger than fiction

[edit] S

    * Seek water in the sea.
    * Someone who gossips to you will gossip about you.
    * Same meat, different gravy.
          o Variation on the above.
    * Same trouble, different day.
    * Same shit, with different flies on it.
    * Seek and ye shall find.
          o Christian New Testament
    * Say something nice or say nothing at all.
    * Self trust is the first secret of success.
    * Sell a man a fish, he eats for a day, teach a man how to fish, you ruin a wonderful business opportunity.
          o Karl Marx
    * Set a thief to catch a thief.
    * Shallow graves for shallow people.
    * Ships happen. -Navy saying.
    * Shit or get off the pot
    * silence is golden
          o Meaning: sometimes it is better not to say anything.
    * Simple minds think alike. (William Truong)
          o Simple things please/amuse simple minds.
          o Alternative: Simple minds, simple pleasures.
    * Six of one, and half a dozen the other.
          o Meaning: Describes two actions with the same result, or two things that are essentially the same.
    * Slow and steady wins the race.
          o Variant: Slow but sure.
    * Smile, and the world smiles with you; cry, and you cry alone.
    * Something worth doing is worth doing well.
    * So close, yet so far.
    * S tart small; T hink tall; R each over the wall; I nvest your all; V isualize the mall; E xpect you may fall; but, if you fall, that's not all; get up and STRIVE again.
          o Dr. Robert Schuller
    * Stolen fruit is the sweetest.
          o Possible interpretation: forbidden things are the most tempting
          o The Bible
    * Sticks and stones may break my bones but words will never hurt me.
          o Contrast: "A blow with a word strikes deeper than a blow with a sword."
    * Still waters run deep.
          o Possible interpretation: Looks can be deceiving, quiet people are often the most deep.
    * Strike while the iron is hot.
          o Possible interpretation: Seize the moment. Take the opportunity now; don't waste it.
    * Success is a journey not a destination.
    * Sufficient unto the day is the evil thereof.
    * Some days you get the bear, other days the bear gets you.
    * Spare the rod, spoil the child.
          o Meaning: Lack of deserved discipline develops undesired behavior in a child.
    * Success grows out of struggles to overcome difficulties.
    * Straightn not the dog's tail even in the bamboo hollow.

[edit] T

    *
          o You can talk easily without waiting for something or someone
    * Talk of the devil and he's sure to appear.
    * Talk the hind legs off a donkey.
    * The rotten apple injures its neighbors.
          o Possible interpretation: Someone who never shuts up - often used in reference to London cab drivers
    * Talking a mile a minute.
    * Talking nineteen to the dozen.
    * Take an old dirty, hungry, mangy, sick and wet dog and feed him and wash him and nurse him back to health, and he will never turn on you and bite you. This is how man and dog differ.
          o (Possibly Lord Byron)
    * Taking care of business.
    * Take care of the pennies and the pounds will take care of themselves.
    * Take it with a grain of salt.
          o Meaning: Regard it with a copious measure scepticism.
          o (See Wikipedia article.)
    * That which does not kill you, makes you stronger.
          o Friedrich Nietzsche, The Twilight of the Idols (1888)
    * The acorn (apple) never falls far from the tree.
          o Meaning: People are similar to their parents/their roots.
    * The ball is in your court.
          o Meaning: It's up to you to decide.
    * The belly has no ears.
          o This Proverb intimates, that there is no arguing the Matter with Hunger,
            the Mother of Impatience and Anger. - Divers Proverbs, Nathan Bailey, 1721 [11]
          o I don't argue with the body Jerry. It's an argument you can't win. - Kramer
    * The best is yet to come.
    * The best of friends need not speak face to face.
    * The best things come in small packages.
    * The best things in life are free.
    * The calm (comes) before the storm.
    * The child is father to the man.
          o Meaning: What is true of a child will still be true when it grows up; or, early experiences shape future character.
    * the child is father of the man
    * The coat makes the man.
    * The cure is worse than the disease.
    * The money is burning a hole in my pocket.
    * The customer is always right.
    * The difference between a man and a cat or a dog is that only a man can write the names of the cat and the dog.
    * The early bird gets (or catches) the worm.
    * The end justifies the means.
    * The English are a nation of shopkeepers
          o (Attributed to Napoleon)
    * The exception proves the rule.
          o Often mistakenly referred to as a misquote. In reality, the Latin probate may mean either to probe or to prove. The key is that prove in this case carries the older meaning of to test, as in the phrases proving (testing) ground or the proof (test) of the pudding is in the eating.
    * The first step to health is to know that we are sick.
    * The grass is always greener on the other side...
          o Meaning: You will always want what you don't (or can't) have.
    * The enemy of my enemy is my friend.
    * The greatest thing that could happen in my lifetime is for all my ideas to be stolen.
    * The head and feet keep warm, the rest will take no harm.
    * The key to all action lies in belief.
    * The law is a jealous mistress.
          o - Professor Ferdinand Fairfax Stone, Tulane Law School, early and mid 1960s.
    * The longest mile is the last mile home.
    * The more you know, the more you know you don't know.
    * The more things change, the more they stay the same.
          o From the French: Plus ça change, c'est la même chose.
    * The nail that sticks up will be hammered down.
    * The only free cheese is in the mouse trap.
          o Russian saying.
    * The only stupid question is the one that is not asked.
    * The only thing you get from picking bottoms (ie. of the stock market) is a smelly finger.
    * The pain o the little finger is felt by the entire body.
    * The pen is mightier than the sword.
    * The pitcher which goes too often to the well gets broken.
    * The proof of the pudding is in the eating.
    * The way to a man's heart is through his stomach.
    * The world is your oyster.
    * The weak can never forgive. Forgiveness is the attribute of the strong.
          o Attributed to Mahatma Gandhi
    * The whole is greater than its parts.
          o Sometimes, jocularly from above: The proof of the eating is in the size of the pudding.
    * There are no endings: only new beginnings.
    * There are no small parts, only small actors.
    * There are so many things to say that are better left unsaid.
    * There are three types of lies - lies, damned lies, and statistics.
    * There is no god except God.
    * There is no point of knowledge or wisdom if not dotted.
    * There is only eight years between success and failure in politics.
          o Jim Brown, Louisiana statesman
    * There is something rotten in the state of Denmark.
          o or There's something rotten in Denmark.
          o Expresses strong suspicion.
          o Shakespeare's Hamlet (Marcellus in act 1, scene 4).
    * There's always a calm before a storm.
          o or The calm before the storm.
    * There's a method in his madness.
    * There's many a slip 'twixt cup and lip.
          o This comes from a Greek legend, as follows: One of the Argonauts returned from his voyage, and went home to his winery. He called for the local soothsayer, who had predicted before his voyage that he would die before he tasted another drop of his wine, from his vinery. As he finished saying this, he raised a cup filled with wine to his lips, in toast to the soothsayer, who said something in reply. Just then, he was called away to hunt a wild boar that was approaching, and died in his attempt to kill it. The phrase that the soothsayer said is translated best as, There's many a slip 'twixt the cup and the lip.
    * There's money in muck.
          o or Where there's muck there's brass.
    * There's more than one way to skin a cat.
    * There's no accounting for taste.
          o From the Latin: De gustibus non est disputandum.
    * There's no arguing with the barrel of a gun.
    * There's no peace for the wicked
    * There's no place like home.
    * There's no such thing as a free lunch.
    * There's no time like the present.
    * There's no point in washing clean things.
          o Meaning: Don't fix things that are fine, just the bad things!
    * The road to hell is paved with good intentions.
          o Earlier variants of this proverb are recorded as Hell is paved with good intentions. recorded as early as 1670, and an even earlier variant by Saint Bernard of Clairvaux Hell is full of good intentions or desires.
          o Similar from Latin: "The gates of hell are open night and day; Smooth the descent, and easy is the way" — Virgil, the Aeneid Book VI line 126
    * The spirit is willing but the flesh is weak.
          o Gospel of Matthew 26:41
    * The squeaky wheel gets the grease.
          o or The squeaky wheel gets replaced.
          o If you speak up, you will go farther in life.
          o Those who complain, will attract more attention (for good or ill) than those who are content.
    * The start of a journey should never be mistaken for success.
    * The straw that broke the camel's back.
          o The last of a number of little things which led to something major.
    * The teacher has not taught, until the student has learned.
    * The truth is in the wine.
          o Possible interpretation: A person will more freely divulge a secret when plied with alcohol.
          o A drunken man's words are a sober man's thoughts.
    * The truth shall set you free, or The truth will set you free.
          o In the Bible, John 8:32.
    * The value is determined by the agreement of two people.
    * The wish is father to the thought.
    * The worst good day is always better than the best bad day.
    * The younger brother the better gentleman.
          o Divers Proverbs, Nathan Bailey, 1721 [12]
    * Think before you speak.
    * Thinking the worst always prepares you for the worst.
    * Those who live in glass houses shouldn't throw stones.
    * Those who run with pigs, smell like pigs.
    * Time flies.
          o Latin: Tempus fugit!
    * Time and tide wait for none.
    * Time is gold.
    * Tit for Tat.
    * This, too, shall pass.
    * To burn the candle at both ends.
    * To put something in a new jacket.
    * To each, his own.
    * To have the fulfilled life, you must question the unanswerable and learn nothing.
          o Meaning: you must build your own opinions, but life is too short to waste trying to understand life
    * To err is human; to forgive, divine. (Pope, Essay on Criticism)
    * To know the road ahead ask those coming back.
    * Tomorrow is another day.
    * Too much of one thing, good for nothing.
          o Meaning: Don't overspecialize
    * Too many Chiefs and not enough Indians.
    * Too many cooks spoil the broth.
    * Trapped between a rock and a hard place.
    * Tread on a worm and it will turn.
          o This Proverb is generally used by Persons who have received gross insults and
            Injuries from others (which they have for some time bore with Patience) to excuse their
            being at last transported to some Warmth of Resentment and Passion. - Divers Proverbs, Nathan Bailey, 1721 [13]
    * Trouble shared is trouble halved.
    * Truth is stranger than fiction.
    * Truth will out.
          o Meaning: The truth will eventually come out, no matter how well it is hidden.
    * Try not to become a man of success but a man of value.
    * Try try but don't cry.
          o Meaning: Never give up in life.
          o Try and try until you suceed
    * Two's company; three's a crowd.
    * Two heads are better than one.
    * Two things prolong your life: A quiet heart and a loving wife.
    * Two wrongs don't make a right.
          o Also jocularly formed from above: Two wrongs don't make a right - but three rights make a left.
    * The greatest pleasure in life is doing what people say you cannot do..
    * The worst way to miss someone is to be sitting right beside them knowing you can't have them.
    * The more you study, the more you know. The more you know, the more you forget. The more you forget, the less you know. The less you know the more you study.
    * The whole dignity of man lies in the power of thought.
          o - B. Pascal
    * There is a thin line between love and hate
    * The dog is nude though the clothing cost a penny.
    * Something is better than nothing.

[edit] U

    * Unprepare to prepare, be prepared to be unprepared
          o supposedly said by W.B.Govo in 1916
    * Use it or lose it

[edit] V

    * Variety is the spice of life.
          o An early version is found in William Cowper, The Task (1785), Book II, "The Timepiece", lines 606–7:
                + Variety's the very spice of life,
                  That gives it all its flavour.

    * Virtue which parleys is near a surrender. - Divers Proverbs, Nathan Bailey, 1721 [14]
    * Vision without action is a daydream. Action without vision is a nightmare. (Japanese proverb)
    * Vengeance is mine, thus sayeth the Lord.

[edit] W

    * Walk the walk and talk the talk.
    * Walk softly, carry a big stick.
          o Variant of an African proverb that was made famous in the U.S. by Teddy Roosevelt, "Speak softly and carry a big stick; you will go far".
    * When a thing is done advice comes too late.
    * When you lie on roses while young, you'll lie on thorns while you're old.
    * Waste not, want not.
    * When the going gets tough, the tough get going.
    * We are all on this earth, we can't get off so get on.
    * We can't always build the future for our youth, but we can build our youth for the future.
          o By: Franklin D. Roosevelt
    * We have nothing to fear but fear itself.
          o By: Franklin D. Roosevelt
    * We must take the bad with the good.
          o Variant: We must take the bitter with the sweet.
    * Well begun is half done.
          o Variant: Well begun is half ended. - Divers Proverbs, Nathan Bailey, 1721 [15]
    * "Well done" is better than "well said".
    * We tend to be perfect. That’s why when we make mistakes we are hard on ourselves.
    * We've qualified for the World Cup, Go and compete.
    * What a tangled web we weave, when first we practice to deceive. (A lie will always spawn a bigger lie.)
    * What goes around comes around.
          o You will eventually have to face the consequences of your actions towards others as people tend to behave towards you as you have behaved towards others.
    * What goes up must come down.
    * What you see is what you get.
    * What you sow is what you reap.
    * What's good for the goose is good for the gander.
    * When in Rome, do as the Romans do.
    * When one door closes, another door opens.
    * When the cat is away, the mice will play.
    * Where ignorance is bliss, 'tis folly to be wise. [[16]]
          o Thomas Gray, "Ode on a Distant Prospect of Eton College"
    * Where there's a will, there's a way.
    * Where vice goes before, vengeance follows after. - Divers Proverbs, Nathan Bailey, 1721 [17]
    * Willful waste makes woeful want.
    * Winning isn't everything... It's the only thing.
    * Winning is earning. Losing is learning.
    * We ourselves feel that what we are doing is just a drop in the ocean,but the ocean would be less without that drop.
    * Women need men like a fish needs a bicycle.
    * Words uttered only causes confusion. Words written only causes history.
    * Working hard or hardly working?
    * Worship the Creator not His creation.
    * Write injuries in the sand, kindnesses in marble.
    * Whom thy care to tamper pots in an abandoned house
Aboriginal people have lived in Australia for tens of thousands of years. During that time, oral history, some aspects dating from extreme antiquity, was passed down through the generations in the form of spoken allegories, myths, and songs.

While there was a long established European tradition of a Great South Land, the written history of Australia began in 1606, when during a voyage of discovery from Bantam, Willem Janszoon, commanding the Duyfken encountered the Australian mainland.
Contents
[hide]

    * 1 Aboriginal Australians
          o 1.1 Aborigines before 1788
          o 1.2 Peaceful settlement or brutal conquest after 1788?
    * 2 European exploration
    * 3 British settlement and colonisation
          o 3.1 Plans for Colonisation
          o 3.2 British Settlements in Australia
          o 3.3 Convicts and Colonial Society
    * 4 Colonial self-government and the discovery of gold
    * 5 Growth of Nationalism and Federation
    * 6 A New Nation for the 20th Century
          o 6.1 Immigration and Defence concerns
          o 6.2 Dominion Status
          o 6.3 The Emergence of party politics and competing visions of Australia
    * 7 To the last man and the last shilling; The First World War
    * 8 Men, money and markets: The 1920s
    * 9 The Depression Decade: The 1930s
    * 10 The Second World War
          o 10.1 Defence Policy in the 30s
          o 10.2 The War
          o 10.3 Australia during the war
    * 11 Australia Post World War 2
    * 12 See also
    * 13 Notes
    * 14 References
    * 15 External links

[edit] Aboriginal Australians
Main article: History of Indigenous Australians
See also: Prehistory of Australia
See also: Aboriginal History of Western Australia
[edit] Aborigines before 1788

The consensus among scholars for the arrival of humans of Australia is placed at 40,000 to 50,000 years ago, but possibly as early as 70,000 years ago.[1][2] The earliest human remains found to date are those found at Lake Mungo, a dry lake in the south west of New South Wales. These have been dated at about 40,000 years old.[3] At the time of first European contact, it has been estimated the population of Australian Aborigines was at least 350,000,[4][5] while recent archaeological finds suggest that a population of 750,000 could have been sustained.[6][7] The ancestors of the Aborigines appear to have arrived by sea during one of the earth’s periods of glaciation, when New Guinea and Tasmania were joined to the continent. The journey still required sea travel however, making them amongst the world’s earlier mariners.[8]

By 1788, the population existed as 250 individual nations, many of which were in alliance with one another, and within each nation there existed several clans, from as few as five or six to as many as 30 or 40. Each nation had its own language and a few had multiple, thus over 250 languages existed, around 200 of which are now extinct. “Intricate kinship rules ordered the social relations of the people and diplomatic messengers and meeting rituals smoothed relations between groups,” keeping group fighting, sorcery and domestic disputes to a minimum.[9]

The mode of life and material cultures varied greatly from nation to nation. Some early European observers like William Dampier described the hunter-gatherer lifestyle of the Aborigines as arduous and “miserable”. In fact, as historians like Geoffrey Blainey argue, the material standard of living for Aborigines was generally high, higher than that of many Europeans living at the time of the Dutch discovery of Australia.[10] Astute nineteenth century settlers like Edward Curr also observed that Aborigines “suffered less and enjoyed life more than the majority of civilized(sic) men.”[11] In south eastern Australia, near present day Lake Condah, semi-permanent villages of beehive shaped shelters of stone developed, near bountiful food supplies.[12] For centuries, Macassan trade flourished with Aborigines on Australia's north coast, particularly with the Yolngu people of north-east Arnhem Land.

The greatest population density was to be found in the southern and eastern regions of the continent, the River Murray valley in particular. Aborigines lived and utilised resources on the continent sustainably, agreeing to cease hunting and gathering at particular times to give populations and resources the chance to replenish. “Firestick farming” amongst northern Australian people was used to encourage plant growth that attracted animals.[13] Aborigines were amongst the oldest, most sustainable and most isolated cultures on Earth prior to European settlement beginning in 1788.

However, life for Aborigines was not without significant changes. Some 10-12,000 years ago, Tasmania became isolated from the mainland, and some stone technologies failed to reach the Tasmanian people(such as the hafting of stone tools and the use of the Boomerang).[14] The land was not always kind; Aboriginal people of south eastern Australia endured “more than a dozen volcanic eruptions…(including) Mount Gambier, a mere 1,400 years ago.” [15] There is evidence that when necessary, Aborigines could keep control of their population growth and in times of drought or arid areas were able to maintain reliable water supplies.

Some of the Pintupi people successfully lived their traditional lifestyle in the Gibson Desert long after the arrival of Europeans. The last group did not meet modern Australia until 1984.[16]

    “ 	When Warlimpirrnga Tjapaltjarri first saw a European he said "I couldn't believe it. I thought he was the devil, a bad spirit and was the colour of clouds at sunrise.[16] 	”

[edit] Peaceful settlement or brutal conquest after 1788?
Poster issued in Van Diemen's Land in 1816 prior to the height of the Black War depicting Lieutenant-Governor Arthur's policy of friendship and equal justice for settlers and Aborigines[17]
Main article: Australian frontier wars

Australian Historian Henry Reynolds argues that there was a "historical neglect" of the Aborigines by historians until the late 1960s.[18] Early commentaries on the Australian colonies often tended to describe Aborigines as doomed to extinction following the arrival of Europeans. For example, William Westgarth’s 1864 book on the colony of Victoria observed; "the case of the Aborigines of Victoria confirms …it would seem almost an immutable law of nature that such inferior dark races should disappear."[19]

In 1968 Anthropologist W.E.H Stanner described the lack of historical accounts of relations between Europeans and Aborigines as "the great Australian silence." [20] It was partly a play on the title of Douglas Pike’s 1962 book "The Quiet Continent," which argued Australian history was largely peaceful.[21] However, by the early 1970s historians like Lyndall Ryan, Henry Reynolds and Raymond Evans were trying to document and estimate the conflict and human toll on the frontier.

It is now accepted by many academics that the impact of the arrival of Europeans was profoundly disruptive to Aboriginal life and there was considerable conflict on the frontier. Even before the arrival of settlers in all parts of Australia, European disease had preceded them. "In 1789, the second year of European settlement…a smallpox epidemic wiped out about half the Aborigines around Sydney." It then spread well beyond the then limits of European settlement, including much of south eastern Australia, reappearing in 1829-1830, killing 40-60% of the Aboriginal population.[22]

At the same time, some settlers were quite aware they were usurping the Aborigines place in Australia. In 1845, settler Charles Griffiths sought to justify this, writing; "The question comes to this; which has the better right – the savage, born in a country, which he runs over but can scarcely be said to occupy…or the civilized man, who comes to introduce into this…unproductive country, the industry which supports life." [23] In expressing this view, Griffiths was probably merely echoing opinions widely held by other colonists in Australia, South Africa, parts of South America and the United States.

The story of Aboriginal-settler conflict has been described by modern historians in numerous ways, from "lamentable" [24] to "disastrous." [25] There are many events that illustrate the extent of the violence and resistance as Aborigines sought to protect their lands from invasion and as settlers and pastoralists attempted to establish their presence and protect their investments. In May 1804, at Risdon Cove, Van Diemen's Land,[26] perhaps 60 Aborigines were killed when they approached the town.[27] From the mid 1820s until the early 1830s, the Black War raged in Van Diemen's Land. In 1838, at least twenty-eight Aborigines were murdered at the Myall Creek in New South Wales. The seven convict settlers responsible for this massacre were hanged.[28] Aborigines were far from helpless however, for example in April 1838 fourteen Europeans were killed at Broken River in Port Phillip District, by Aborigines of the Ovens River, almost certainly in revenge for the illicit use of Aboriginal women.[29] The most recent massacre of Aborigines was at Coniston in the Northern Territory in 1928. There are numerous other massacre sites in Australia, although supporting documentation varies.

That a murderous intent existed amongst many European settlers there appears little doubt. Captain Hutton of Port Phillip District once told Chief Protector of Aborigines George Augustus Robinson that "if a member of a tribe offend, destroy the whole." [30] Queensland’s Colonial Secretary A.H. Palmer wrote in 1884 "the nature of the blacks was so treacherous that they were only guided by fear – in fact it was only possible to rule…the Australian Aboriginal…by brute force" [31]

The removal of indigenous children, which the Human Rights and Equal Opportunity Commission argue constituted attempted genocide,[32] had a major impact on the Indigenous population.[33] Such interpretations of Aboriginal history are disputed by Keith Windschuttle as being exaggerated or fabricated for political or ideological reasons.[34] This debate is part of what is known within Australia as the History Wars.

Indigenous Australians were given the right to vote in Commonwealth elections in Australia in November 1962, and in Western Australian state elections in the same year. Aborigines in Queensland were given the vote in state elections in 1965. There were never any racial qualifications to vote in the other four states. The 1967 federal referendum removed references to Aborigines from the Australian constitution, and prevented states from excluding Aborigines when the country does a count to determine electoral representation. The referendum passed with a 90.2% majority, the largest affirmative vote in the history of Australia's referendums. The first Indigenous Australian to serve in the Australian Parliament was Neville Bonner, who took up a Senate place in 1971.

On 13 February 2008, Prime Minister Kevin Rudd formally apologised to the Aborigines of the stolen generation. However, the interpretation of the history of Australia is currently a matter of contention, particularly regarding the British settlement and the early treatment of Indigenous Australians.[citation needed]
[edit] European exploration
Main article: European exploration of Australia

Willem Janszoon is credited with the first authenticated European discovery of Australia in 1606.[35] Luis Váez de Torres passed through Torres Strait in the same year and may have sighted Australia's northern coast.[36] Janszoon's discoveries inspired several mariners, among them, Dutch explorer Abel Tasman, to further chart the area.

Although Abel Tasman is best known for his voyage of 1642; in which he became the first known European to reach the islands of Van Diemen's Land (now Tasmania) and New Zealand, and to sight the Fiji islands, he also contributed significantly to the mapping of Australia proper. With three ships on his second voyage (Limmen, Zeemeeuw and the tender Braek) in 1644, he followed the south coast of New Guinea westward. He missed the Torres Strait between New Guinea and Australia, but continued his voyage along the Australian coast and ended up mapping the north coast of Australia making observations on the land and its people.[37]

With the exception of further Dutch voyages in the west, however, Australia remained largely unvisited by Europeans up until the first British explorations. In 1769, James Cook in command of the HMS Endeavour, traveled to Tahiti to observe and record the transit of Venus. Cook also carried secret Admiralty instructions to locate the supposed Southern Continent:[38] "There is reason to imagine that a continent, or land of great extent, may be found to the southward of the track of former navigators."[39] On 19 April 1770, HMS Endeavour sighted the eastern coastline of Australia and ten days later landed at Botany Bay.

Several writers have made attempts to prove the Dutch and British were not the first to sight Australia. In the 1970s, Kenneth McIntyre argued the Portuguese had secretly discovered Australia in the 1520s [40] Although this theory attracted popular interest in Australia, it is generally regarded by historians as highly contentious or lacking credible evidence. It has recently been argued that Jave la Grande on the Dieppe maps is a theoretical construction, reflecting 16th century views of cosmography. The Dieppe world maps reflected the state of geographical knowledge of their time, both actual and theoretical, not some unknown Portuguese voyages.[41]

By the 1650s, as a result of the Dutch discoveries, most of the Australian coast was charted reliably enough for the navigational standards of the day, and this was revealed for all to see in the map of the world inlaid into the floor of the Burgerzaal —the Great Hall or Citizens’Chamber—of the new Amsterdam Stadhuis (town hall) in 1655. Although various proposals for colonisation were made, notably by Pierre Purry from 1717 to 1744, none were attempted until the British sent the First Fleet to Botany Bay in 1787.[42]

Unlike Indonesia and Japan, there were no cities in Australia for the Dutch to trade with, and the Dutch East India Company concluded that there was “no good to be done there”. They turned down Purry’s scheme with the comment that, “There is no prospect of use or benefit to the Company in it, but rather very certain and heavy costs”. Francois Alesne de Saint-Allouarn claimed Western Australia for France in 1772 but no attempt was made to follow this with colonisation.[43]

The ambition of Sweden’s King Gustav III to establish a colony for his country at the Swan River in 1786 remained stillborn.[44] It was not until the 1780s that economic, technological and political conditions in Great Britain made it possible and worthwhile for that country to make the large effort of sending the First Fleet of eleven ships and over a thousand settlers, with two following fleets, to colonise New South Wales.[45]
[edit] British settlement and colonisation
Main article: History of Australia (1788–1850)
[edit] Plans for Colonisation

Seventeen years after Cook's landfall on the east coast of Australia, the British Government decided to establish a colony at Botany Bay.

In 1779 Sir Joseph Banks, the eminent scientist who had accompanied Lieutenant James Cook on his 1770 voyage, recommended Botany Bay as a suitable site.[46] Banks accepted an offer of assistance made by the American Loyalist James Matra in July 1783. Matra had visited Botany Bay with Banks in 1770 as a junior officer on the Endeavour commanded by James Cook. Under Banks’s guidance, he rapidly produced "A Proposal for Establishing a Settlement in New South Wales" (23 August 1783), with a fully developed set of reasons for a colony composed of American Loyalists, Chinese and South Sea Islanders (but not convicts).[47]

These reasons were: the country was suitable for plantations of sugar, cotton and tobacco; New Zealand timber and hemp or flax could prove valuable commodities; it could form a base for trade with China, Korea, Japan, the North West coast of America, and with the Moluccas; and it could be a suitable compensation for displaced American Loyalists, "where they may repair their broken fortunes & again enjoy their former domestick felicity".[48]

Following an interview with Secretary of State Lord Sydney in March 1784, Matra amended his proposal to include convicts as settlers:

    “ 	When I conversed with Lord Sydney on this Subject, it was observed that New South Wales would be a very proper Region for the reception of Criminals condemned to Transportation. I believe that it will be found, that in this Idea, good Policy, & Humanity are united.... By the Plan which I have now proposed....two Objects of most desirable and beautiful Union, will be permanently blended: Economy to the Publick, & Humanity to the Individual.[49] 	”

Matra’s plan can be seen to have “provided the original blueprint for settlement in New South Wales”.[50] A cabinet memorandum December 1784 shows the Government had Matra’s plan in mind when considering the erection of a settlement in New South Wales.[51] The Government also incorporated into the colonisation plan the project for settling Norfolk Island, with its attractions of timber and flax, proposed by Banks’s Royal Society colleagues, Sir John Call and Sir George Young.[52]

At the same time, humanitarians and reformers were campaigning in Britain against the appalling conditions in British prisons and hulks. In 1777 prison reformer John Howard "wrote The State of Prisons in England and Wales which painted a devastating picture of the reality of prisons and brought into the open much of what had been out of sight ...to genteel society."[53] Penal transportation was already well established as a central plank of English criminal law and until the American War of Independence about a thousand criminals per year were sent to Maryland and Virgina.[54] It served as a powerful deterrent to lawbreaking. At the time, "Europeans knew little about the geography of the globe" and to "convicts in England, transportation to Botany Bay was a frightening prospect." Australia "might as well have been another planet." [55]

In the early 1960s, historian Geoffrey Blainey questioned the traditional view that New South Wales was founded purely as a convict dumping ground. His book The Tyranny of Distance [56] suggested ensuring supplies of flax and timber after the loss of the American colonies may have also been motivations for the British government, and Norfolk Island was the key to the British decision. A number of historians responded, although the debate attracted limited interest beyond academic circles. One result of the debate has been to bring to light a large amount of additional source material on the reasons for settlement.[57]

The decision to settle New South Wales was taken when it seemed the outbreak of civil war in the Netherlands might precipitate a war in which Britain would be again confronted with the alliance of the three naval Powers, France, Holland and Spain, which had brought her to defeat in 1783. Under these circumstances, the attractions were obvious of the strategic advantages of a colony in New South Wales described in James Matra's proposal.[58] Matra indicated how a settlement in New South Wales could facilitate British attacks upon the Spanish colonies in South America and the Philippines, and against the Dutch possessions in the East Indies: “If a Colony from Great Britain was established in that large Tract of Country, and if we were at War with Holland or Spain, we might very powerfully annoy either State from our new Settlement. We might, with a safe and expeditious Voyage, make Naval Incursions on Java and the other Dutch Settlements, and we might with equal facility invade the Coasts of Spanish America, and intercept the Manilla Ships, laden with the Treasures of the West.”[59]
[edit] British Settlements in Australia

The British Crown Colony of New South Wales was established with the arrival of the First Fleet of 11 vessels under the command of Captain Arthur Phillip in January 1788. Some 1,332 people made the voyage, which took over eight months. A few days after arrival at Botany Bay the fleet moved to the more suitable Port Jackson where a settlement was established at Sydney Cove on January 26, 1788.[60] This date later became Australia's national day, Australia Day. The colony was formally proclamed by Governor Arthur Phillip on 7 February 1788 at Sydney Cove in Port Jackson.

The territory claimed included all of that portion of the continent of Australia eastward of the meridian of 135º East and all the islands in the Pacific Ocean between the latitudes of Cape York and the southern tip of Van Diemen’s Land (Tasmania). The extent of this territorial claim excited the amazement of many when they first learned of it. “Extent of Empire demands grandeur of design”, wrote Watkin Tench in A Narrative of the Expedition to Botany Bay.[61] “Truly an astonishing extent!” remarked the Dutch translator of Tench’s book, who went on to say: “The outermost or easternmost of the Marquesas Islands lie, even according to the English maps, at least eighty-five degrees eastward of the line where they place the commencement of the Territory of New South Wales. They have therefore formed a single province which, beyond all doubt, is the largest on the whole surface of the earth. From their definition it covers, in its greatest extent from East to West, virtually a fourth of the whole circumference of the Globe”.[62]

The colony included the current islands of New Zealand, which was administered as part of New South Wales. In 1817, the British Government withdrew the extensive territorial claim over the South Pacific. In practice, the governors' writ had been shown not to run in the islands of the South Pacific.[63] The Church Missionary Society in London described the many atrocities which had been committed against the natives of the South Sea Islands, and the ineffectiveness of the New South Wales government and courts to deal with this lawless situation. As a result, on 27 June 1817 Parliament passed an Act for the more effectual Punishment of Murders and Manslaughters committed in Places not within His Majesty's Dominions, which described Tahiti, New Zealand and other islands of the South Pacific as being not within His Majesty's dominions.[64]

Romantic descriptions of the beauty, mild climate, and fertile soil of Norfolk Island in the South Pacific led the British government to establish a subsidiary settlement of the New South Wales colony there in 1788. It was hoped that the giant Norfolk Island pine trees and flax plants growing wild on the island might provide the basis for a local industry which, particularly in the case of flax, would provide an alternative source of supply to Russia for an article which was essential for making cordage and sails for the ships of the British navy. However, the island had no safe harbor, which led the colony to be abandoned and the settlers evacuated to Tasmania in 1807.[65] The island was subsequently re-settled as a penal settlement in 1824.

Van Diemen's Land, now known as Tasmania, was settled in 1803, following a failed attempt to settle at Sullivan's Cove in what is now Victoria. Other British settlements followed, at various points around the continent, many of them unsuccessful. The East India Trade Committee recommended in 1823 that a settlement be established on the coast of northern Australia to forestall the Dutch, and Captain J.J.G.Bremer, RN, was commissioned to form a settlement between Bathurst Island and the Cobourg Peninsula. Bremer fixed the site of his settlement at Fort Dundas on Melville Island in 1824 and, because this was well to the west of the boundary proclaimed in 1788, proclaimed British sovereignty over all the territory as far west as Longitude 129˚ East.[66]

The new boundary included Melville and Bathurst Islands, and the adjacent mainland. In 1826, the British claim was extended to the whole Australian continent when Major Edmund Lockyer established a settlement on King George Sound (the basis of the later town of Albany), but the eastern border of Western Australia remained unchanged at Longitude 129˚ East. In 1824, a penal colony was established near the mouth of the Brisbane River (the basis of the later colony of Queensland). In 1829, the Swan River Colony and its capital of Perth were founded on the west coast proper and also assumed control of King George Sound. Initially a free colony, Western Australia later accepted British convicts, because of an acute labour shortage.

The German scientist and man of letters Georg Forster, who had sailed under Captain James Cook in the voyage of the Resolution (1772–1775), wrote a remarkably prescient essay in 1786 on the future prospects of the English colony, in which he said: “New Holland, an island of enormous extent or it might be said, a third continent, is the future homeland of a new civilized society which, however mean its beginning may seem to be, nevertheless promises within a short time to become very important.” [67]
[edit] Convicts and Colonial Society
Black-eyed Sue and Sweet Poll of Plymouth, England mourning their lovers who are soon to be transported to Botany Bay, 1792
Main article: Convicts in Australia

Jan Bassett estimates that between 1788 and 1868, 161,700 convicts (of whom 25,000 were women) were transported to the Australian colonies of New South Wales, Van Diemen’s land and Western Australia.[68] Historian Lloyd Robson has estimated that perhaps two thirds were thieves from working class towns, particularly from the midlands and north of England. The majority were repeat offenders.[69] Whether transportation managed to achieve its goal of reforming or not, some convicts were able to leave the prison system in Australia; after 1801 they could gain "tickets of leave" for good behaviour and be assigned to work for free men for wages. A few went on to have successful lives as emancipists, having been pardoned at the end of their sentence.Female convicts had fewer opportunities.

The first five Governors of New South Wales realised the urgent need to encourage free settlers, but the British Government remained largely indifferent. As early as 1790, Governor Arthur Phillip wrote; "Your lordship will see by my…letters the little progress we have been able to make in cultivating the lands...At present this settlement only affords one person that I can employ in cultivating the lands...” [70] It was not until the 1820s that numbers of free settlers began to arrive and government schemes began to be introduced to encourage free settlers. Philanthropists Caroline Chisholm and John Dunmore Lang developed their own migration schemes. Land grants of crown land were made by Governors, and settlement schemes such as those of Edward Gibbon Wakefield carried some weight in encouraging migrants to make the long voyage to Australia, as opposed to the United States or Canada.[71]

From the 1820s, increasing numbers of Squatters[72] occupied land beyond the fringes of European settlement. Often running sheep on large stations with relatively few overheads, Squatters could make considerable profits. By 1834, nearly 2 million kilograms of wool were being exported to Britain from Australia.[73] By 1850, barely 2,000 Squatters had gained 30 million hectares of land, and they formed a powerful and "respectable" interest group in several colonies.[74]

In 1835 the British Colonial Office issued the Proclamation of Governor Bourke, implementing the legal doctrine of terra nullius upon which British settlement was based, reinforcing the notion that the land belonged to no one prior to the British Crown taking possession of it and quashing any likelyhood of treaties with Aboriginal peoples, including that signed by John Batman. Its publication meant that from then, all people found occupying land without the authority of the government would be considered illegal trespassers.[75]

Separate settlements and later, colonies, were created from parts of New South Wales: South Australia in 1836, New Zealand in 1840, Port Phillip District in 1834, later becoming the colony of Victoria in 1851, and Queensland in 1859. The Northern Territory was founded in 1863 as part of South Australia. The transportation of convicts to Australia was phased out between 1840 and 1868.
Pioneer Settler Lucy Sawtell in front of cottage with her children, Dorrigo district, NSW,c.1910

Massive areas of land were cleared for agriculture and various other purposes in the first 100 years of Europeans settlement. In addition to the obvious impacts this early clearing of land and importation of hard-hoofed animals had on the ecology of particular regions, it severely affected indigenous Australians, by reducing the resources they relied on for food, shelter and other essentials. This progressively forced them into smaller areas and reduced their numbers as the majority died of newly-introduced diseases and lack of resources. Indigenous resistance against the settlers was widespread, and prolonged fighting between 1788 and the 1920s led to the deaths of at least 20,000 Indigenous people and between 2,000 and 2,500 Europeans.[76] During the mid-late 19th century, many indigenous Australians in south eastern Australia were relocated, often forcibly, to reserves and missions. The nature of many of these institutions enabled disease to spread quickly and many were closed as their populations fell.
[edit] Colonial self-government and the discovery of gold
Main article: History of Australia (1851–1900)
See also: Australian gold rushes
Gold washing cradles for sale, probably near Rockhampton. Photo taken by Richard Daintree.

The discovery of gold in Australia is traditionally attributed to Edward Hammond Hargraves, near Bathurst, New South Wales, in February 1851. It is now accepted that traces of gold had been found in Australia as early as 1823 by surveyor James McBrien. As by English law all minerals belonged to the Crown, there was at first, “little to stimulate a search for really rich goldfields in a colony prospering under a pastoral economy.”[77] Richard Broome also argues that the California Gold Rush at first overawed the Australian finds, until “the news of Mount Alexander reached England in May 1852, followed shortly by six ships carrying eight tons of gold.”[78]

The gold rushes brought many immigrants to Australia from Great Britain, Ireland, continental Europe, North America and China. For example, the Colony of Victoria’s population grew rapidly, from 76,000 in 1850 to 530,000 by 1859.[79] Discontent arose amongst diggers almost immediately, particularly on the crowded Victorian fields. The causes of this were the colonial government’s administration of the diggings and the gold licence system. Following a number of protests and petitions for reform, violence erupted at Ballarat in late 1854.

Early on the morning of Sunday December 3, 1854, British soldiers and Police attacked a stockade built on the Eureka lead holding some of the aggrieved diggers. In a short fight, at least 30 miners were killed and an unknown number wounded.[80] Blinded by his fear of agitation with democratic overtones, local Commissioner Robert Rede had felt “it was absolutely necessary that a blow should be struck” against the miners.[81]

But a few months later, a Royal commission made sweeping changes to the administration of Victoria’s goldfields. Its recommendations included the abolition of the licence, reforms to the police force and voting rights for miners holding a Miner’s Right.[82] The Eureka flag that was used to represent the Ballarat miners has been seriously considered by some as an alternative to the Australian flag, because of its association with democratic developments.

In the 1890s, visiting author Mark Twain famously characterised the battle at Eureka as:

    “ 	The finest thing in Australasian history. It was a revolution-small in size, but great politically; it was a strike for liberty, a struggle for principle, a stand against injustice and oppression...it is another instance of a victory won by a lost battle.[83] 	”

Later Australian gold rushes occurred at the Palmer River, Queensland, in the 1870s, and Coolgardie and Kalgoorlie in Western Australia, in the 1890s. Confrontations between Chinese and European miners occurred on the Buckland River in Victoria and Lambing Flat in New South Wales, in the late 1850s and early 1860s. Driven by European jealousy of the success of Chinese efforts as alluvial (surface) gold ran out, it fixed emerging Australian attitudes in favour of a White Australia policy, according to historian Geoffrey Serle.[84]

New South Wales in 1855 was the first colony to gain responsible government, managing most of its own affairs while remaining part of the British Empire. Victoria, Tasmania, and South Australia followed in 1856; Queensland, from its foundation in 1859; and Western Australia, in 1890. The Colonial Office in London retained control of some matters, notably foreign affairs, defence and international shipping.

The gold era led to a long period of prosperity, sometimes called "the long boom."[85] This was fed by British investment and the continued growth of the pastoral and mining industries, in addition to the growth of efficient transport by rail, river and sea. By 1891, the sheep population of Australia was estimated at 100 million. Gold production had declined since the 1850s, but in the same year was still worth £5.2 million.[86] Eventually the economic expansion came to an end, and the 1890s were a period of economic depression, felt most strongly in Victoria, and its capital Melbourne.

The late nineteenth century had however, seen a great growth in the cities of south eastern Australia. Australia's population (not including Aborigines) in 1900 was 3.7 million, almost 1 million of whom lived in Melbourne and Sydney.[87] More than two thirds of the population overall lived in cities and towns by the close of the century, making "Australia one of the most urbanised societies in the western world." [88]
[edit] Growth of Nationalism and Federation
The opening of the first Parliament of Australia in 1901
Main article: Federation of Australia

By the late 1880s, a majority of people living in the Australian colonies were native born, although over 90% were of British and Irish origin.[89] Historian Don Gibb suggests that bushranger Ned Kelly represented one dimension of the emerging attitudes of the native born population. Identifying strongly with family and mates, Kelly was opposed to what he regarded as oppression by Police and powerful Squatters. Almost mirroring the Australian stereotype later defined by historian Russel Ward, Kelly became "a skilled bushman, adept with guns, horses and fists and winning admiration from his peers in the district."[90] Journalist Vance Palmer suggested although Kelly came to typify "the rebellious persona of the country for later generations, (he really) belonged...to another period." [91]

Despite suspicion from some sections of the colonial community (especially in smaller colonies) about the value of nationhood, improvements in inter-colonial transport and communication, including the linking of Perth to the south eastern cities by telegraph in 1877,[92] helped break down inter-colonial rivalries. By 1895, powerful interests including various colonial politicians, the Australian Natives' Association and some newspapers were advocating Federation. Increasing nationalism, a growing sense of national identity amongst white colonial Australians, as well as a desire for a national immigration policy, (to become the white Australia policy), combined with a recognition of the value of collective national defence also encouraged the Federation movement. The vision of most colonists was probably staunchly imperial however. At a Federation Conference banquet in 1890, New South Wales politician Henry Parkes said

    “ 	The crimson thread of kinship runs through us all. Even the native born Australians[93] are Britons as much as those born in London or Newcastle. We all know the value of that British origin. We know that we represent a race for which the purpose of settling new countries has never had its equal on the face of the earth... A united Australia means to me no separation from the Empire.[94] 	”

Despite a more radical vision for a separate Australia by some colonists, including writer Henry Lawson, trade unionist William Lane and as found in the pages of the Sydney Bulletin, by the end of 1899, and after much colonial debate, the citizens of five of the six Australian colonies had voted in referendums in favour of a constitution to form a Federation. Western Australia voted to join in July 1900. The "Commonwealth of Australia Constitution Act (UK)" was passed on 5 July 1900 and given Royal Assent by Queen Victoria on 9 July 1900.[95]

The bush ballad Waltzing Matilda, written in 1895 by poet Banjo Paterson,[96] has sometimes been suggested as a Australia's national anthem. Advance Australia Fair, the Australian national anthem since the late 1970s, was written in 1887. The Heidelberg School of Australian painting, inspired by the European Impressionist movement, also emerged in the 1880s and created "the first distinctive Australian school of painting."[97] A common theme throughout the nationalist art, music and writing of late nineteenth century was the romantic rural or bush myth, ironically produced by one of the most urbanised societies in the world.[98] Paterson's well known poem Clancy of the Overflow, written in 1889, evokes the romantic myth.
[edit] A New Nation for the 20th Century
Main article: History of Australia (1901–1945)
[edit] Immigration and Defence concerns
HMAS Australia

The Commonwealth of Australia came into being when the Federal Constitution was proclaimed by the Governor General, Lord Hopetoun, on January 1, 1901. The first Federal elections were held in March 1901. Edmund Barton, the first Australian Prime Minister, laid out his policies almost immediately, his first speech reflecting many of the concerns of the time. Barton promised to "create a high court, ...and an efficient federal public service... He proposed to extend conciliation and arbitration, create a uniform railway gauge between the eastern capitals,[99] to introduce female federal franchise, to establish a...system of old age pensions."[100] He also promised to introduce legislation to safeguard "White Australia" from any influx of Asian or pacific Island labour.

The Immigration Restriction Act 1901 was one of the first laws passed by the new Australian parliament. Aimed to restrict immigration from Asia (especially China), it found strong support in the national parliament, arguments ranging from economic protection to outright racism.[101] A few politicians spoke of the need to avoid hysterical treatment of the question. Member of Parliament Bruce Smith said he had "no desire to see low-class Indians, Chinamen or Japanese...swarming into this country... But there is obligation...not (to) unnecessarily offend the educated classes of those nations"[102] Actual dissent was rare. Donald Cameron, a member from Tasmania expressed views perhaps 100 years before his time when he opposed the law, stating

    “ 	I would like to ask...what treatment the Chinese have received from the English people as a race? I say, without fear of contradiction that no race on...this earth has been treated in a more shameful manner than have the Chinese... They were forced at the point of a bayonet to admit Englishmen...into China. Now if we compel them to admit our people...why in the name of justice should we refuse to admit them here? [103] 	”

The law passed both houses of Parliament and remained a central feature of Australia's immigration laws until abandoned in the 1950s. The absurdity of the law (which allowed for a dictation test in "any European language" to be given to arrivals in Australia), was most famously demonstrated in the Egon Kisch case in the 1930s.[104]

Before 1901, units of soldiers from all six Australian colonies had been active as part of British forces in the Boer War. When the British government asked for more troops from Australia in early 1902, the Australian government obliged with a national contingent. Some 16,500 men had volunteered for service by the war's end in June 1902.[105] But Australians soon felt vulnerable closer to home. The Anglo-Japanese Alliance of 1902 "allowed the Royal Navy to withdraw its capital ships from the Pacific by 1907. Australians saw themselves in time of war a lonely, sparsely populated outpost." [106] The impressive visit of the US Navy's Great White Fleet in August-September 1908 emphasised to the Australian government the value of an Australian navy. The Defence Act of 1909 reinforced the importance of Australian defence, and in February 1910, Lord Kitchener provided further advice on a defence scheme based on conscription. By 1913, the Battle Cruiser Australia led the fledgling Royal Australian Navy. Historian Bill Gammage estimates on the eve of war, Australia had 200,000 men "under arms of some sort".[107]
[edit] Dominion Status

Australia achieved independent Sovereign Nation status after World War One, under the Statute of Westminster. This formalised the Balfour Declaration of 1926, a report resulting from the 1926 Imperial Conference of British Empire leaders in London, which defined Dominions of the British empire in the following way

    “ 	They are autonomous Communities within the British Empire, equal in status, in no way subordinate one to another in any aspect of their domestic or external affairs, though united by a common allegiance to the Crown, and freely associated as members of the British Commonwealth of Nations.[108] 	”

However, Australia did not ratify the Statute of Westminster until 1942. According to historian Frank Crowley, this was because Australians had little interest in redefining their relationship with Britain until the crisis of World War Two.[109]

The Australia Act 1986 removed any remaining links between the British Parliament and the Australian states.

From 1 February 1927 until 12 June 1931, the Northern Territory was divided up as North Australia and Central Australia at latitude 20°S. New South Wales has had one further territory surrendered, namely Jervis Bay Territory comprising 6,677 hectares, in 1915. The external territories were added: Norfolk Island (1914); Ashmore Island, Cartier Islands (1931); the Australian Antarctic Territory transferred from Britain (1933); Heard Island, McDonald Islands, and Macquarie Island transferred to Australia from Britain (1947).

The Federal Capital Territory (FCT) was formed from New South Wales in 1911 to provide a location for the proposed new federal capital of Canberra (Melbourne was the seat of government from 1901 to 1927). The FCT was renamed the Australian Capital Territory (ACT) in 1938. The Northern Territory was transferred from the control of the South Australian government to the Commonwealth in 1911.
[edit] The Emergence of party politics and competing visions of Australia
Eight Hour Procession, 4th October 1909

The Australian Labor Party (ALP) (the spelling “Labour” was dropped in 1912) had been established in the 1890s, after the failure of the Maritime and Shearer’s strikes. Its strength was in the Australian Trade Union movement "which grew from a membership of just under 100,000 in 1901 to more than half a million in 1914."[110] While the platform of the ALP was democratic socialist, the rise of its support at elections, together with its formation of federal government in 1904, and again in 1908, helped to unify competing conservative, free market and anti-socialist forces into the Commonwealth Liberal Party in 1909. The Country Party of Australia was formed in 1913 to represent rural interests.

Historians like Humphrey McQueen argue that in terms of working and living conditions for Australia’s working classes, the years of early twentieth century were ones of “frugal comfort.”[111] While the establishment of an Arbitration court for Labour disputes was divisive, it was an acknowledgement of the need to set Industrial awards, where all wage earners in one industry enjoyed the same conditions of employment and wages. The Harvester Judgment of 1907 also set a benchmark in Australian labour law by recognising the concept of a basic wage. In 1908 the Federal government also began an old age pension scheme.

Catastrophic droughts plagued areas of Australia between in the late 1890s and early 1900s and together with a growing rabbit plague, created great hardship in rural Australia. Despite this, a number of writers “imagined a time when Australia would outstrip Britain in wealth and importance, when its open spaces would support rolling acres of farms and factories to match those of the United States. Some estimated the future population at 100 million, 200 million or more.” [112] Amongst these was E. J. Brady, whose 1918 book Australia Unlimited described Australia’s inland as ripe for development and settlement, "destined one day to pulsate with life." [113]
[edit] To the last man and the last shilling; The First World War
Main article: Military history of Australia during World War I
Australian soldiers in Egypt with a kangaroo as regimental mascot, 1914.

The outbreak of war in Europe in August 1914 automatically involved "all of Britain's colonies and dominions".[114] Prime Minister Andrew Fisher probably expressed the views of most Australians when during the election campaign of late July he said

    “ 	Turn your eyes to the European situation, and give the kindest feelings towards the mother country...I sincerely hope that international arbitration will avail before Europe is convulsed in the greatest war of all time... But should the worst happen...Australians will stand beside our own to help and defend her to the last man and the last shilling.[114] 	”

More than 416,000 Australian men volunteered to fight during the First World War between 1914 and 1918[115] from a total national population of 4.9 million.[116] Historian Lloyd Robson estimates this as between one third and one half of the eligible male population.[117] The Sydney Morning Herald referred to the outbreak of war as Australia's "Baptism of Fire."[118] 8,141 men[119] were killed in 8 months of fighting at Gallipoli, on the Turkish coast. After the Australian Imperial Forces (AIF) was withdrawn in late 1915, and enlarged to five divisions, most were moved to France to serve under British command.

The AIF's first experience of warfare on the Western Front was also the most costly single encounter in Australian military history. In July 1916, at Fromelles, in a diversionary attack during the Battle of the Somme, the AIF suffered 5,533 killed or wounded in 24 hours.[120] Sixteen months later, the five Australian divisions became the Australian Corps, first under the command of General Birdwood, and later the Australian General Sir John Monash. Two bitterly fought and divisive conscription referendums were held in Australia in 1916 and 1917. Both failed, and Australia's army remained a volunteer force.

Monash's approach to the planning of military action was meticulous, and unusual for military thinkers of the time. His first operation at the relatively small Battle of Hamel demonstrated the validity of his approach and later actions before the Hindenburg Line in 1918 confirmed it. Monash wrote

    “ 	The true role of infantry was not to expend itself upon heroic physical effort, not to wither away under merciless machine-gun fire, not to impale itself on hostile bayonets, but on the contrary, to advance under the maximum possible protection of the maximum possible array of mechanical resources...guns, machine-guns, tanks, mortars and aeroplanes...to be relieved as far as possible of the obligation to fight their way forward.[121] 	”

The Australian 4th Battalion lands at the Gallipoli Peninsula on 25 April 1915.

Over 60,000 Australians had died during the conflict and 160,000 were wounded, a high proportion of the 330,000 who had fought overseas.[115] Australia's annual holiday to remember its war dead is held on ANZAC Day, 25 April, each year, the date of the first landings at Gallipoli in 1915. The choice of date is often mystifying to non-Australians; it was after all, an allied invasion that ended in military defeat. Bill Gammage has suggested that the choice of 25 April has always meant much to Australians because at Gallipoli, "the great machines of modern war were few enough to allow ordinary citizens to show what they could do." In France, between 1916 and 1918, "where almost seven times as many (Australians) died,...the guns showed cruelly, how little individuals mattered." [122]
[edit] Men, money and markets: The 1920s

In June 1920 the last Australian soldiers returned home, some eighteen months after the war's end.[123] Prime Minister William Morris Hughes led a new conservative force, the Nationalist Party, formed from the old Liberal party and breakaway elements of Labor (of which he was the most prominent), after the deep and bitter split over Conscription. An estimated 12,000 Australians died as a result of the Spanish flu pandemic of 1919, almost certainly brought home by returning soldiers.[124]

The success of the Bolshevik Revolution in Russia posed a threat in the eyes of many Australians, although to a small group of socialists, it was an inspiration. The Communist Party of Australia was formed in 1920 and has remained active ever since, despite several splits, its banning in 1940-2 and a second attempt to ban it in 1951.[125] Other significant after-effects of the war included ongoing industrial unrest, which included the 1923 Victorian Police strike. "Why should there be the ludicrous and tragic situation of poverty in the midst of plenty, demanded the radicals." [126] Industrial disputes characterised the 1920s in Australia. Other major strikes occurred on the waterfront, in the coalmining and timber industries in the late 1920s. The union movement had established the Australian Council of Trades Unions (ACTU) in 1927 in response to the Nationalist Government's efforts to change working conditions and reduce the power of the unions.

Jazz music, entertainment culture, new technology and consumerism that characterised the 1920s in the USA was, to some extent, also found in Australia. Prohibition did not succeed in Australia however, although anti-alcohol forces were successful in having hotels closed after 6 pm, and closed altogether in a few city suburbs.[127]

The fledgling film industry fared badly as the 1920s went on however, despite the fact that in the mid 1920s over 2 million Australians went to the movies each week in 1250 cinemas. A Royal Commission in 1927 failed to assist and the industry that had begun so brightly in 1906 with the release of The Story of the Kelly Gang, atrophied until its revival in the 1970s.[128][129]

Stanley Bruce became Australian Prime Minister in 1923, when members of the Nationalist Party Government voted to remove W.M. Hughes. Speaking at the Sydney Royal Agricultural Society in early 1925, Bruce summed up the priorities and optimism of many Australians when he spoke of the need for “men, money and markets.”

The Argus newspaper reported:

    “ 	Mr Bruce said… he was more than ever convinced that men, money and markets accurately defined the essential requirements of Australia… Negotiations (had been conducted) with the British Government for the provision of money…to carry out development works which would greatly increase Australia’s power to absorb migrants… A greater flow of British immigrants had occupied the attention of the British and Australian ministries since the end of the Great War.[130] 	”

According to historian Stuart Macintyre, the migration campaign of the 1920s, operated by the Development and Migration Commission, brought almost 300,000 Britons to Australia by the end of the decade,[131] although schemes to settle migrants and returned soldiers "on the land" were generally not a success. "The new irrigation areas in Western Australia and the Dawson Valley of Queensland proved disastrous" [132]

Traditionally in Australia the costs of major investment have been met by state and Federal governments. Heavy borrowing from overseas was made by the governments in the 1920s, and a Loan Council set up in 1928 to coordinate loans, three quarters of which came from overseas.[133] Despite Imperial preference, a balance of trade was not successfully achieved with Britain. "In the five years from 1924..to..1928, Australia bought 43.4% of its imports from Britain and sold 38.7% of its exports. Wheat and wool made up more than two thirds of all Australian exports," a dangerous reliance on just two export commodities.[134]

The 1920s saw significant development in transport, including the final abandonment of the coastal sailing ship in favour of steam, and improvements in rail and motor transport heralded dramatic changes in work and leisure. In 1918 there were 50,000 cars and lorries in the whole of Australia. By 1929 there were 500,000.[135] The stage coach company Cobb and Co, established in 1853, ran its last coach in outback Queensland in 1924.[136] In 1920, the Queensland and Northern Territory Aerial Service (to become the Australian airline QANTAS) was established.[137]
[edit] The Depression Decade: The 1930s
Main article: Great Depression in Australia
Sir Otto Ernst Niemeyer. In July 1930 he visited Australia to provide advice to the Scullin Government

The Great Depression of the 1930s was an economic catastrophe that severely affected most nations of the world, and Australia was not immune. In fact, Australia, with its extreme dependence on exports, particularly primary products such as wool and wheat,[138] is thought to have been one of the hardest-hit countries in the Western world along with Canada and Germany.

Exposed by continuous borrowing to fund capital works in the 1920s, the Australian and State Governments were "already far from secure in 1927, when most economic indicators took a turn for the worse. Australia's dependence of exports left her extraordinaily vulnerable to world market fluctuations," according to economic historian Geoff Spenceley.[139] Debt by the state of New South Wales accounted for almost half Australia’s accumulated debt by December 1927. The situation caused alarm amongst a few politicians and economists, notably Edward Shann of the University of Western Australia, but most political, union and business leaders were reluctant to admit anything was seriously wrong.[140] In 1926, Australian Finance magazine stated:

    “ 	In the whole British Empire, there is no more voracious borrower than the Australian Commonwealth. Loan follows loan with disconserting frequency. It may be a loan to pay off maturing loans or a loan to pay the interest on existing loans, or a loan to repay temporary loans from the bankers...[141] 	”

Thus, well before the Wall Street Crash of October 29, 1929, the Australian economy was already facing significant difficulties. As the economy slowed in 1927, so did manufacturing and the country slipped into recession as profits slumped and unemployment rose.[142] At elections held on October 12, 1929 the Labor Party was swept to power in a landslide, the former Prime Minister Stanley Bruce losing his own seat in the House of Representatives. The new Prime Minister James Scullin and his largely inexperienced Government were almost immediately faced with a series of crises. Hamstrung by their lack of control of the Senate, a lack of control over the Banking system and divisions within the Labor Party over how best to deal with the situation, the government was forced to accept solutions that eventually split the party, as it had in 1917.

Various "plans" to resolve the crisis were suggested; Sir Otto Niemeyer, a representative of the English banks who visited in mid 1930, proposed a deflationary plan, involving cuts to government spending and wages. Treasurer Ted Theodore proposed a mildly inflationary plan, while the Labor Premier of New South Wales, Jack Lang proposed a radical plan which repudiated overseas debt.[143] The "Premier's Plan" finally accepted by Federal and State governments in June 1931, followed the deflationary model advocated by Niemeyer and included a reduction of 20% in Government spending, a reduction in bank interest rates and an increase in taxation.[144] Niemeyer famously told the Australian leaders of government;

    “ 	There is evidence...that the standard of living in Australia has reached a point which is economically beyond the capacity of the country to bear.[145] 	”

There is debate today over the extent of unemployment in Australia, which is often cited as peaking at 29% in 1932. "Trade Union figures are the most often quoted, but the people who were there…regard the figures as wildly understating the extent of unemployment" wrote historian Wendy Lowenstein in her collection of oral histories of the Depression.[146] However, David Potts argues that "over the last thirty years …historians of the period have either uncritically accepted that figure (29% in the peak year 1932) including rounding it up to ‘a third,’ or they have passionately argued that a third is far too low." [147] Potts suggests a peak national figure of 25% unemployed.[148]

However, there seems little doubt that there was great variation in levels of unemployment. For example, statistics collected by historian Peter Spearritt show 17.8% of men and 7.9% of women unemployed in 1933 in the comfortable Sydney suburb of Woollahra. In the working class suburb of Paddington, 41.3% of men and 20.7% of women were listed as unemployed.[149] Geoffrey Spenceley argues that apart from variation between men and women, unemployment was also much higher in some industries, such as the building and construction industry, and comparatively low in the public administrative and professional sectors.[150] In the bush, worst hit were small farmers in the wheat belts as far afield as north-east Victoria and Western Australia, who saw more and more of their income absorbed by interest payments.[151]

In May 1931, a new conservative political force, the United Australia Party was formed by breakaway members of the Labor Party combining with the Nationalist Party. At Federal elections in December 1931, the United Australia Party, led by former Labor member Joseph Lyons, easily won office. They remained in power until September 1940. The Lyons government has often been credited with steering recovery from the depression, although just how much of this was owed to their policies remains contentious.[152] Stuart Macintyre also points out that although Australian GDP grew from £386.9 million to £485.9 million between 1931-2 and 1938-9, real domestic product per head of population was still "but a few shillings greater in 1938-39 (£70.12), than it had been in 1920-21 (£70.04).[153]
[edit] The Second World War
Main articles: Military history of Australia during World War II and Axis naval activity in Australian waters
The light cruiser HMAS Sydney, lost in a battle in the Indian Ocean, November 1941.
[edit] Defence Policy in the 30s

Until the late 1930s, defence was not a significant issue for Australians. At the 1937 elections, both political parties advocated increased defence spending, in the context of increased Japanese aggression in China and Germany’s aggression in Europe. There was a difference in opinion over how the defence spending should be allocated however. The UAP government emphasised cooperation with Britain in "a policy of imperial defence." The lynchpin of this was the British naval base at Singapore and the Royal Navy battle fleet "which, it was hoped, would use it in time of need." [154] Defence spending in the inter-war years reflected this priority. In the period 1921-1936 totalled £40 million on the RAN, £20 million on the Australian Army and £6 million on the RAAF (established in 1921, the "youngest" of the three services). In 1939, the Navy, which included two heavy cruisers and four light cruisers, was the service best equipped for war.[155]

Gavin Long argues that the Labor opposition urged greater national self-reliance through a build up of manufacturing and more emphasis on the Army and RAAF, as Chief of the General Staff, John Lavarack also advocated.[156] In November 1936, Labor leader John Curtin said "The dependence of Australia upon the competence, let alone the readiness, of British statesmen to send forces to our aid is too dangerous a hazard upon which to found Australia’s defence policy.".[157] According to John Robertson, "some British leaders had also realised that their country could not fight Japan and Germany at the same time." But "this was never discussed candidly at…meeting(s) of Australian and British defence planners", such as the 1937 Imperial Conference.[158]

By September 1939 the Australian Army numbered 3,000 regulars. A recruiting campaign in late 1938, led by Major-General Thomas Blamey increased the reserve militia to almost 80,000.[159] The first division raised for war was designated the 6th Division, of the 2nd AIF, there being 5 Militia Divisions on paper and a 1st AIF in the First World War.[160]
[edit] The War

On Sunday September 3, 1939, Prime Minister Robert Menzies made a national radio broadcast:

    “ 	My fellow Australians. It is my melancholy duty to inform you, officially, that, in consequence of the persistence by Germany in her invasion of Poland, Great Britain has declared war upon her, and that, as a result, Australia is also at war.[161] 	”

In this statement, Menzies, who had been Prime Minister and leader of the UAP since Lyon's death in 1939, reflected a widely held Australian "detestation of Germany's agression and a conviction that Britain, France and the Commonwealth countries were involved in a just war."[162]
A patrol from the 2/13th Infantry Battalion at Tobruk (AWM 020779)

Some writers emphasise how extraordinarily varied combat experience was to be for soldiers from Australia; "more varied, geographically than that of (some of) the great powers, Russia, China and Japan…The war could mean young men at Rabaul taking off in Wirraways to meet certain death from more numerous Zeros. It could mean an infantryman on a jungle patrol behind Japanese lines, or facing German tanks on the Tobruk perimeter. It was men of the Perth fighting until their ammunition was exhausted…or a young man, not long out of school, flying a Lancaster on his first mission over Germany."[163]
An Australian propaganda poster from 1942.

In 1940-41, Australian forces played prominent roles in the fighting in the Mediterranean theatre, including Operation Compass, the Siege of Tobruk, the Greek campaign, the Battle of Crete, the Syria-Lebanon campaign and the Second Battle of El Alamein. The war came closer to home when HMAS Sydney was lost with all hands in battle with the German raider Kormoran in November 1941.

After the attacks on Pearl Harbor and on Allied states throughout East Asia and the Pacific, from 8 December (Australian time) 1941, Prime Minister John Curtin insisted that Australian forces be brought home to fight Japan. After the Fall of Singapore in February 1942, 15,000 Australian soldiers became prisoners of war. A few days later, Darwin was heavily bombed by Japanese planes, the first time the Australian mainland had ever been attacked by enemy forces. Over the following 19 months, Australia was attacked from the air almost 100 times. The shock of Britain's defeat in Asia in 1942 and the threat of Japanese invasion caused Australia to turn to the United States as a new ally. On December 27, 1941 Curtin wrote a New Year's message for an Australian newspaper, which included the famous lines:

    “ 	"The Australian Government...regards the Pacific struggle as primarily one in which the United States and Australia must have the fullest say in the direction of the democracies' fighting plan. Without inhibitions of any kind, I make it clear that Australia looks to America, free of any pangs as to our traditional links or kinship with the United Kingdom."[164] 	”

An Australian light machine gun team in action near Wewak in June 1945

At a press conference the following day Curtin qualified the message as not meaning a "weakening of Australia's ties with the British Empire."[165] However, Curtin's Labor Government forged a close alliance with the United States, and a fundamental shift in Australia's foreign policy began. General Douglas MacArthur, the Supreme Allied Commander in the South West Pacific Area, moved his headquarters to Australia in March 1942. In late May 1942, Japanese midget submarines sank one troop transport in a daring raid on Sydney Harbour. On 8 June 1942, two Japanese submarines briefly shelled Sydney's eastern suburbs and the city of Newcastle.[166] The threat of Japanese invasion was averted by Allied successes in the battles of Coral Sea and Midway.

Australian forces then bitterly fought Japanese attempts to take Port Moresby, by way of the Kokoda Track, in the highlands of New Guinea. The Australian victory in the Battle of Milne Bay in August 1942 was the first Allied defeat of Japanese land forces. However, the Battle of Buna-Gona set the tone for the bitter final stages of the New Guinea campaign, which persisted into 1945. It was followed by Australian-led amphibious assaults against Japanese bases in Borneo.
[edit] Australia during the war
Main article: Australian home front during World War II

Historian Geoffrey Bolton argues the Australian economy was markedly affected by World War II.[167] In economic terms - expenditure on war reached 37% of GDP by 1943-4, compared to 4% expenditure in 1939-1940.[168] Total war expenditure was £2,949 million between 1939 and 1945.[169] Of Australia’s wartime population of 7 million, almost 1 million men and women served in a branch of the services at some stage of the six years of warfare. By war’s end, gross enlistments totalled 727,200 men and women in the Australian Army (of whom 557,800 served overseas), 216,900 in the RAAF and 48,900 in the RAN. Over 39,700 were killed or died as prisoners of war, about 8,000 of whom died as prisoners of the Japanese.[170]

Although the peak of Army enlistments occurred in June-July 1940, when over 70,000 enlisted, it was the Curtin Labor Government, formed in October 1941, that was largely responsible for "a complete revision of the whole Australian economic, domestic and industrial life."[171] Rationing of fuel, clothing and some food was introduced, (although less severely than in Britain) Christmas holidays curtailed, "brown outs" introduced and some public transport reduced. From December 1941, the Government evacuated all women and children from Darwin and northern Australia, and over 10,000 refugees arrived from South East Asia as Japan advanced.[172] In January, 1942, the Manpower Directorate was set up "to ensure the organisation of Australians in the best possible way to meet all defence requirements."[171] Minister for War Organisation of Industry, John Dedman introduced a degree of austerity and government control previously unknown, to such an extent that he was nicknamed "the man who killed Father Christmas."
Western Australia, 1943. The Minister for the Australian Army, Frank Forde, inspecting personnel of the Australian Women's Army Service.

In May 1942 uniform tax laws were introduced in Australia, as state governments relinquished their control over income taxation. "The significance of this decision was greater than any other… made throughout the war, as it added extensive powers to the Federal Government and greatly reduced the financial autonomy of the states." [173] In the post war world, Federal power would grow significantly as a result of this change.

Manufacturing grew significantly because of the war. "In 1939 there were only three Australian firms producing machine tools, but by 1943 there were more than one hundred doing so."[174] From having few front line aircraft in 1939, the RAAF had become the fourth largest allied Air force by 1945. A number of aircraft were built under licence in Australia before the war’s end, notably the Beaufort and Beaufighter, although the majority of aircraft were from Britain and later, the USA.[175] The Boomerang fighter, designed and built in four months of 1942, emphasised the desperate state Australia found itself in as the Japanese advanced.

Australia also created, virtually from nothing, a significant female workforce engaged in direct war production. Between 1939 and 1944 the number of women working in factories rose from 171,000 to 286,000.[176] Dame Enid Lyons, widow of former Prime Minster Joseph Lyons, became the first woman elected to the House of Representatives in 1943, joining the Robert Menzies' new centre-right Liberal Party of Australia, formed in 1945. At the same election, Dorothy Tangney became the first woman elected to the Senate.
[edit] Australia Post World War 2
Main article: History of Australia since 1945

Following World War II, the Australian government instigated a massive program of European immigration. After narrowly preventing a Japanese invasion and suffering attacks on Australian soil for the first time, it was seen that the country must "populate or perish". Immigration brought traditional migrants from the United Kingdom along with, for the first time, large numbers of southern and central Europeans. A booming Australian economy stood in sharp contrast to war-ravaged Europe, and newly-arrived migrants found employment in government assisted programs such as the Snowy Mountains Scheme. Two million immigrants arrived between 1948 and 1975.

Robert Menzies' newly-founded Liberal Party of Australia dominated much of the immediate post war era, defeating the Australian Labor Party government of Ben Chifley in 1949. Menzies oversaw the post-war expansion and became the country's longest-serving leader. Manufacturing industry, previously playing a minor part in an economy dominated by primary production, greatly expanded. Since the 1970s and the abolition of the White Australia policy from Asia and other parts of the world, Australia's demography, culture and image of itself has been radically transformed.

The ANZUS defence treaty was signed in 1951 with the United States and New Zealand, and Australia committed troops to the Korean War and the Malayan Emergency. Melbourne hosted the 1956 Summer Olympics and joint British-Australia nuclear tests and rocket launches began near Woomera, South Australia. The population reached 10 million in 1959.

Since 1951, Australia has been a formal military ally of the U.S. under the auspices of the ANZUS treaty. The final constitutional ties between Australia and Britain ended in 1986 with the passing of the Australia Act 1986, ending any British role in the Australian States, and ending judicial appeals to the UK Privy Council.

Australia remains a constitutional monarchy with Queen Elizabeth II the Queen of Australia; the 1999 referendum to establish a republic was marginally rejected. Australia's formal links to its British past are increasingly tenuous, although people-to-people and cultural connections between Australia and Britain remain significant. Since the election of the Whitlam Government in 1972, there has been an increasing focus on the nation's future as a part of the so-called "Asia-Pacific" region.

Territories transferred in this period were: Christmas Island and Cocos (Keeling) Islands. The Coral Sea Islands Territory was established as a Territory of the Commonwealth under the Coral Sea Islands Act 1969. In 1989 when the Australian Capital Territory achieved self government, Jervis Bay became a separate territory administered by the Minister for Territories.

The history of New Zealand dates back at least 700 years to when it was discovered and settled by Polynesians, who developed a distinct Māori culture centred on kinship links and land. The first European explorer to discover New Zealand was Abel Janszoon Tasman on 13 December 1642[1]. From the late 18th century, the country was regularly visited by explorers and other sailors, missionaries, traders and adventurers. In 1840 the Treaty of Waitangi was signed between the British Crown and various Māori chiefs, bringing New Zealand into the British Empire and giving Māori equal rights with British citizens. There was extensive European and some Asian settlement throughout the rest of the century. War and the imposition of a European economic and legal system led to most of New Zealand's land passing from Māori to Pākehā (European) ownership, and most Māori subsequently became impoverished.

From the 1890s the New Zealand parliament enacted a number of progressive initiatives, including women's suffrage and old age pensions. From the 1930s the economy was highly regulated and an extensive welfare state was developed. Meanwhile, Māori culture underwent a renaissance, and from the 1950s Māori began moving to the cities in large numbers. This led to the development of a Māori protest movement which in turn led to greater recognition of the Treaty of Waitangi in the late twentieth century. In the 1980s the economy was largely deregulated and a number of socially liberal policies, such as decriminalisation of homosexuality, were put in place. Foreign policy, which had previously consisted mostly of following Britain or the United States, became more independent. Subsequent governments have generally maintained these policies, although tempering the free market ethos somewhat.
Contents
[hide]

    * 1 Polynesian foundation
    * 2 Early contact period
          o 2.1 Explorers and other visitors
          o 2.2 Māori response
          o 2.3 European settlement
    * 3 British sovereignty
          o 3.1 Treaty of Waitangi
    * 4 Colonial period
          o 4.1 Immigration
          o 4.2 Māori adaptation and resistance
          o 4.3 South Island
          o 4.4 1890s
    * 5 Dominion and Realm
          o 5.1 First World War
          o 5.2 Depression
          o 5.3 Second World War
          o 5.4 Maori Urbanisation
          o 5.5 Post-war
          o 5.6 Reform
          o 5.7 New Zealand today
    * 6 See also
    * 7 References
    * 8 Further reading
    * 9 External links

[edit] Polynesian foundation

New Zealand was originally settled by Polynesians from Eastern Polynesia. The most current reliable evidence strongly indicates that initial settlement of New Zealand occurred around 1280 CE. Previous dating of some Kiore (Polynesian rat) bones at 50 - 150 CE has now been shown to have been unreliable; new samples of bone (and now also of unequivocally rat-gnawed woody seed cases) match the 1280 CE date of the earliest archaeological sites and the beginning of sustained, anthropogenic deforestation[2].

The descendants of these settlers became known as the Māori, forming a distinct culture of their own. Separate settlement of the tiny Chatham Islands in the east of New Zealand about 1500 CE produced the Moriori people; linguistic evidence indicates that the Moriori were mainland Māori who ventured eastward[3].

The original settlers quickly exploited the abundant large game in New Zealand, such as moa, large flightless ratites that were pushed to extinction by about 1500. As moa and other large game became scarce or extinct, Māori culture underwent major change, with regional differences. In areas where it was possible to grow taro and kūmara, horticulture became more important. In the south of the South Island, however elsewhere wild plants such as fernroot were often available for harvest and cabbage trees were harvested and cultivated for food. Warfare also increased in importance, reflecting increased competition for land and other resources. In this period, fortified pā became more common, although there is debate about the actual frequency of warfare. As elsewhere in the Pacific, Cannibalism was part of warfare. For an overview of New Zealand (Māori) history from the 11th to the 16th century, see BELICH, James, [4].

Leadership was based on a system of chieftainship, which was often but not always hereditary, although chiefs (male or female) needed to demonstrate leadership abilities to avoid being superseded by more dynamic individuals. The most important units of pre-European Māori society were the whānau or extended family, and the hapū or group of whānau. After these came the iwi or tribe, consisting of groups of hapū. Related hapū would often trade goods and co-operate on major projects, but conflict between hapū was also relatively common. Traditional Māori society preserved history orally through narratives, songs, and chants; skilled experts could recite the tribal genealogies (whakapapa) back for hundreds of years. Arts included whaikōrero (oratory), song composition in multiple genres, dance forms including haka, as well as weaving, highly developed wood carving, and tā moko (tattoo).

Birds, fish and sea mammals were important sources of protein[citation needed].[5] Māori cultivated food plants which they had brought with them from Polynesia, including sweet potatoes (called kūmara), taro, gourds and yams. They also cultivated the cabbage tree, a plant endemic to New Zealand, and exploited wild foods such as fern root, which provided a starchy paste.
[edit] Early contact period
[edit] Explorers and other visitors
First map of New Zealand, drawn by Captain James Cook.

The first Europeans known to reach New Zealand were the crew of Dutch explorer Abel Tasman who arrived in his ships Heemskerck and Zeehaen. Tasman anchored at the northern end of the South Island in Golden Bay (he named it Murderers Bay) in December 1642 and sailed northward to Tonga following a clash with local Māori. Tasman sketched sections of the two main islands' west coasts. Tasman called them Staten Landt, after the States-General of the Netherlands, and that name appeared on his first maps of the country. Dutch cartographers changed the name to Nova Zeelandia in Latin, from Nieuw Zeeland, after the Dutch province of Zeeland. It was subsequently Anglicised as New Zealand by British naval captain James Cook of HM Bark Endeavour who visited the islands more than 100 years after Tasman during 1769–1770. Cook returned to New Zealand on both of his subsequent voyages. Various claims have been made that New Zealand was reached by other non-Polynesian voyagers before Tasman, but these are not widely accepted.[6]

From the 1790s, the waters around New Zealand were visited by British, French and American whaling, sealing and trading ships. Their crews traded European goods, including guns and metal tools, for Māori food, water, wood, flax and sex.[7] Māori were reputed to be enthusiastic and canny traders. Although there were some conflicts, such as the killing of French explorer Marc-Joseph Marion du Fresne and the destruction of the Boyd, most contact between Māori and European was peaceful. From the 1800s missionaries began settling in New Zealand and attempting to convert Māori to Christianity and control the considerably lawless European visitors.
[edit] Māori response

The impact of contact on Māori varied. In some inland areas life went on more or less unchanged, although a European metal tool such as a fish-hook or hand axe might be acquired through trade with other tribes. At the other end of the scale, tribes that frequently encountered Europeans, such as Ngā Puhi in Northland, underwent major changes.

Pre-European Māori had no distance weapons except for tao (spears)[8] and the introduction of the musket had an enormous impact on Māori warfare. The peaceful Moriori of the Chatham Islands were nearly exterminated and enslaved by mainland Ngāti Mutunga and Ngāti Tama Māori[9]. In the 1901 census, only 35 Moriori were recorded although the numbers subsequently increased[10]. Tribes with muskets would attack tribes without them, killing or enslaving many.[citation needed] As a result, guns became very valuable and Māori would trade huge quantities of goods for a single musket. The Musket Wars died out in the 1830s after most tribes had acquired muskets and a new balance of power was achieved.

Around this time, many Māori converted to Christianity. The reasons for this have been hotly debated, and may include social and cultural disruption caused by the Musket Wars and European contact. Other factors may have been the appeal of a religion that promotes peace and forgiveness, a desire to emulate the Europeans and to gain a similar abundance of material goods, and the Māori's polytheistic culture that easily accepted the new god.
[edit] European settlement

European settlement increased through the early decades of the nineteenth century, with numerous trading stations established, especially in the North. The first full-blooded European infant in the territory, Thomas King, was born in 1815 in the Bay of Islands. Many Europeans bought land from Māori, but misunderstanding and different concepts of land ownership led to conflict and bitterness. In 1839, the New Zealand Company announced plans to buy large tracts of land and establish colonies in New Zealand. This alarmed the missionaries, who called for British control of European settlers in New Zealand.
[edit] British sovereignty

In 1788 the colony of New South Wales had been founded. According to Captain Phillip's amended Commission, dated 25 April 1787, the colony included all the islands adjacent in the Pacific Ocean within the latitudes of 10°37'S and 43°39'S which included most of New Zealand except for the southern half of the South Island. In 1825 with Van Diemen's Land becoming a separate colony, the southern boundary of New South Wales was altered[11] to the islands adjacent in the Pacific Ocean with a southern boundary of 39°12'S which included only the northern half of the North Island. However, these boundaries had no real impact as the New South Wales administration had little interest in New Zealand.[12]

In response to complaints about lawless white sailors and adventurers in New Zealand, the British government appointed James Busby as Official Resident in 1832. In 1834 he encouraged Māori chiefs to assert their sovereignty with the signing of the Declaration of Independence in 1835. This was acknowledged by King William IV. Busby was provided with neither legal authority nor military support and was thus ineffective in controlling the European population.
[edit] Treaty of Waitangi
Main article: Treaty of Waitangi
Signing of the Treaty of Waitangi.

In 1839, the New Zealand Company announced its plans to establish colonies in New Zealand. This, and the continuing lawlessness of many of the established settlers, spurred the British to take stronger action. Captain William Hobson was sent to New Zealand to persuade Māori to cede their sovereignty to the British Crown. In reaction to the New Zealand Company's moves, on 15 June 1839 a new Letters patent was issued to expand the territory of New South Wales to include all of New Zealand. Governor of New South Wales George Gipps was appointed Governor over New Zealand. This was the first clear expression of British intent to annex New Zealand.

On 6 February 1840, Hobson and about forty Māori chiefs signed the Treaty of Waitangi at Waitangi in the Bay of Islands. Copies of the Treaty were subsequently taken around the country to be signed by other chiefs. A significant number refused to sign or were not asked but, in total, more than five hundred Māori eventually signed.

The Treaty gave Māori sovereignty over their lands and possessions and all of the rights of British citizens. What it gave the British in return depends on the language-version of the Treaty that is referred to. The English version can be said to give the British Crown sovereignty over New Zealand but in the Māori version the Crown receives kawanatanga, which, arguably, is a lesser power (see Treaty of Waitangi#Meaning and interpretation). Dispute over the true meaning and the intent of either party remains an issue.

Britain was motivated by the desire to forestall other European powers (France established a very small settlement at Akaroa in the South Island later in 1840), to facilitate settlement by British subjects and, possibly, to end the lawlessness of European (predominantly British and American) whalers, sealers and traders. Officials and missionaries had their own positions and reputations to protect.

Māori chiefs were motivated by a desire for protection from foreign powers, the establishment of governorship over European settlers and traders in New Zealand, and to allow for wider settlement that would increase trade and prosperity for Māori.[13]

Hobson died in September 1842. Robert FitzRoy, the new governor, took some legal steps to recognise Māori custom. However, his successor, George Grey, promoted rapid cultural assimilation and reduction of the land ownership, influence and rights of the Māori. The practical effect of the Treaty was, in the beginning, only gradually felt, especially in predominantly Māori regions.
[edit] Colonial period

Having been administered, through 1840 when the treaty was signed, as a part of the Australian colony of New South Wales, New Zealand became a colony in its own right on 3 May 1841. It was divided into provinces that were reorganised in 1846 and in 1853, when they acquired their own legislatures, and then abolished in 1876. The country rapidly gained some measure of self-government through the New Zealand Constitution Act 1852, which established central and provincial government.
[edit] Immigration
Main article: Immigration to New Zealand

From 1840 there was considerable European settlement, primarily from England and Wales, Scotland and Ireland; and to a lesser extent the United States, India, and various parts of continental Europe, including the province of Dalmatia[14] in what is now Croatia, and Bohemia[15] in what is now the Czech Republic. Already a majority of the population by 1859, the number of white settlers (called Pākehā by Māori) increased rapidly to reach a million by 1911.

In the 1870s and 1880s, several thousand Chinese men, mostly from the Guangdong province, migrated to New Zealand to work on the South Island goldfields. Although the first Chinese migrants had been invited by the Otago Provincial government they quickly became the target of hostility from white settlers and laws were enacted specifically to discourage them from coming to New Zealand.[16]
[edit] Māori adaptation and resistance

Māori had welcomed Pākehā for the trading opportunities and guns they brought. However it soon became clear that they had underestimated the number of settlers that would arrive in their lands. Iwi (tribes) whose land was the base of the main settlements quickly lost much of their land and autonomy through government acts. Others prospered—until about 1860 the city of Auckland bought most of its food from Māori who grew and sold it themselves. Many iwi owned flour mills, ships and other items of European technology, some exported food to Australia. Although race relations were generally peaceful in this period, there were conflicts over who had ultimate power in particular areas—the Governor or the Māori chiefs. One such conflict was the Northern or Flagstaff War of the 1840s, during which the town of Kororareka was destroyed.

As the Pākehā population grew, pressure grew on Māori to sell more land. A few tribes had become nearly landless and others feared losing their lands. Land is not only an economic resource, but also the basis of Māori identity and a connection with their ancestors. Land was held communally, it was not given up without discussion and consultation—or loss during warfare.

Pākehā had little understanding of all that and accused Māori of holding onto land they did not use efficiently. Competition for land was a primary cause of the New Zealand Land Wars of the 1860s and 1870s, in which the Taranaki and Waikato regions were invaded by colonial troops and Māori of these regions had much of their land taken from them. The wars and confiscation left bitterness that remains to this day.

Some iwi sided with the government and, later, fought with the government. They were motivated partly by the thought that an alliance with the government would benefit them, and partly by old feuds with the iwi they fought against. One result of their co-operation strategy was the establishment of the four Māori seats in parliament, in 1867.

After the wars, some Māori began a strategy of passive resistance, most famously at Parihaka in Taranaki. Others continued co-operating with Pākehā. For example, tourism ventures were established by Te Arawa around Rotorua. Resisting and co-operating iwi both found that the Pākehā desire for land remained. In the last decades of the century, most iwi lost substantial amounts of land through the activities of the Native Land Court. This was set up to give Māori land European-style titles and to establish exactly who owned it. Due to its Eurocentric rules, the high fees, its location remote from the lands in question, and unfair practices by many Pākehā land agents, its main effect was to directly or indirectly separate Māori from their land.

The combination of war, confiscations, disease,[17] assimilation and intermarriage,[18] land loss leading to poor housing and alcohol abuse, and general disillusionment, caused a fall in the Māori population from around 86,000 in 1769 to around 70,000 in 1840 and around 48,000 by 1874, hitting a low point of 42,000 in 1896.[19] Subsequently their numbers began to recover.
[edit] South Island

While the North Island was convulsed by the Land Wars, the South Island, with its low Māori population, was generally peaceful. In 1861 gold was discovered at Gabriel's Gully in Central Otago, sparking a gold rush. Dunedin became the wealthiest city in the country and many in the South Island resented financing the North Island’s wars. In 1865 Parliament voted on a Bill to make the South Island independent: it was defeated 17 to 31.

The South Island contained most of the Pākehā population until around 1900 when the North Island again took the lead and has supported an ever greater majority of the country's total population through the 20th century and into the 21st.
[edit] 1890s

Major changes occurred during this decade. The economy—based on wool and local trade—changed to the export of frozen meat and dairy products to Britain. This change was enabled by the invention of refrigerated shipping that allowed foodstuff to be transported over long distances. Refrigerated shipping remained the basis of New Zealand’s economy until the 1970s. In the 21st century, New Zealand's trade in skim milk and butter increased, thanks to their high price.

The decade also saw the advent of party politics, with the establishment of the First Liberal government. This government established the basis of the welfare state, with old age pensions, developed a system for settling industrial disputes, which was accepted by both employers and unions, and in 1893 extended voting rights to women, making New Zealand the first country in the world to enact universal female suffrage.
[edit] Dominion and Realm
Main article: Independence of New Zealand
Historical map of Australia and New Zealand, 1788-1911.

New Zealand decided against joining the Commonwealth of Australia in 1901, and instead changed from being a colony to a separate "dominion" in 1907, equal in status to Australia and Canada.
[edit] First World War
Main article: Military history of New Zealand in World War I

The country remained an enthusiastic member of the British Empire, and many New Zealanders fought in World War I (see New Zealand Expeditionary Force). New Zealand forces took Western Samoa from Germany in the early stages of the war, and New Zealand administered the country until Samoan Independence in 1962.
[edit] Depression

Like most other countries, New Zealand was hard hit by the Great Depression of the 1930s, which affected the country via its international trade, with farming export drops then going on to affect the money supply and in turn consumption, investment and imports. The country was most affected around 1930-1932, when average farm incomes for a short time dipped below zero, and the unemployment rates peaked. Though actual unemployment numbers were not officially counted, the country was affected especially strongly in the North Island.[20]

Unlike later years, there were no public benefit ('dole') payments — the unemployed were given 'relief work', much of which was however not very productive, partly because the size of the problem was unprecedented. Women also increasingly registered as unemployed, while Maori received government help through other channels such as the land development schemes organised by Apirana Ngata. In 1933, 8.5% of the unemployed were organised in work camps, while the rest received work close to their homes. Typical occupations in relief work were road work (undertaken by 45% of all part-time and 19% of all full-time relief workers in 1934, with park improvement works (17%) and farm work (31%) being the other two most common types of work for part-time and full-time relief workers respectively).[20]

Attempts by the conservative Liberal-Reform coalition to deal with the situation with spending cuts and relief work were ineffective and unpopular. In 1935, the First Labour Government was elected, and the post-depression decade showed that average Labour support in New Zealand had roughly doubled comparable to pre-depression times. By 1935 economic conditions had improved somewhat, and the new government had more positive financial conditions,[20] under which it established a full welfare state, which included free health care and education and state assistance for the elderly, infirm, and unemployed. The programme was retained and expanded by successive National and Labour governments.
[edit] Second World War
Main article: Military history of New Zealand in World War II

When World War II broke out, New Zealand contributed some 120,000 troops. They mostly fought in Europe, relying on the Royal Navy and later the United States to protect New Zealand from the Japanese forces, who never reached as far as the New Zealand mainland except with some highly publicised but essentially ineffective scouting incursions. The cooperation with the United States meanwhile set a direction of policy which resulted in the ANZUS Treaty between New Zealand, America and Australia in 1951, which was to hold until disagreements over nuclear armaments decades later.
[edit] Maori Urbanisation

Many Māori fought in World War II, and many others moved from their rural homes to the cities to take up jobs vacated by Pākehā servicemen.[21] The shift to the cities was also caused by their strong birth rates in the early twentieth century, with the existing rural farms in Māori ownership having increasing difficulty in providing enough jobs.[21] Māori culture had meanwhile undergone a renaissance thanks in part to politician Apirana Ngata. World War II saw the beginning of a mass Māori migration to the cities, and by the 1980s 80% of the Māori population was urban, in contrast to only 20% before the war. The migration led to better pay, standards of living and education for most Māori, but also exposed problems of racism and discrimination. By the late 1960s, a protest movement had emerged to combat racism, promote Māori culture and seek fulfillment of the Treaty of Waitangi.

The urbanisation of the country was far from restricted to Māori. In the late 1940s, town planners noted that the country was "possibly the third most urbanised country in the world", with two thirds of the population living in cities or towns. There was also increasing concern that this trend was badly managed, with it being noted that there was an "ill-defined urban pattern that appears to have few of the truly desirable urban qualities and yet manifests no compensating rural characteristics."[22]
[edit] Post-war

The Māori protest movement was just one of several movements which emerged at this time to challenge the conservatism of mainstream New Zealand culture. This culture, and the country's economy, was based on being an offshoot of Britain. From the 1890s, the economy had been based almost entirely on the export of frozen meat and dairy products to Britain, and in 1961, the share of New Zealand exports going to the United Kingdom was still at slightly over 51%, with approximately 15% more going to other European countries.[23] This system was irreparably damaged by Britain joining the European Economic Community in 1973. Britain's accession to the European Community forced New Zealand to not only find new markets, but also re-examine its national identity and place in the world.

Robert Muldoon, Prime Minister from 1975 to 1984, and his Third National government responded to the crises of the 1970s by attempting to preserve the New Zealand of the 1950s. His conservatism and antagonistic style helped create an atmosphere of conflict in New Zealand, most violently expressed during the 1981 Springbok Tour. Some innovations did take place, for example the Closer Economic Relations agreement with Australia, and in 1983 the term "dominion" was replaced with "realm" by letters patent.
[edit] Reform

In 1984, the Fourth Labour government was elected. Propelled into office amid a constitutional and economic crisis, the new government embarked on a policy of restructuring, known as Rogernomics. This involved floating the New Zealand dollar, cutting government spending, reducing most taxes and introducing a sales tax (GST), and removing almost all industry subsidies. Although many of these changes improved the economy, they also created widespread unemployment, which was made worse by the 1987 stock market crash.

The Fourth Labour Government also revolutionised New Zealand's foreign policy, making the country a nuclear-free zone and effectively leaving the ANZUS alliance. Immigration policy was liberalised, allowing an influx of immigrants from Asia. Previously most immigrants to New Zealand had been European and especially British, apart from some migrants from other Pacific Islands such as Samoa. Other fourth Labour government innovations included greater recognition of the Treaty of Waitangi through the Waitangi Tribunal, Homosexual Law Reform, the Constitution Act 1986 and the New Zealand Bill of Rights.

Unhappy with the speed and extent of reforms, voters elected a National government in 1990, led by Jim Bolger. However the new government continued the economic reforms of the previous Labour government. Unhappy with what seemed to be a pattern of governments failing to reflect the mood of the electorate, New Zealanders voted to change the electoral system to Mixed Member Proportional (MMP), a form of proportional representation. New Zealand's first MMP election was held in 1996. Following the election National was returned to power in coalition with the New Zealand First Party.
[edit] New Zealand today

The Fifth Labour government led by Helen Clark was elected in 1999. It maintained most of the previous governments' economic reforms — restricting government intervention in the economy much more so than previous governments — while putting more of an emphasis on social policy and outcomes. For example, employment law was modified to give more protection to workers, and the student loan system was changed to eliminate interest payments for New Zealand resident students and graduates. Helen Clark's Labour government remained in power for nine years before being replaced in 2008 by New Zealand's fifth National government led by John Key.

New Zealand retains strong but informal links to Britain, with many young New Zealanders travelling to Britain for their "OE" (overseas experience) due to favourable working visa arrangements with Britain. Despite New Zealand's immigration liberalisation in the 1980s, Britons are still the largest group of migrants to New Zealand, due in part to recent immigration law changes which privilege fluent speakers of English. A few constitutional links to Britain remain — the New Zealand Sovereign is a British resident, for example. However, British imperial honours were discontinued in 1996, the Governor-General has taken a more active role in representing New Zealand overseas, and appeals from the Court of Appeal to the Judicial Committee of the Privy Council were replaced by a local Supreme Court of New Zealand in 2003. From time to time there is public debate about whether New Zealand should become a republic, and public sentiment is divided on the issue.

Foreign policy has been essentially independent since the mid 1980s. New Zealand contributed troops to the Afghanistan War, but did not contribute troops to the Iraq War although some medical and engineering units were sent.

For a developed country, New Zealand's economy is still very dependent on farming, although the old trinity of meat, dairy and wool has been supplemented by fruit, wine, timber and other products. Tourism is a major industry, and the country has been successful in attracting several major film productions, most notably the Lord of the Rings trilogy, directed by New Zealander Peter Jackson, which in turn bolstered New Zealand's tourism image.

History of South Africa
From Wikipedia, the free encyclopedia
  (Redirected from History of south africa)
Jump to:navigation, search
History of South Africa
Flag of South Africa.svg
This article is part of a series
General periods
Before 1652
1652 to 1815
1815 to 1910
1910 to 1948
1948 to 1994
1994 to present
Specific themes
Economics
Military
Religious
Social
South Africa Portal
 v • d • e 
Historical states
in present-day
South Africa
South Africa topo continent.png
[show]
 
before 1600
Mapungubwe (1050–1270)
[show]
 
1600-1700
Cape Colony (1652–1910)
[show]
 
1700-1800
Swellendam (1795)
Graaff Reinet (1795–1796)
[show]
 
1800-1850
Waterboer's Land (1813–1871)
Zulu Kingdom (1818–1897)
Adam Kok's Land (1825–1861)
Winburg (1836–1844)
Potchefstroom (1837–1848)
Natalia Republic (1839–1843)
[show]
 
1850-1875
Orange Free State (1854–1902)
Republic of Utrecht (1854–1858)
Lydenburg Republic (1856–1860)
South African Republic (1857–1902)
Griqualand East (1861–1879)
Griqualand West (1870)
[show]
 
1875-1900
Stellaland (1882–1885)
Goshen (1882–1883)
Nieuw Republiek (1884–1888)
Klein Vrystaat (1886–1891)
[show]
 
1900-present
Cape Colony (1652–1910)
Union of South Africa (1910–1961)
Transkei (1976–1994)
Bophuthatswana (1977–1994)
Venda (1979–1994)
Ciskei (1981–1994)
Republic of South Africa (1961–present)
more

The history of South Africa is marked by immigration and ethnic conflict. The Khoisan peoples are the aboriginal people of the region who have lived there for millennia. Black South Africans are believed to originate from the Great Lakes region of Africa in prehistoric times. White South Africans, descendants of later European migrations, regard themselves equally as products of South Africa, as do South Africa's Coloureds, Indians, Asians, and Jews.
Contents
[hide]

    * 1 Ancient and medieval history
          o 1.1 The San
          o 1.2 Bantu expansion
          o 1.3 Mapungubwe and the rise of Thulamela
    * 2 Colonization
          o 2.1 European expeditions
          o 2.2 Arrival of the Dutch
          o 2.3 Burgher expansion
    * 3 British at the Cape
          o 3.1 Difaqane and destruction
          o 3.2 The Great Trek
          o 3.3 British, Boers and Zulus
    * 4 Growth of independent South Africa
          o 4.1 The Boer republics
    * 5 Anglo-Boer Wars
          o 5.1 First Anglo-Boer War
          o 5.2 Inter-war period
          o 5.3 Second Anglo-Boer War
    * 6 Union of South Africa
    * 7 World War I
          o 7.1 Military action against Germany during World War I
          o 7.2 Military contributions and casualties in World War I
    * 8 World War II
          o 8.1 Political choices at outbreak of war
          o 8.2 Declaration of war against the Axis
          o 8.3 Prime Minister and Field Marshal Smuts
          o 8.4 Military contributions and casualties in World War II
    * 9 Aftermath of World War II
    * 10 General elections and the slow evolution of democracy
    * 11 Apartheid era
          o 11.1 Afrikaner nationalism
          o 11.2 Legalised discrimination
          o 11.3 Dismantling
    * 12 Truth and Reconciliation Commission
    * 13 Late-1990s
    * 14 See also
    * 15 Further reading
    * 16 References
          o 16.1 Footnotes
    * 17 External links

[edit] Ancient and medieval history
[edit] The San
Main article: Ancient History of South Africa
Question book-new.svg
	This section needs additional citations for verification.
Please help improve this article by adding reliable references. Unsourced material may be challenged and removed. (April 2010)


Beginning around 500 BC, some San groups acquired livestock from further north. Gradually, hunting and gathering gave way to herding as the dominant economic activity as these San People tended to small herds of cattle and oxen. The arrival of livestock introduced concepts of personal wealth and property-ownership into San society. Community structures solidified and expanded, and chieftaincies developed. These pastoralist San People became known as Khoikhoi ('men of men'), as opposed to the still hunter-gatherer San People, whom the Colonialist Settlers referred to as Bushmen. At the point where the two groups became intermarried, mixed and hard to tell apart, the term Khoisan arose. Over time the Khoikhoi established themselves along the coast, while small groups of San continued to inhabit the interior.
[edit] Bantu expansion
Main article: Bantu expansion
Question book-new.svg
	This section needs additional citations for verification.
Please help improve this article by adding reliable references. Unsourced material may be challenged and removed. (April 2010)

Around 2,500 years ago Bantu peoples starting migrating across sub-Saharan Africa from the Niger River Delta. The San People of Southern Africa and the Bantu-speakers lived mostly peacefully together, although since neither had any method of writing, researchers know little of this period outside of archaeological artefacts

The Bantu-speakers had started to make their way south and eastwards in about 1000 BC, reaching the present-day KwaZulu-Natal Province by 500 CE. The Bantu-speakers had an advanced Iron Age culture, keeping domestic animals and also practising agriculture, farming sorghum and other crops. They lived in small settled villages. The Bantu-speakers arrived in South Africa in small waves rather than in one cohesive migration. Some groups, the ancestors of today's Nguni peoples (the Zulu, Xhosa, Swazi, and Ndebele), preferred to live near the coast. Others, now known as the Sotho-Tswana peoples (Tswana, Pedi, and Basotho), settled in the Highveld, while today's Venda, Lemba, and Shangaan-Tsonga peoples made their homes in the north-eastern areas of South Africa

Bantu-speakers and Khoisan mixed, as evidenced by rock paintings showing the two different groups interacting. The type of contact remains unknown, although linguistic proof of integration survives, as several Southern Bantu languages (notably Xhosa and Zulu) incorporated many click consonants of earlier Khoisan languages. Archaeologists have found numerous Khoisan artefacts at the sites of Bantu settlements
[edit] Mapungubwe and the rise of Thulamela
Question book-new.svg
	This section needs additional citations for verification.
Please help improve this article by adding reliable references. Unsourced material may be challenged and removed. (April 2010)

From around 1200 AD a trade network began to emerge just to the North as is evidenced at such sites as Mapungubwe. Additionally, the idea of sacred leadership emerged – concept that transcends English terms such as “Kings” or “Queens”.[1] Sacred leaders were elite members of the community, types of prophets, people with supernatural powers and the ability to predict the future.
Looking out over the floodplains of the Luvuvhu River (right) and the Limpopo River (Far distance and left).

Through interactions and trade with Muslim traders plying the Indian ocean as far south as present day Mozambique – the region emerged as a trade centre producing gold and ivory and trading for glass beads and porcelain from as far away as China[1].
[edit] Colonization
Main article: History of South Africa (1652-1815)
[edit] European expeditions
Main article: History of Cape Colony
Bartolomeu Dias rounding the Cape of Good Hope.

Although the Portuguese basked in the nautical achievement of successfully navigating the cape, they showed little interest in colonization. The area's fierce weather and rocky shoreline posed a threat to their ships, and many of their attempts to trade with the local Khoikhoi ended in conflict. The Portuguese found the Mozambican coast more attractive, with appealing bays to use as way stations, prawns, and links to gold ore in the interior.

The Portuguese had little competition in the region until the late 16th century, when the English and Dutch began to challenge the Portuguese along their trade routes. Stops at the continent's southern tip increased, and the cape became a regular stopover for scurvy-ridden crews. In 1647, a Dutch vessel was wrecked in the present-day Table Bay at Cape Town. The marooned crew, the first Europeans to attempt settlement in the area, built a fort and stayed for a year until they were rescued.
[edit] Arrival of the Dutch
Painting of an account of the arrival of Jan van Riebeeck, by Charles Bell.

Shortly thereafter, the Dutch East India Company (in the Dutch of the day: Vereenigde Oostindische Compagnie, or VOC) decided to establish a permanent settlement. The VOC, one of the major European trading houses sailing the spice route to the East, had no intention of colonising the area, instead wanting only to establish a secure base camp where passing ships could shelter, and where hungry sailors could stock up on fresh supplies of meat, fruit, and vegetables. To this end, a small VOC expedition under the command of Jan van Riebeeck reached Table Bay on 6 April 1652.[2]

While the new settlement traded out of necessity with the neighbouring Khoikhoi, it was not a friendly relationship, and the company authorities made deliberate attempts to restrict contact. Partly as a consequence, VOC employees found themselves faced with a labour shortage. To remedy this, they released a small number of Dutch from their contracts and permitted them to establish farms, with which they would supply the VOC settlement from their harvests. This arrangement proved highly successful, producing abundant supplies of fruit, vegetables, wheat, and wine; they also later raised livestock. The small initial group of free burghers, as these farmers were known, steadily increased in number and began to expand their farms further north and east into the territory of the Khoikhoi.

The majority of burghers had Dutch ancestry and belonged to the Calvinist Reformed Church of the Netherlands, but there were also numerous Germans as well as some Scandinavians. In 1688 the Dutch and the Germans were joined by French Huguenots, also Calvinists, who were fleeing religious persecution in France under King Louis XIV.

In addition to establishing the free burgher system, van Riebeeck and the VOC also began to import large numbers of slaves, primarily from Madagascar and Indonesia. These slaves often married Dutch settlers, and their descendants became known as the Cape Coloureds and the Cape Malays. A significant number of the offspring from the White and slave unions were absorbed into the local proto-Afrikaans speaking White population. With this additional labour, the areas occupied by the VOC expanded further to the north and east, with inevitable clashes with the Khoikhoi. The newcomers drove the Khoikhoi from their traditional lands, decimated them with introduced diseases, and destroyed them with superior weapons when they fought back, which they did in a number of major wars and with guerilla resistance movements that continued into the 19th century. Most survivors were left with no option but to work for the Europeans in an exploitative arrangement that differed little from slavery.[citation needed] Over time, the Khoisan, their European overseers, and the imported slaves mixed, with the offspring of these unions forming the basis for today's Coloured population.

The best-known Khoikhoi groups included the Griqua, who had originally lived on the western coast between St Helena Bay and the Cederberg Range. In the late 18th century, they managed to acquire guns and horses and began trekking north-east. En route, other groups of Khoisan, Coloureds, and even white adventurers joined them, and they rapidly gained a reputation as a formidable military force. Ultimately, the Griquas reached the Highveld around present-day Kimberley, where they carved out territory that came to be known as Griqualandalina.
[edit] Burgher expansion
An account of the first trekboers.

As the burghers, too, continued to expand into the rugged hinterlands of the north and east, many began to take up a semi-nomadic pastoralist lifestyle, in some ways not far removed from that of the Khoikhoi they displaced. In addition to its herds, a family might have a wagon, a tent, a Bible, and a few guns. As they became more settled, they would build a mud-walled cottage, frequently located, by choice, days of travel from the nearest European settlement. These were the first of the Trekboers (Wandering Farmers, later shortened to Boers), completely independent of official controls, extraordinarily self-sufficient, and isolated. Their harsh lifestyle produced individualists who were well acquainted with the land. Like many pioneers with Christian backgrounds, the burghers attempted to live their lives - and to construct a theocracy - based on their particular Christian denomination's (Dutch Reformed Church) reading into eisegesis characters and plot found in the Hebrew scriptures (as distinct from the Christian Gospels and Epistles).
[edit] British at the Cape
Main article: History of South Africa (1815-1910)

As the 18th century drew to a close, Dutch mercantile power began to fade and the British moved in to fill the vacuum. They seized the Cape in 1795 to prevent it from falling into the hands of Napoleonic France, then briefly relinquished it back to the Dutch (1803), before definitively conquering it in 1806. British sovereignty of the area was recognised at the Congress of Vienna in 1815.

At the tip of the continent the British found an established colony with 25,000 slaves, 20,000 white colonists, 15,000 Khoisan, and 1,000 freed black slaves. Power resided solely with a white élite in Cape Town, and differentiation on the basis of race was deeply entrenched. Outside Cape Town and the immediate hinterland, isolated black and white pastoralists populated the country.

Like the Dutch before them, the British initially had little interest in the Cape Colony, other than as a strategically located port. As one of their first tasks they tried to resolve a troublesome border dispute between the Boers and the Xhosa on the colony's eastern frontier. In 1820 the British authorities persuaded about 5,000 middle-class British immigrants (most of them "in trade") to leave Great Britain behind and settle on tracts of land between the feuding groups with the idea of providing a buffer zone. The plan was singularly unsuccessful. Within three years, almost half of these 1820 Settlers had retreated to the towns, notably Grahamstown and Port Elizabeth, to pursue the jobs they had held in Britain.

While doing nothing to resolve the border dispute, this influx of settlers solidified the British presence in the area, thus fracturing the relative unity of white South Africa. Where the Boers and their ideas had before gone largely unchallenged, white South Africa now had two distinct language groups and two distinct cultures. A pattern soon emerged whereby English-speakers became highly urbanised, and dominated politics, trade, finance, mining, and manufacturing, while the largely uneducated Boers were relegated to their farms.

The gap between the British settlers and the Boers further widened with the abolition of slavery in 1834, a move that the Boers generally regarded as against the God-given ordering of the races. Yet the British settlers' conservatism stopped any radical social reforms, and in 1841 the authorities passed a Masters and Servants Ordinance, which perpetuated white control. Meanwhile, numbers of British immigrants increased rapidly in Cape Town, in the area east of the Cape Colony (present-day Eastern Cape Province), in Natal. The discovery of diamonds at Kimberley and the subsequent discovery of gold in parts of the Transvaal, mainly around present-day Gauteng led to a rapid increase in immigration of fortune seekers from all parts of the globe, including Africa itself.[3]
[edit] Difaqane and destruction
Main article: Difaqane
Shaka Zulu in traditional Zulu military garb.

The early 19th century saw a time of immense upheaval relating to the military expansion of the Zulu Kingdom. Sotho-speakers know this period as the difaqane ("forced migration"); while Zulu-speakers call it the mfecane ("crushing").

The full causes of the difaqane remain in dispute, although certain factors stand out. The rise of a unified Zulu kingdom had particular significance. In the early 19th century, Nguni tribes in KwaZulu-Natal began to shift from a loosely-organised collection of kingdoms into a centralised, militaristic state. Shaka Zulu, son of the chief of the small Zulu clan, became the driving force behind this shift. At first something of an outcast, Shaka proved himself in battle and gradually succeeded in consolidating power in his own hands. He built large armies, breaking from clan tradition by placing the armies under the control of his own officers rather than of the hereditary chiefs. Shaka then set out on a massive programme of expansion, killing or enslaving those who resisted in the territories he conquered. His impis (warrior regiments) were rigorously disciplined: failure in battle meant death.

Peoples in the path of Shaka's armies moved out of his way, becoming in their turn aggressors against their neighbours. This wave of displacement spread throughout Southern Africa and beyond. It also accelerated the formation of several states, notably those of the Sotho (present-day Lesotho) and of the Swazi (now Swaziland).

In 1828 Shaka was killed by his half-brothers Dingaan and Umhlangana. The weaker and less-skilled Dingaan became king, relaxing military discipline while continuing the despotism. Dingaan also attempted to establish relations with the British traders on the Natal coast, but events had started to unfold that would see the demise of Zulu independence.
[edit] The Great Trek
Trekboers on the karoo.
Main article: Great Trek

Meanwhile, the Boers had started to grow increasingly dissatisfied with British rule in the Cape Colony. The British proclamation of the equality of the races particularly angered them, and they were also unhappy with the process of payment of compensation for slave-owners whose slaves had been freed. Beginning in 1835, several groups of Boers, together with large numbers of Khoikhoi and black servants, decided to trek off into the interior in search of greater independence. North and east of the Orange River (which formed the Cape Colony's frontier) these Boers or Voortrekkers ("Pioneers") found vast tracts of apparently uninhabited grazing lands. They had, it seemed, entered their promised land, with space enough for their cattle to graze and their culture of anti-urban independence to flourish. Little did they know that what they found - deserted pasture lands, disorganised bands of refugees, and tales of brutality - resulted from the difaqane, rather than representing the normal state of affairs.

With the exception of the more powerful Ndebele, the Voortrekkers encountered little resistance among the scattered peoples of the plains. The difaqane had dispersed them, and the remnants lacked horses and firearms. Their weakened condition also solidified the Boers' belief that European occupation meant the coming of civilisation to a savage land. However, the mountains where King Moshoeshoe I had started to forge the Basotho nation that would later become Lesotho and the wooded valleys of Zululand proved a more difficult proposition. Here the Boers met strong resistance, and their incursions set off a series of skirmishes, squabbles, and flimsy treaties that would litter the next 50 years of increasing white domination.
[edit] British, Boers and Zulus
Indians arriving in Durban for the first time.

The Great Trek first halted at Thaba Nchu, near present-day Bloemfontein, where the trekkers established a republic. Following disagreements among their leadership, the various Voortrekker groups split apart. While some headed north, most crossed the Drakensberg into Natal with the idea of establishing a republic there.

Since the Zulus controlled this territory, the Voortrekker leader, accompanied by about 70 men of his Trek-Boer community, Piet Retief paid a visit to King Dingane kaSenzangakhona (Shaka's brother). Dingane promised them land in payment for a favour. The Batlokwa people, under chief Sekonyela had stolen cattle from him and he wanted it back. Retief went to them and retrieved the cattle. After receiving the specified cattle, Dingane invited Retief and his men into his kraal, where they were given all the land between the iZimvubu and Tugela rivers up to the Drakensberg. The treaty between the two men currently sits in a museum in The Netherlands. As a celebration, Dingane invited Retief and all his men to come and drink uTshwala (Traditional Zulu Beer) in his kraal. Also including with the offer guns and money. While drinking and being entertained by Zulu dancers, Dingane cried out "Bulalani abathakathi" (Kill the wizards"; also sometimes reported as "Bambani abathakathi", "Seize the wizards"). Dingane's men, having taken Retief's men by surprise, dragged the men to a hill Hloma Mabuto (or perhaps kwaMatiwane) where, one by one, they were all killed, leaving Retief for last so that he could watch. One proposed reason for their killing is that, for some reason, they had withheld some of the recovered cattle.

After the massacre, the impis went back to the encampment where Retief and his fellow farmers had left their wives, children and livestock. Taken by surprise, the women, children and remaining farmers (numbering about 500) were also killed at the site called "Weenen", but not without retribution, they themselves managed to stop the initial onslaught and managed to get away, without many of their guns and animals. A missionary, Rev. Owen, had seen all of this take place and approached Dingane in order to give the dead an appropriate burial. While the reverend and a helper of his were burying the dead and reading them their last rights, they happened to come across Retief's rucksack, still containing the treaty and a few personal belongings.

At the Battle of Itala, a Boer army's attempt at revenge failed miserably.[4] The culmination came on 16 December 1838, at the Ncome River in Natal. The Boers established a defensive enclosure or laager before the Zulu attack. Though only three Boers suffered injuries, they killed about three thousand Zulu warriors using three cannons and an elephant gun (along with other weapons) to their advantage in this massive slaughter that in the 1920s became a South African holiday. So much bloodshed reportedly caused the Ncome's waters to run red, thus the clash is historically known as the Battle of Blood River.
Zulu warriors, late 19th century

The Voortrekkers, victorious despite their numbers, saw their victory as an affirmation of divine approval. Yet their hopes for establishing a Natal republic remained short lived. The British annexed the area in 1843, and founded their new Natal colony at present-day Durban. Most of the Boers, feeling increasingly squeezed between the British on one side and the native African populations on the other, headed north.

The British set about establishing large sugar plantations in Natal, but found few inhabitants of the neighbouring Zulu areas willing to provide labour. The British confronted stiff resistance to their encroachments from the Zulus, a nation with well-established traditions of waging war, who inflicted one of the most humiliating defeats on the British army at the Battle of Isandlwana in 1879, where over 1400 British soldiers were killed. During the ongoing Anglo-Zulu Wars, the British eventually established their control over what was then named Zululand, and is today known as KwaZulu-Natal Province.

The British turned to India to resolve their labour shortage, as Zulu men refused to adopt the servile position of labourers and in 1860 the SS Truro arrived in Durban harbour with over 300 people on board. Over the next 50 years, 150,000 more indentured Indians arrived, as well as numerous free "passenger Indians", building the base for what would become the largest Indian community outside of India. As early as 1893, when Mahatma Gandhi arrived in Durban, Indians outnumbered whites in Natal. (See Asians in South Africa.)
[edit] Growth of independent South Africa
[edit] The Boer republics
Main articles: South African Republic and Orange Free State
The farm outside of Johannesburg on the Witwatersrand - site of the first discovery of gold in 1886.

The Boers meanwhile persevered with their search for land and freedom, ultimately establishing themselves in various Boer Republics, e.g. the Transvaal or South African Republic and the Orange Free State. For a while it seemed that these republics would develop into stable states, despite having thinly-spread populations of fiercely independent Boers, no industry, and minimal agriculture. The discovery of diamonds near Kimberley turned the Boers' world on its head (1869). The first diamonds came from land belonging to the Griqua, but to which both the Transvaal and Orange Free State laid claim. Britain quickly stepped in and resolved the issue by annexing the area for itself.

The discovery of the Kimberley diamond-mines unleashed a flood of European and black labourers into the area. Towns sprang up in which the inhabitants ignored the "proper" separation of whites and blacks, and the Boers expressed anger that their impoverished republics had missed out on the economic benefits of the mines.
[edit] Anglo-Boer Wars
Main article: Boer Wars
The Relief of Ladysmith. Sir George Stuart White greets Major Hubert Gough on 28 February. Painting by John Henry Frederick Bacon (1868-1914)
Boer women and children in a concentration camp.
[edit] First Anglo-Boer War
Main article: First Boer War

Long-standing Boer resentment turned into full-blown rebellion in the Transvaal (under British control from 1877), and the first Anglo-Boer War, known to Afrikaners as the "War of Independence", broke out in 1880. The conflict ended almost as soon as it began with a crushing Boer victory at Battle of Majuba Hill (27 February 1881). The republic regained its independence as the Zuid-Afrikaansche Republiek ("South African Republic"), or ZAR. Paul Kruger, one of the leaders of the uprising, became President of the ZAR in 1883. Meanwhile, the British, who viewed their defeat at Majuba as an aberration, forged ahead with their desire to federate the Southern African colonies and republics. They saw this as the best way to come to terms with the fact of a white Afrikaner majority, as well as to promote their larger strategic interests in the area.
[edit] Inter-war period

In 1879 Zululand came under British control. Then in 1886 an Australian prospector discovered gold in the Witwatersrand, accelerating the federation process and dealing the Boers yet another blow. Johannesburg's population exploded to about 100,000 by the mid-1890s, and the ZAR suddenly found itself hosting thousands of uitlanders, both black and white, with the Boers squeezed to the sidelines. The influx of Black labour in particular worried the Boers, as the shortage of jobs meant that they would suffer further economic hardships.

The enormous wealth of the mines, largely controlled by European "Randlords" soon became irresistible for the British. In 1895, a group of renegades led by Captain Leander Starr Jameson entered the ZAR with the intention of sparking an uprising on the Witwatersrand and installing a British administration. This incursion became known as the Jameson Raid. The scheme ended in fiasco, but it seemed obvious to Kruger that it had at least the tacit approval of the Cape Colony government, and that his republic faced danger. He reacted by forming an alliance with Orange Free State.
[edit] Second Anglo-Boer War
Main article: Second Boer War
Boer guerillas during the Second Boer War.

The situation peaked in 1899 when the British demanded voting rights for the 60,000 foreign whites on the Witwatersrand. Until that point, Kruger's government had excluded all foreigners from the franchise. Kruger rejected the British demand and called for the withdrawal of British troops from the ZAR's borders. When the British refused, Kruger declared war. This Second Anglo-Boer War lasted longer than the first, and the British preparedness surpassed that of Majuba Hill. By June 1900, Pretoria, the last of the major Boer towns, had surrendered. Yet resistance by Boer bittereinders continued for two more years with guerilla-style battles, which the British met in turn with scorched earth tactics. By 1902 26,000 Boers had died of disease and neglect in concentration camps. On 31 May 1902 a superficial peace came with the signing of the Treaty of Vereeniging. Under its terms, the Boer republics acknowledged British sovereignty, while the British in turn committed themselves to reconstruction of the areas under their control.
[edit] Union of South Africa
Main article: Union of South Africa
Johannesburg around 1890

During the immediate post-war years the British focussed their attention on rebuilding the country, in particular the mining industry. By 1907 the mines of the Witwatersrand produced almost one-third of the world's annual gold production. But the peace brought by the treaty remained fragile and challenged on all sides. The Afrikaners found themselves in the ignominious position of poor farmers in a country where big mining ventures and foreign capital rendered them irrelevant. Britain's unsuccessful attempts to Anglicise them, and to impose English as the official language in schools and the workplace particularly incensed them. Partly as a backlash to this, the Boers came to see Afrikaans as the volkstaal ("people's language") and as a symbol of Afrikaner nationhood. Several nationalist organisations sprang up.

The system left Blacks and Coloureds completely marginalised. The authorities imposed harsh taxes and reduced wages, while the British caretaker administrator encouraged the immigration of thousands of Chinese to undercut any resistance. Resentment exploded in the Bambatha Rebellion of 1906, in which 4,000 Zulus lost their lives after protesting against onerous tax legislation.

The British meanwhile moved ahead with their plans for union. After several years of negotiations, the South Africa Act 1909 brought the colonies and republics - Cape Colony, Natal, Transvaal, and Orange Free State - together as the Union of South Africa. Under the provisions of the act, the Union remained British territory, but with home-rule for Afrikaners. The British High Commission territories of Basutoland (now Lesotho), Bechuanaland (now Botswana), Swaziland, and Rhodesia (now Zambia and Zimbabwe) continued under direct rule from Britain.

English and Dutch became the official languages. Afrikaans did not gain recognition as an official language until 1925. Despite a major campaign by Blacks and Coloureds, the voter franchise remained as in the pre-Union republics and colonies, and only whites could gain election to parliament.

Most significantly, the new Union of South Africa gained international respect with British Dominion status putting it on par with three other important British dominions and allies: Canada, Australia, and New Zealand.

The Natives' Land Act of 1913[5] was the first major piece of segregation legislation passed by the Union Parliament, and remained a cornerstone of Apartheid until the 1990s when it was replaced by the current policy of land restitution. Under the act, blacks were severely restricted in the ownership of land, at that stage to a mere 7% of the country, although this amount was eventually increased marginally. The Act created a system of land tenure that deprived the majority of South Africa's inhabitants of the right to own land which had major socio-economic repercussions.

British segregationist legislation also included the Franchise and Ballot Act (1892), which limited the black vote by finance and education, the Natal Legislative Assembly Bill (1894), which deprived Indians of the right to vote; the General Pass Regulations Bill (1905), which denied blacks the vote altogether, limited them to fixed areas and inaugurated the infamous Pass System; the Asiatic Registration Act (1906) requiring all Indians to register and carry passes; the South Africa Act (1910) that enfranchised whites, giving them complete political control over all other race groups; the above-mentioned Native Land Act (1913) which prevented all blacks, except those in the Cape, from buying land outside 'reserves' and effectively stole 87% of their land; the Natives in Urban Areas Bill (1918) designed to force blacks into 'locations'; the Urban Areas Act (1923) which introduced residential segregation in South Africa and provided cheap labour for the white mining and farming industry; the Colour Bar Act (1926), preventing blacks from practising skilled trades; the Native Administration Act (1927) that made the British Crown, rather than paramount chiefs, the supreme head over all African affairs; the Native Land and Trust Act (1936) that complemented the 1913 Native Land Act and, in the same year, the Representation of Natives Act, which removed blacks from the Cape voters' roll. The final 'apartheid' legislation by the British was the Asiatic Land Tenure Bill (1946), which banned any further land sales to Indians. (This para. quoted with permission from Apartheid South Africa: An Insider's Overview of the Origin and Effects of Separate Development, by John Allen.
[edit] World War I
Main article: History of South Africa (1910-1948)

The Union of South Africa was a British Domain and automatically joined with Great Britain and the allies against the German Empire. Both Prime Minister Louis Botha and Defence Minister of South Africa were part of significant military operations against Germany. In spite of Boer resistance at home, the Afrikaner-led government of Louis Botha unhesitatingly joined the side of the Allies of World War I and fought alongside its armies. The South African Government agreed to the withdrawal of British Army units so that they were free to join the European war, and laid plans to invade German South-West Africa. Elements of the South African army refused to fight against the Germans and along with other opponents of the Government rose in open revolt. The government declared martial law on 14 October 1914, and forces loyal to the government under the command of General Louis Botha and Jan Smuts proceeded to destroy the Maritz Rebellion. The leading Boer rebels got off lightly with terms of imprisonment of six-seven years and heavy fines. (See World War I and the Maritz Rebellion.)
[edit] Military action against Germany during World War I

The South African Union Defence Force saw action in a number of areas:

   1. It dispatched its army to German South-West Africa (later known as South West Africa and now known as Namibia). The South Africans expelled German forces and gained control of the former German colony. (See German South-West Africa in World War I.)
   2. A military expedition under General Jan Smuts was dispatched to German East Africa (later known as Tanganyika and now known as Tanzania). The objective was to fight German forces in that colony and to try to capture the elusive German General von Lettow-Vorbeck. Ultimately, Lettow-Vorbeck fought his tiny force out of German East Africa into Mozambique, where he surrendered a few weeks after the end of the war. (See German East Africa in First World War.)
   3. 1st South African Brigade troops were shipped to France to fight on the Western Front. The most costly battle that the South African forces on the Western Front fought in was the Battle of Delville Wood in 1916. (See South African Army in World War I.)
   4. South Africans also saw action with the Cape Corps as part of the Egyptian Expeditionary Force in Palestine. (See Cape Corps 1915 - 1991.)

[edit] Military contributions and casualties in World War I

More than 146,000 whites, 83,000 blacks and 2,500 people of mixed race ("Coloureds") and Asians served in South African military units during the war, including 43,000 in German South-West Africa and 30,000 on the Western Front. An estimated 3,000 South Africans also joined the Royal Flying Corps. The total South African casualties during the war was about 18,600 with over 12,452 killed - more than 4,600 in the European theatre alone.
The British Empire is red on the map, at its zenith in 1919. (India highlighted in purple.) South Africa, bottom centre, lies between both halves of the Empire.

There is no question that South Africa greatly assisted the Allies, and Great Britain in particular, in capturing the two German colonies of German-West-Africa and German-East-Africa as well as in battles in Western Europe and the Middle East. South Africa's ports and harbours, such as at Cape Town, Durban, and Simon's Town, were also important rest-stops, refuelling-stations, and served as strategic assets to the British Royal Navy during the war, helping to keep the vital sea lanes to the British Raj open.
[edit] World War II
[edit] Political choices at outbreak of war

On the eve of World War II the Union of South Africa found itself in a unique political and military quandary. While it was closely allied with Great Britain, being a co-equal Dominion under the 1931 Statute of Westminster with its head of state being the British king, the South African Prime Minister on September 1, 1939, was Barry Hertzog, the leader of the pro-Afrikaner, anti-British National Party that had joined in a unity government as the United Party.

Hertzog's problem was that South Africa was constitutionally obliged to support Great Britain against Nazi Germany. The Polish-British Common Defence Pact obligated Britain, and in turn its dominions, to help Poland if attacked by the Nazis. After Hitler's forces attacked Poland on the night of August 31, 1939, Britain declared war on Germany within a few days. A short but furious debate unfolded in South Africa, especially in the halls of power in the Parliament of South Africa, that pitted those who sought to enter the war on Britain's side, led by the pro-Allied, pro-British Afrikaner, ex-General, and former Prime Minister Jan Smuts against then-current Prime Minister Barry Hertzog who wished to keep South Africa "neutral," if not pro-Axis.
[edit] Declaration of war against the Axis

On September 4, 1939 the United Party caucus refused to accept Hertzog's stance of neutrality in World War II and deposed him in favour of Smuts. Upon becoming Prime Minister of South Africa, Smuts declared South Africa officially at war with Germany and the Axis. Smuts immediately set about fortifying South Africa against any possible German sea invasion because of South Africa's global strategic importance controlling the long sea route around the Cape of Good Hope.

Smuts took severe action against the pro-Nazi South African Ossewabrandwag movement (they were caught committing acts of sabotage) and jailed its leaders for the duration of the war. (One of them, John Vorster, was to become future Prime Minister of South Africa.) (See Jan Smuts during World War II.)
[edit] Prime Minister and Field Marshal Smuts
Main article: Jan Smuts

Prime Minister Jan Smuts was the only important non-British general whose advice was constantly sought by Britain's war-time Prime Minister Winston Churchill. Smuts was invited to the Imperial War Cabinet in 1939 as the most senior South African in favour of war. In 28 May 1941, Smuts was appointed a Field Marshal of the British Army, becoming the first South African to hold that rank. Ultimately, Smuts would pay a steep political price for his closeness to the British establishment, to the King, and to Churchill which had made Smuts very unpopular among the conservative nationalistic Afrikaners, leading to his eventual downfall, whereas most English-speaking whites and a minority of liberal Afrikaners in South Africa remained loyal to him. (See Jan Smuts during World War II.)
[edit] Military contributions and casualties in World War II

South Africa and its military forces contributed in many theatres of war. South Africa's contribution consisted mainly of supplying troops, men and material for the North African campaign (the Desert War) and the Italian Campaign as well as to Allied ships that docked at its crucial ports adjoining the Atlantic Ocean and Indian Ocean that converge at the tip of Southern Africa. Numerous volunteers also flew for the Royal Air Force, while the South African Air Force made a major contribution to the air war in the Mediterranean, Middle East and African theatres of World War II. (See: South African Army in World War II; South African Air Force in World War II; South African Navy in World War II; South Africa's contribution in World War II.)

   1. The South African Army and Air Force helped defeat the Italian army of the Fascist Benito Mussolini that had invaded Abyssinia (now known as Ethiopia) in 1935. During the 1941 East African Campaign South African forces made important contribution to this early Allied victory.
   2. Another important victory that the South Africans participated in was the liberation of Malagasy (now known as Madagascar) from the control of the Vichy French who were allies of the Nazis. British troops aided by South African soldiers, staged their attack from South Africa, occupied the strategic island in 1942 to preclude its seizure by the Japanese.
   3. The South African 1st Infantry Division took part in several actions in East and North Africa in 1941 and 1942, including the Battle of El Alamein, before being withdrawn to South Africa.
   4. The South African 2nd Infantry Division also took part in a number of actions in North Africa during 1942, but on 21 June 1942 two complete infantry brigades of the division as well as most of the supporting units were captured at the fall of Tobruk.
   5. The South African 3rd Infantry Division never took an active part in any battles but instead organised and trained the South African home defence forces, performed garrison duties and supplied replacements for the 1st and 2nd Divisions. However, one of this division's constituent brigades - 7 SA Motorised Brigade - did take part in the invasion of Madagascar in 1942.
   6. The South African 6th Armoured Division fought in numerous actions in Italy from 1944 to 1945.
   7. South Africa contributed to the war effort against Japan, supplying men and manning ships in naval engagements against the Japanese.[6]

Of the 334,000 men volunteered for full time service in the South African Army during the war (including some 211,000 whites, 77,000 blacks and 46,000 "coloureds" and Asians), nearly 9,000 were killed in action.

However, not all South Africans supported the war effort. The Anglo-Boer war had ended only thirty five years earlier and to some, siding with the "enemy" was considered disloyal and unpatriotic. These sentiments gave rise to "The Ossewabrandwag" ("Oxwagon Sentinel"), originally created as a cultural organisation on the Centenary of the Great Trek becoming more militant and openly opposing South African entry into the war on side of the British. The organisation created a paramilitary group called Stormjaers ('storm chasers'), modelled on the Nazi SA or Sturmabteilung ("Storm Division") and which was linked to the German Intelligence (Abwehr) and the German Foreign Office (Dienstelle Ribbentrop) via Dr. Luitpold Werz - the former German Consul in Pretoria. The Stormjaers carried out a number of sabotage attacks against the Smuts government and actively tried to itimidate and discourage volunteers from joining the army recruitment programs.[7] Many members of the Ossewabrandwag were incarcerated during the war, amongst them - John Vorster, who would later become Prime Minister. After the war, the Ossewabrandwag went underground and a number of its erstwhile members, including future South African State President P.W. Botha, went on to rise in the ranks of the Apartheid government.
[edit] Aftermath of World War II

South Africa emerged from the Allied victory with its prestige and national honour enhanced as it had fought tirelessly for the Western Allies. South Africa's standing in the international community was rising, at a time when the Third World's struggle against colonialism had still not taken centre stage. In May 1945, Prime Minister Smuts represented South Africa in San Francisco at the drafting of the United Nations Charter. Just as he did in 1919, Smuts urged the delegates to create a powerful international body to preserve peace; he was determined that, unlike the League of Nations, the United Nations would have teeth. Smuts signed the Paris Peace Treaty, resolving the peace in Europe, thus becoming the only signatory of both the treaty ending the First World War, and that ending the Second.

However, internal political struggles in the disgruntled and essentially impoverished Afrikaner community would soon come to the fore leading to Smuts' defeat at the polls in the 1948 elections (in which only whites and coloureds could vote) at the hands of a resurgent National Party after the war. This began the road to South Africa's eventual isolation from a world that would no longer tolerate any forms of political discrimination or differentiation based on race only.
[edit] General elections and the slow evolution of democracy
Main articles: Elections in South Africa and South African general elections

From 1910 until the same time, a series of important general elections have been held in a united South Africa. From 1910 until 1948 the franchise to vote was given to whites and to Cape Coloureds (people of mixed race) only. After the ascent of the Nationalist Party in 1948, the Cape Coloureds were taken off the voters' role. Only eligible whites were permitted to vote from 1948 until 1994 when the vote was granted to South Africans of every racial group. The 1994 general election was the first post-apartheid vote based on universal suffrage.

There have been three referendums in South Africa: 1960 referendum on becoming a republic; 1983 referendum on implementing the tricameral parliament; and 1992 referendum on becoming a multiracial democracy all of which were held during the era of Nationalist Party control.
[edit] Apartheid era
Apartheid in South Africa
Events and Projects

Sharpeville massacre
Soweto uprising · Treason Trial
Rivonia Trial
Church Street bombing · CODESA
St James Church massacre
Cape Town peace march
Organisations

ANC · IFP · AWB · Black Sash · CCB
Conservative Party · ECC · PP · RP
PFP · HNP · MK · PAC · SACP · UDF
Broederbond · National Party
COSATU · SADF · SAP
People

P. W. Botha · D. F. Malan
Nelson Mandela
Desmond Tutu · F. W. de Klerk
Walter Sisulu · Helen Suzman
Harry Schwarz · Andries Treurnicht
H. F. Verwoerd · Oliver Tambo
B. J. Vorster · Kaiser Matanzima
Jimmy Kruger · Steve Biko
Mahatma Gandhi · Joe Slovo
Trevor Huddleston · Hector Pieterson
Places

Bantustan · District Six · Robben Island
Sophiatown · South-West Africa
Soweto · Sun City · Vlakplaas
Other aspects

Afrikaner nationalism
Apartheid laws · Freedom Charter
Sullivan Principles · Kairos Document
Disinvestment campaign
South African Police
This box: view • talk • edit
Main article: History of South Africa (1948 to 1994)
Main article: South Africa under apartheid
[edit] Afrikaner nationalism
Main article: Afrikaner nationalism

General Louis Botha headed the first government of the new Union, with General Jan Smuts as his deputy. Their South African National Party, later known as the South African Party or SAP, followed a generally pro-British, white-unity line. The more radical Boers split away under the leadership of General Barry Hertzog, forming the National Party (NP) in 1914. The NP championed Afrikaner interests, advocating separate development for the two white groups and independence from Britain.

The new Union had no place for Blacks, despite their constituting over 75 percent of the population. The Act of Union denied them voting-rights in the Transvaal and Orange Free State areas, and in Cape Province Blacks gained the vote only if they met a property-ownership qualification. Blacks saw the failure to grant the franchise, coming on the heels of British wartime propaganda promoting freedom from "Boer slavery", as a blatant betrayal. Before long the Union passed a barrage of oppressive legislation, making it illegal for black workers to strike, reserving skilled jobs for whites, barring blacks from military service, and instituting restrictive pass laws. In 1913 parliament enacted the Natives' Land Act, setting aside eight percent of South Africa's land for black occupancy. Whites, who made up only 20 percent of the population, held 90 percent of the land. Black Africans could not buy or rent land or even work as share-croppers outside their designated area. The authorities evicted thousands of squatters from farms and forced them into increasingly overcrowded and impoverished reserves, or into the cities. Those who remained sank to the status of landless labourers.
The original architects of apartheid gathered around a map of a planned township.

Black and Coloured opposition began to coalesce, and leading figures such as John Jabavu, Walter Rubusana and Abdullah Abdurahman laid the foundations for new non-tribal black political groups. Most significantly, a Columbia University-educated attorney, Pixley ka Isaka Seme, called together representatives of the various African tribes to form a unified, national organisation to represent the interests of blacks, and to ensure that they had an effective voice in the new Union. Thus there originated the South African Native National Congress, known from 1923 as the African National Congress (ANC). Parallel to this, Mahatma Gandhi worked with the Indian populations of Natal and the Transvaal to fight against the ever-increasing encroachment on their rights.

The international recession which followed World War I put pressures on mine-owners, and they sought to reduce costs by recruiting lower-paid, black, semi-skilled workers. White mine-workers saw this as a threat and in 1922 rose in the armed Rand Rebellion, supported by the new Communist Party of South Africa under the slogan "Workers of the World, unite and fight for a white South Africa." Smuts suppressed the rising violently, but the failure led to a convergence of views between Afrikaner nationalists and white English-speaking trade-unionists. The Communists saw the failure as having resulted from a lack of mobilisation by black workers, and re-oriented their recruitment.

In 1924 the NP, under Hertzog, came to power in a coalition government with the Labour Party, and Afrikaner nationalism gained greater hold. Afrikaans, previously regarded only as a low-class dialect of Dutch, replaced Dutch as an official language of the Union, and the so-called swart gevaar (black peril) became the dominant issue of the 1929 election. In the mid-1930s, Hertzog joined the NP with the more moderate SAP of Jan Smuts to form the United Party; this coalition fell apart at the start World War II when Smuts took the reins and, amid much controversy, led South Africa into war on the side of the Allies. However, any hopes of turning the tide of Afrikaner nationalism faded when Daniel François Malan led a radical break-away movement, the Purified National Party, to the central position in Afrikaner political life. The Afrikaner Broederbond, a secret Afrikaner brotherhood formed in 1918 to protect Afrikaner culture, soon became an extraordinarily influential force behind both the NP and other organisations designed to promote the volk ("people", the Afrikaners).

Due to the booming wartime economy, black labour became increasingly important to the mining and manufacturing industries, and the black urban population nearly doubled. Enormous squatter camps grew up on the outskirts of Johannesburg and (though to a lesser extent) outside the other major cities. Despite the appalling conditions in the townships, not only blacks knew poverty: wartime surveys found that 40 percent of white schoolchildren suffered from malnutrition.
[edit] Legalised discrimination
Main article: South Africa under apartheid
Racial-demographic map of South Africa published by CIA in 1979 with data from the 1970 South African census

From 1948 successive National Party administrations formalised and extended the existing system of segregation and denial of rights into the legal system of apartheid, which lasted until the 1990s. Although many important events occurred during this period, apartheid remained the central system around which most of the historical issues of this period revolved.
[edit] Dismantling

With increasing opposition to apartheid in the final decades of the 20th century - including an armed struggle, economic and cultural sanctions by the international community, pressure from the anti-apartheid movement around the world, a rebellion amongst Afrikaner and English-speaking youth as well as open revolt within the ruling National Party - State President F.W. de Klerk announced the unbanning of the African National Congress and Pan Africanist Congress as well as the release of Nelson Mandela on 2 February 1990, which signalled the beginning of a transition to democracy. In the referendum held on March 17, 1992 a white electorate voted 68% in favour of dismantling apartheid through negotiations.

After years of negotiations under the auspices of the Convention for a Democratic South Africa (CODESA), a draft constitution appeared on 26 July 1993, containing concessions towards all sides: a federal system of regional legislatures, equal voting-rights regardless of race, and a bicameral legislature.

From April 26 to 29, 1994 the South African population voted in the first universal suffrage general elections. The African National Congress won election to govern for the very first time, leaving the National Party and the Inkatha Freedom Party behind it and parties such as the Democratic Party and Pan Africanist Congress took up their seats as part of the parliamentary opposition in the first genuine multiracial parliament. Nelson Mandela was elected as President on 9 May 1994 and formed -according to the interim constitution of 1993- a government of national unity, consisting of the ANC, the NP and the Inkatha. On May 10 Mandela was inaugurated as South Africa's new President in Pretoria and Thabo Mbeki and FW De Klerk as his vice-presidents.

After considerable debate, and following submissions from advocacy groups, individuals and ordinary citizens, the Parliament enacted a new Constitution and Bill of Rights in 1996.
[edit] Truth and Reconciliation Commission
Main article: History of South Africa since 1994

After the enactment of the constitution focus turned to the Truth and Reconciliation Commission, established in 1995 under the dictum of Archbishop Desmond Tutu to expose crimes committed during the apartheid era. The commission heard many stories of brutality and injustice from all sides and offered some catharsis to people and communities shattered by their past experiences.

The Commission operated by allowing victims to tell their stories and by allowing perpetrators to confess their guilt; with amnesty on offer to those who made a full confession. Those who chose not to appear before the commission would face criminal prosecution if the authorities could prove their guilt. But while some soldiers, police, and ordinary citizens confessed their crimes, few of those who had given the orders or commanded the police presented themselves. For example, State President P.W. Botha himself, notably, refused to appear before the Commission. It has proven difficult to gather evidence against these alleged higher-level criminals.
[edit] Late-1990s
Former President Thabo Mbeki

In 1999 South Africa held its second universal-suffrage elections. In 1997, Mandela had handed over leadership of the ANC to his deputy, Thabo Mbeki, and speculation grew that the ANC vote might therefore drop. In fact, it increased, putting the party within one seat of the two-thirds majority that would allow it to alter the constitution.

The NP, restyled as the New National Party (NNP), lost two-thirds of its seats, as well as official opposition status to the Democratic Party (DP). The DP had traditionally functioned as a stronghold of liberal whites, and now gained new support from conservatives disenchanted with the NP, and from some middle-class blacks. Just behind the DP came the KwaZulu-Natal Inkatha Freedom Party (IFP), historically the voice of Zulu nationalism. While the IFP lost some support, its leader, Chief Buthelezi, continued to exercise power as the national Home Affairs minister.

While the ANC grass-roots hold Mbeki in far less affection than the beloved "Madiba" (Mandela), he has proven himself a shrewd politician, maintaining his political pre-eminence by isolating or co-opting opposition parties. In 2003, Mbeki manoeuvred the ANC to a two-thirds majority in parliament for the first time

Yet not everything has gone the ANC's way. In the early days of his presidency, Mbeki's effective denial of the HIV crisis invited global criticism, and his conspicuous failure to condemn the forced reclamation of white-owned farms in neighbouring Zimbabwe unnerved both South African landowners and foreign investors.

Violent crime escalated dramatically in the early 90's. The Economist reports the killing of approximately 1,500 white farmers in non-political attacks since 1991. In 1998, South Africa led the world in reported murders and robberies.

From 1994 onwards and more recently, the South African Police Service and South African Medical Research Council respectively have published statistics showing a decrease in homicides at national and city level.[8][9][10] A widely used estimate of over 32,000 homicides was reported by the South African Medical Research Council for the 2000/01 financial year. This, however, has been scrutinised and is now considered erroneous.[11]

According to The Economist, an estimated 250,000 white South Africans have emigrated since 1994

History of slavery
From Wikipedia, the free encyclopedia
Jump to:navigation, search
Part of a series on
Slavery
Early history

History · Antiquity · Aztec · Ancient Greece · Rome · Medieval Europe · Thrall · Kholop · Serfdom
Religion

The Bible · Judaism · Christianity · Islam
By country or region

Africa · Atlantic · Arab · Coastwise · Angola · Barbary Coast · Britain and Ireland · British Virgin Islands · Brazil · Canada · India · Iran · Japan · Libya · Mauritania · Romania · Spanish New World colonies · Sudan · Swedish · United States
Contemporary slavery

Modern Africa · Debt bondage · Penal labour · Sexual slavery · Unfree labour · Human trafficking
Opposition and resistance

Timeline · Abolitionism · Compensated emancipation · Opponents of slavery‎ · Slave rebellion · Slave narrative
This box: view • talk • edit
Diagram of a slave ship from the Atlantic slave trade. From an Abstract of Evidence delivered before a select committee of the House of Commons of Great Britain in 1790 and 1791.

The history of slavery covers systems throughout human history in which one human being is legally the property of another, can be bought or sold, is not allowed to escape and must work for the owner without any choice involved. A critical element is that children of a slave mother automatically become slaves.[1] It does not include forced labor by prisoners, labor camps, or other forms of unfree labor in which laborers are not considered property.

Slavery can be traced back to the earliest records, such as the Code of Hammurabi (ca. 1760 BC), which refers to it as an established institution.[2] Slavery is rare among hunter-gatherer populations as slavery depends on a system of social stratification. Slavery typically also requires a shortage of labor and a surplus of land to be viable. [3]
Contents
[hide]

    * 1 The ancient Mesopotamian and Mediterranean civilizations
    * 2 Rome
    * 3 Europe
          o 3.1 The Vikings and Scandinavia
          o 3.2 Middle Ages
          o 3.3 Portugal
          o 3.4 Spain
          o 3.5 Netherlands
          o 3.6 Great Britain and Ireland
          o 3.7 Pre-industrial Europe
          o 3.8 Modern Eastern Europe
    * 4 Slavery in the Muslim World
          o 4.1 Modern times
    * 5 Afghanistan
    * 6 Africa
          o 6.1 North Africa
                + 6.1.1 Barbary pirates
          o 6.2 Sub-Saharan Africa
          o 6.3 Modern times
    * 7 The Americas
          o 7.1 Among indigenous peoples
          o 7.2 Brazil
                + 7.2.1 Resistance and abolition
                + 7.2.2 Modern times
          o 7.3 Other South American countries
          o 7.4 British and French Caribbean
          o 7.5 North America
                + 7.5.1 Early events
                + 7.5.2 Slavery in American colonial law
                + 7.5.3 Development of slavery
                + 7.5.4 Early United States law
                + 7.5.5 Civil War
                + 7.5.6 Modern times
    * 8 Asia
          o 8.1 Indian subcontinent
                + 8.1.1 Modern times
          o 8.2 Nepal
          o 8.3 China
                + 8.3.1 Slavery under Manchurian rule in the 17th century
                + 8.3.2 Modern times
          o 8.4 Japan
                + 8.4.1 World War II
          o 8.5 Korea
          o 8.6 Southeast Asia
                + 8.6.1 Modern times
          o 8.7 Central Asia and the Caucasus
    * 9 Oceania
          o 9.1 Hawaii
          o 9.2 New Zealand
          o 9.3 Chatham Islands
          o 9.4 Rapa Nui / Easter Island
    * 10 Abolitionist movements
          o 10.1 Britain
          o 10.2 France
          o 10.3 United States
          o 10.4 Congress of Vienna
          o 10.5 Twentieth century worldwide
    * 11 See also
    * 12 References
    * 13 External links

[edit] The ancient Mesopotamian and Mediterranean civilizations
Main article: Slavery in antiquity
Gustave Boulanger's painting The Slave Market.
Ancient Greek art, showing a slave giving a mother her child.

Slavery in ancient cultures was known to occur in civilizations as old as Sumer, and it was found in every civilization, including Ancient Egypt, the Akkadian Empire, Assyria, Ancient Greece,[4] Rome and parts of its empire. Such institutions were a mixture of debt-slavery, punishment for crime, the enslavement of prisoners of war, child abandonment, and the birth of slave children to slaves.[5] In the Roman Empire, probably over 25% of the empire's population,[6] and 30 to 40% of the population of Italy[7] was enslaved. Records of slavery in Ancient Greece go as far back as Mycenaean Greece. It is often said that the Greeks as well as philosophers such as Aristotle accepted the theory of natural slavery i.e. that some men are slaves by nature.[8][9] At the time of Plato and Socrates, slavery was so accepted by the Greeks (including philosophers) that few people indeed protested it as an institution,[10] although there were in fact a few voices of opposition; Aristotle in Politics, Book 1, Chapter 6 noted and then discounted three voices opposed to his view of slavery, a jurist, philosopher and one other:

    But that those who take the opposite view have in a certain way right on their side, may be easily seen. For the words slavery and slave are used in two senses. There is a slave or slavery by law as well as by nature. The law of which I speak is a sort of convention- the law by which whatever is taken in war is supposed to belong to the victors. But this right many jurists impeach, as they would an orator who brought forward an unconstitutional measure: they detest the notion that, because one man has the power of doing violence and is superior in brute strength, another shall be his slave and subject. Even among philosophers there is a difference of opinion. The origin of the dispute, and what makes the views invade each other's territory, is as follows: in some sense virtue, when furnished with means, has actually the greatest power of exercising force; and as superior power is only found where there is superior excellence of some kind, power seems to imply virtue, and the dispute to be simply one about justice (for it is due to one party identifying justice with goodwill while the other identifies it with the mere rule of the stronger). If these views are thus set out separately, the other views have no force or plausibility against the view that the superior in virtue ought to rule, or be master. Others, clinging, as they think, simply to a principle of justice (for law and custom are a sort of justice), assume that slavery in accordance with the custom of war is justified by law, but at the same moment they deny this. For what if the cause of the war be unjust? And again, no one would ever say he is a slave who is unworthy to be a slave. Were this the case, men of the highest rank would be slaves and the children of slaves if they or their parents chance to have been taken captive and sold. Wherefore Hellenes do not like to call Hellenes slaves, but confine the term to barbarians. Yet, in using this language, they really mean the natural slave of whom we spoke at first; for it must be admitted that some are slaves everywhere, others nowhere.

During the 8th and the 7th centuries BC, in the course of the two Messenian Wars the Spartans reduced an entire population to a pseudo-slavery called helotry.[11] According to Herodotus (IX, 28–29), helots were seven times as numerous as Spartans. In some Ancient Greek city states about 30% of the population consisted of slaves, but paid and slave labor seem to have been equally important.[12]

Greeks however were among the first Europeans to abolish slavery with their constitution on 1823, which specifically noted that "in Greek territory no human being can be sold or bought, no matter his or her religion, and if a slave enters Greece, he is automatically considered an absolutely free man or woman and nobody can make claims on him or her".[citation needed]
[edit] Rome

Romans inherited the institution of slavery from the Greeks and the Phoenicians.[13] As the Roman Republic expanded outward, entire populations were enslaved, thus creating an ample supply to work in Rome's farms and households. The people subjected to Roman slavery came from all over Europe and the Mediterranean. Such oppression by an elite minority eventually led to slave revolts (see Roman Servile Wars); the Third Servile War led by Spartacus was the most famous and severe. Greeks, Berbers, Germans, Britons, Thracians, Gauls (or Celts), Jews, Arabs, and many more were slaves used not only for labor, but also for amusement (e.g. gladiators and sex slaves). If a slave ran away, he was liable to be crucified. By the late Republican era, slavery had become a vital economic pillar in the wealth of Rome.[14]
[edit] Europe
[edit] The Vikings and Scandinavia
Main articles: Thrall and Volga trade route

In the Viking era starting c. 793, the Norse raiders often captured and enslaved militarily weaker peoples they encountered. In the Nordic countries the slaves were called thralls (Old Norse: Þræll).[15] The thralls were mostly from Western Europe, among them many Franks, Anglo-Saxons, and Celts. Many Irish slaves participated in the colonization of Iceland.[16] There is evidence of German, Baltic, Slavic and Latin slaves as well. The slave trade was one of the pillars of Norse commerce during the 6th through 11th centuries.[17] The Persian traveller Ibn Rustah described how Swedish Vikings, the Varangians or Rus, terrorized and enslaved the Slavs. The slave raids came to an end when Catholicism became widespread throughout Scandinavia. As in the rest of Catholic Europe, the Scandinavian representatives for the church held that a Christian could not morally own another Christian. The thrall system was finally abolished in the mid-14th century in Scandinavia.
[edit] Middle Ages
Main article: Slavery in medieval Europe

Chaos and invasion made the taking of slaves habitual throughout Europe in the early Middle Ages. St. Patrick, himself captured and sold as a slave, protested against an attack that enslaved newly baptized Christians in his Letter to the Soldiers of Coroticus.

Slavery during the Early Middle Ages had several distinct sources. The Vikings raided across Europe, though their slave raids were the most destructive in the British Isles and Eastern Europe. While the Vikings kept some slaves for themselves as servants, known as thralls, most people captured by the Vikings would be sold on the Byzantine or Islamic markets. In the West the targets of Viking slavery were primarily English, Irish, and Scottish, while in the East they were mainly Slavs. The Viking slave trade slowly ended in the 1000s, as the Vikings settled in the European territories they once raided, Christianized serfdom, and merged with the local populace. The Normans made slaves of the English gentry after invasion in 1066 and deported them to Spain. They continued taking Welsh Slaves during the Medieval period who were traded in London.

The Islamic World was also a main factor in Medieval European slavery. Although slavery had different implications for slaves: (i) If they converted to Islam their master had the obligation to free them. (ii) Slaves could rise socially by marriage and attain high office. (iii) The principal requirement was for military service. The conquest by Tariq took place in 711 AD to liberate the population from Visigoth persecution. Raiding often involved taking slaves and involved Viking assistance. In the 16th and 17th centuries) during the Protestant Catholic Wars North Africans intensified the white slave trade with European collaboration, although slavery was practised in Christian states as well until the end of Serfdom (a practice that could be more brutal than slavery). The Muslim powers of Iberia both raided for slaves and purchased slaves from European merchants, often the Jewish Radhanites, one of the few groups that could easily move between the Christian and Islamic worlds. As the Muslims failed to conquer Europe in the 8th century they n alliance with the Vikings who raided the shores of Spain, southern Portugal and France, and Italy, that would last roughly from the 9th century until the 12th century, when the Italian city-states of Genoa, Venice, and Pisa, along with the Spanish kingdoms of Aragon and Castile, as well as the Sicilian Normans, began to dominate the Mediterranean trade. The Middle Ages from 1100 to 1500 saw a continuation of the European slave trade, though with a shift from the Western Mediterranean Islamic nations to the Eastern, as Venice and Genoa, in firm control of the Eastern Mediterranean from the 12th century and the Black Sea from the 13th century sold both Slavic and Baltic slaves, as well as Georgians, Turks, and other ethnic groups of the Black Sea and Caucasus, to the Muslim nations of the Middle East. The sale of European slaves by Europeans slowly ended as the Slavic and Baltic ethnic groups Christianized by the Late Middle Ages. European slaves in the Islamic World would, however, continue into the Modern time period as Muslim pirates, primarily Algerians, with the support of the Ottoman Empire, raided European coasts and shipping from the 16th to the 19th centuries, ending their attacks with the naval decline of the Ottoman Empire in the late 16th and 17th centuries, as well as the European conquest of North Africa throughout the 19th century.

The Mongol invasions and conquests in the 13th century made the situation worse.[18] The Mongols enslaved skilled individuals, women and children and marched them to Karakorum or Sarai, whence they were sold throughout Eurasia. Many of these slaves were shipped to the slave market in Novgorod.[19][20][21]

Slave commerce during the Late Middle Ages was mainly in the hands of Venetian and Genoese merchants and cartels, who were involved in the slave trade with the Golden Horde. In 1382 the Golden Horde under Khan Tokhtamysh sacked Moscow, burning the city and carrying off thousands of inhabitants as slaves. Between 1414 and 1423, some 10,000 eastern European slaves were sold in Venice.[22] Genoese merchants organized the slave trade from the Crimea to Mamluk Egypt. For years the Khanates of Kazan and Astrakhan routinely made raids on Russian principalities for slaves and to plunder towns. Russian chronicles record about 40 raids of Kazan Khans on the Russian territories in the first half of the 16th century.[23] In 1521, the combined forces of Crimean Khan Mehmed Giray and his Kazan allies attacked Moscow and captured thousands of slaves.[24]

In 1441, Haci I Giray declared independence from the Golden Horde and established the Crimean Khanate. For a long time, until the early 18th century, the khanate maintained a massive slave trade with the Ottoman Empire and the Middle East. In a process called the "harvesting of the steppe", they enslaved many Slavic peasants. About 30 major Tatar raids were recorded into Muscovite territories between 1558-1596.[25] In 1571, the Crimean Tatars attacked and sacked Moscow, burning everything but the Kremlin and taking thousands of captives as slaves.[26] In Crimea, about 75% of the population consisted of slaves.[27]

Medieval Spain and Portugal were the scene of almost constant warfare between Muslims and Christians. Periodic raiding expeditions were sent from Al-Andalus to ravage the Iberian Christian kingdoms, bringing back booty and slaves. In a raid against Lisbon, Portugal in 1189, for example, the Almohad caliph Yaqub al-Mansur took 3,000 female and child captives, while his governor of Córdoba, in a subsequent attack upon Silves, Portugal in 1191, took 3,000 Christian slaves.[28]

The Byzantine-Ottoman wars and the Ottoman wars in Europe brought large numbers of Christian slaves into the Islamic world too.[29] After the battle of Lepanto approximately 12,000 Christian galley slaves were freed from the Ottoman Turks.[30] Christians were also selling Muslim slaves captured in war. The Knights of Malta attacked pirates and Muslim shipping, and their base became a centre for slave trading, selling captured North Africans and Turks. Malta remained a slave market until well into the late 18th century. It required a thousand slaves to equip merely the galleys (ships) of the Order.[31][32]

Slavery in Poland was forbidden in the 15th century; in Lithuania, slavery was formally abolished in 1588; they were replaced by the second enserfment. Slavery remained a minor institution in Russia until the 1723, when the Peter the Great converted the household slaves into house serfs. Russian agricultural slaves were formally converted into serfs earlier in 1679.[33] The runaway Polish and Russian serfs and kholops known as Cossacks (‘outlaws’) formed autonomous communities in the southern steppes.[34]
[edit] Portugal
See also: Portuguese Empire, Economic history of Portugal, and Black ladino

The 15th century Portuguese exploration of the African coast is commonly regarded as the harbinger of European colonialism. In 1452, Pope Nicholas V issued the papal bull Dum Diversas, granting Afonso V of Portugal the right to reduce any "Saracens, pagans and any other unbelievers" to hereditary slavery which legitimized slave trade under Catholic beliefs of that time. This approval of slavery was reaffirmed and extended in his Romanus Pontifex bull of 1455. These papal bulls came to serve as a justification for the subsequent era of slave trade and European colonialism. Although for a short period as in 1462, Pius II declared slavery to be "a great crime".[35] The followers of the church of England and Protestants did not use the papal bull as a justification. The position of the church was to condemn the slavery of Christians, but slavery was regarded as an old established and necessary institution which supplied Europe with the necessary workforce. In the 16th century African slaves had substituted almost all other ethnicities and religious enslaved groups in Europe.[36] Within the Portuguese territory of Brazil, and even beyond its original borders, the enslavement of native Americans was carried out by the Bandeirantes.

Among many other European slave markets, Genoa, Venice and Verdun-sur-Meuse were some well known markets, their importance and demand growing after the great plague of the 14th century which decimated much of the European work force.[37] The maritime town of Lagos, Portugal, was the first slave market created in Portugal for the sale of imported African slaves - the Mercado de Escravos, opened in 1444.[38][39] In 1441, the first slaves were brought to Portugal from northern Mauritania.[39] Prince Henry the Navigator, major sponsor of the Portuguese African expeditions, as of any other merchandise, taxed one fifth of the selling price of the slaves imported to Portugal.[39] By the year 1552 black African slaves made up 10 percent of the population of Lisbon.[40][41] In the second half of the 16th century, the Crown gave up the monopoly on slave trade and the focus of European trade in African slaves shifted from import to Europe to slave transports directly to tropical colonies in the Americas - in the case of Portugal, especially Brazil.[39] In the 15th century one third of the slaves were resold to the African market in exchange of gold.[36]
[edit] Spain
See also: Spanish Empire, Spanish colonization of the Americas, and Black ladino

Spain had to fight against relatively powerful civilizations of the New World. However, the Spanish conquest of the indigenous peoples in the Americas was also facilitated by the spread of diseases (e.g. smallpox) due to lack of biological immunity.[42] (like the Europeans that had lack of biological immunity to African diseases) Natives were used as forced labor (the Spanish employed the pre-Columbian draft system called the mita),[43] but the diseases caused a labor shortage and so the Spanish colonists were gradually involved in the Atlantic slave trade.

The first Europeans to use African slaves in the New World were the Spaniards on islands such as Cuba and Hispaniola, where the native population starved themselves rather than work for the Spanish. The first African slaves arrived in Hispaniola in 1501.[44]
[edit] Netherlands

Although only minor part of the wealth of the Dutch came directly through the slave trade, it is nevertheless closely associated with the Dutch Golden Age.[45] In 1619 The Netherlands began the slave trade between Africa and America (Virginia)[45], by 1650 becoming the pre-eminent slave trading country in Europe, a position overtaken by Britain around 1700. The port city of Amsterdam was the European capital of slavery, helping to manage the slave trade also of neighbouring nations and with up to 10,000 slaving vessels associated with the port.[46]
[edit] Great Britain and Ireland
Main articles: Slavery in Britain and Ireland and Slavery in the colonial United States

Slavery was practised by the Romans, but when they left in the 5th century they took their slaves with them. Anglo-Saxon Germanic settlers brought in slaves, and reduced many native Celts to slavery by 600 AD. Capture in war, voluntary servitude and debt slavery became common, and slaves were routrinely bought and sold, but running away was common and slavery was never a major economic factor. Ireland and Denmark were markets for captured Anglo Saxon and Celtic slaves. Pope Gregory I reputably made the pun, Non Angli, sed Angeli ("Not Angles, but Angels"), after a response to his query regarding the identity of a group of fair-haired Angles slave children whom he had observed in the marketplace. After 1100 slavery faded away as uneconomical.[47] The Normans revived the trade in Wales, and Ireland.

Following the Cromwellian conquest of Ireland, as many as 550,000 Irish men, women and children were forced into temporary indentured service and transported to the colonies in the British West Indies and British America.[48][49]. Some 50,000 British convicts were sent to colonial America, representing perhaps one-quarter of all British emigrants during the eighteenth century.[50].

From the 16th to 19th century, Barbary Corsairs raided the coasts of Europe and attacked lone ships at sea. From 1609 to 1616, England lost 466 merchant ships to Barbary pirates. 160 English ships were captured by Algerians between 1677 and 1680.[51]. Many of the captured sailors were made into slaves. The corsairs were no strangers to the South West of England where raids were known in a number of coastal communities. Around 1645 Barbary Pirates under command of the Dutch renegade Jan Janszoon operating from the Moroccan port of Salé occupied the island of Lundy. During this time there were reports of captured slaves being sent to Algiers.[52][53]

Ireland, despite its northern position, was not immune from attacks by the corsairs. In June 1631 Murat Reis, with pirates from Algiers and armed troops of the Ottoman Empire, stormed ashore at the little harbor village of Baltimore, County Cork. They captured almost all the villagers and took them away to a life of slavery in North Africa.[54] The prisoners were destined for a variety of fates — some lived out their days chained to the oars as galley slaves, while others would spend long years in the scented seclusion of the harem or within the walls of the sultan's palace. Only two of them ever saw Ireland again.

Britain played a prominent role in the Atlantic slave trade which began around the mid-fifteenth century when Portuguese interests in Africa moved away from the fabled deposits of gold to a much more readily available commodity; slaves. Slavery was a legal institution in all of the 13 American colonies, and the profits of the slave trade and of West Indian plantations amounted to 5% of the British economy at the time of the Industrial Revolution.[55] In 1807, following many years of lobbying by the Abolitionist movement, the British Parliament voted to make the slave trade illegal anywhere in the Empire with the Slave Trade Act 1807. Thereafter Britain took a prominent role in combating the trade, and slavery itself was abolished in the British Empire with the Slavery Abolition Act 1833. Between 1808 and 1860, the West Africa Squadron seized approximately 1,600 slave ships and freed 150,000 Africans who were aboard.[56] Action was also taken against African leaders who refused to agree to British treaties to outlaw the trade, for example against "the usurping King of Lagos", deposed in 1851. Anti-slavery treaties were signed with over 50 African rulers.[57]

In 1811, Arthur William Hodge was the first slave owner executed for the murder of a slave in the British West Indies.[58] He was not, however, as some have claimed, the first white person to have been lawfully executed for the killing of a slave.[59][60]

Although indentured slavery or child trafficking continued throughout the British Empire emptying British workhouses. It finally ended in 1956. Australia, Catholic Orphanages and St. Bernardo's 20 to 30 years later were faced with a class action in the Australian courts. child trafficking
[edit] Pre-industrial Europe
Galley with rowing slaves

It became the custom among the Mediterranean powers to sentence condemned criminals to row in the war-galleys of the state (initially only in time of war).[61] The French Huguenots filled the galleys after the revocation of the Edict of Nantes in 1685 and Camisard rebellion.[62] Galley-slaves lived in unsavoury conditions, so even though some sentences prescribed a restricted number of years, most rowers would eventually die, even if they survived shipwreck and slaughter or torture at the hands of enemies or of pirates.[63] Naval forces often turned 'infidel' prisoners-of-war into galley-slaves. Several well-known historical figures served time as galley slaves after being captured by the enemy—the Ottoman corsair and admiral Turgut Reis and the Knights Hospitaller Grand Master Jean Parisot de la Valette among them.[64]

The so-called second serfdom took place in Eastern Europe during this period (particularly in Austria-Hungary, Prussia, Russia and Poland). During the seventeenth and the eighteenth centuries, Ukraine was controlled by the Polish-Lithuanian Commonwealth. During this period, it is estimated that hundreds of thousands of Ukrainians were sold into slavery to the Turks.[65] Only in 1768 was a law passed in Poland that discontinued the nobility's right of life or death over serfs. Serfdom remained the practice in most of Russia until 19 February 1861. Some of the Roma people were enslaved over five centuries in Romania until abolition in 1864 (see Slavery in Romania).[66]

Slavery in the French Republic was abolished on 4 February 1794 however it was re-established by Napoleon Bonaparte in 1804. Slavery would be permanently abolished in France after his first exile to Elba in 1814. The Haitian Revolution established Haiti as a free republic ruled by blacks, the first of its kind.[67] At the time of the revolution, Haiti was known as Saint-Domingue and was a colony of France.[68]
[edit] Modern Eastern Europe
Main articles: Human trafficking and Sexual slavery

Since the fall of the Iron Curtain, the impoverished former Eastern bloc countries such as Albania, Moldova, Romania, Bulgaria, Russia, Belarus and Ukraine have been identified as major trafficking source countries for women and children.[69] Young women and girls are often lured to wealthier countries by the promises of money and work and then reduced to sexual slavery.[70] It is estimated that 2/3 of women trafficked for prostitution worldwide annually come from Eastern Europe, three-quarters have never worked as prostitutes before.[71][72] The major destinations are Western Europe (Germany, Italy, Netherlands, Spain, UK, Greece), the Middle East (Turkey, Israel, the United Arab Emirates), Asia, Russia and the United States.[73][74]

It is estimated that half million Ukrainian women were trafficked abroad since 1991 (80% of all unemployed in Ukraine are women).[75][76] Russia is a major source of women trafficked globally for the purpose of sexual exploitation, Russian women are in prostitution in over 50 countries.[77][78][79] In poverty-stricken Moldova, where the unemployment rate for women ranges as high as 68% and one-third of the workforce live and work abroad, experts estimate that since the collapse of the Soviet Union between 200,000 and 400,000 women have been sold into prostitution abroad — perhaps up to 10% of the female population.[80][81]
[edit] Slavery in the Muslim World
Main articles: Arab slave trade and Slavery (Ottoman Empire)
13th century slave market in Yemen
Capt. William Bainbridge paying tribute to the Dey of Algiers. Gradually in the 18th century slave raids became less frequent, but the Barbary pirates continued to enslave captured crews. Payments in ransom and tribute to the Barbary states amounted to 20% of United States government annual revenues in 1800.[82]

The Arab slave trade lasted more than a millennium.[83][84] Slaves in the Arab World came from many different regions, including Sub-Saharan Africa (mainly Zanj), the Caucasus (mainly Circassians),[85] Central Asia (mainly Tartars), and Central and Eastern Europe (mainly Saqaliba).[86]

The medieval scholar and traveller Ibn Battuta states several times that he was given or purchased slaves.[87] The Arab or Middle Eastern slave trade is thought to have originated with trans-Saharan slavery.[88][89] Arab, Indian, and Oriental traders were involved in the capture and transport of slaves northward across the Sahara desert and the Indian Ocean region into Arabia and the Middle East, Persia, Central Asia and the Indian subcontinent.[90][91] The slave trade from East Africa to Arabia was dominated by Arab and African traders in the coastal cities of Zanzibar, Dar Es Salaam and Mombasa.[91][92] Tens of thousands of black Zanj slaves were imported to lower Iraq, where they may have, according to Richard Hellie, constituted at least a half of the total population there in the 9th and 10th centuries. At the same time, many tens of thousands of slaves in the region were also imported from Central Asia and the Caucasus.[93]

Male slaves were employed as servants, soldiers, or laborers, while female slaves were traded to Middle Eastern countries and kingdoms by Arab, Indian, or Oriental traders, some as domestic servants and others in harems.[94][95][96] Some historians estimate that between 11 and 17 million slaves crossed the Red Sea, Indian Ocean, and Sahara Desert from 650 to 1900 AD.[97][98] The Moors, starting in the 8th century, raided coastal areas around the Mediterranean and Atlantic Ocean, and became known as the Barbary pirates. It is estimated that they captured 1.25 million slaves from Western Europe and North America between the 16th and 19th centuries.[99][100]

In 1400 Tamerlane invaded Armenia and Georgia. More than 60,000 people from the Caucasus were captured as slaves, and many districts of Armenia were depopulated.[101]

From 1569 the Polish-Lithuanian Commonwealth suffered a series of Tatar invasions, the goal of which was to loot, pillage and capture slaves into jasyr. The borderland area to the south-east was in a state of semi-permanent warfare until the 18th century. Some researchers estimate that altogether more than 3 million people, predominantly Ukrainians but also Circassians, Russians, Belarusians, Poles and Jews were captured and enslaved during the time of the Crimean Khanate.[86][102] Russian conquest of the Crimea led to the abolition of slavery by the 1780s.[103]

Slavery was an important part of Ottoman society. In Constantinople (today Istanbul), about 1/5 of the population consisted of slaves.[27] As late as 1908 women slaves were still sold in the Ottoman Empire.[104] In the middle of the 14th century, Murad I built his own personal slave army called the Kapıkulu. The new force was based on the sultan's right to a fifth of the war booty, which he interpreted to include captives taken in battle. The captive slaves were converted to Islam and trained in the sultan's personal service. In the devşirme (Turkish for 'gathering'), young Christian boys from the Balkans were taken away from their homes and families, converted to Islam and enlisted into special soldier classes of the Ottoman army or the civil service. These soldier classes were named Janissaries, the most famous branch of the Kapıkulu. The Janissaries eventually became a decisive factor in the Ottoman invasions of Europe.[105] Most of the military commanders of the Ottoman forces, imperial administrators and de facto rulers of the Ottoman Empire, such as Pargalı İbrahim Pasha and Sokollu Mehmet Paşa, were recruited in this way.[106][107] By 1609 the Sultan's Kapıkulu forces increased to about 100,000.[108] By this time however, the expeditions for young Christian boys were rare. The increased numbers of janissaries came from Muslim peasants who were now allowed into service as a result of increased military demands of 17th century warfare.

The Mamluks were slave soldiers who converted to Islam and served the Muslim caliphs and the Ayyubid sultans during the Middle Ages. The first mamluks served the Abbasid caliphs in 9th century Baghdad. Over time they became a powerful military caste, and on more than one occasion they seized power for themselves, for example, ruling Egypt in the years 1250-1517. From 1250 Egypt had been ruled by the Bahri dynasty of Kipchak Turk origin. White slaves from the Caucasus served in the army and formed an elite corps of troops eventually revolting in Egypt to form the Burgi dynasty. Mamluks were mainly responsible for the expulsion of the Crusaders from Palestine and preventing the Mongol Ilkhanate of Persia and Iraq from entering Egypt.[109]

The Moroccan Sultan Moulay Ismail "the Bloodthirsty" (1672–1727) raised a corps of 150,000 black slaves, called his Black Guard, who coerced the country into submission.[110]

Nautical traders from the United States became targets, and frequent victims, of the Barbary pirates, as soon as that nation began trading with Europe and refused to pay the required tribute to the North African states.[111][112]
[edit] Modern times
Child Slavery: Trafficked children as young as 2 years old are forced to work up to 18 hours a day as camel jockeys in some Middle Eastern states.

The Arab or Middle Eastern slave trade continued into the early 1900s,[113] and by some accounts continue to this day. Slavery in Morocco was outlawed in the 1930s.[114] As recently as the 1950s, Saudi Arabia had an estimated 450,000 slaves, 20% of the population.[115][116] It is estimated that as many as 200,000 black south Sudanese children and women (mostly from the Dinka tribe sold by the Sudanese Arabs of the north) have been taken into slavery in Sudan during the Second Sudanese Civil War.[117][118] In Mauritania it is estimated that up to 600,000 men, women and children, or 20% of the population, are currently enslaved, many of them used as bonded labor.[119] Slavery in Mauritania was criminalized in August 2007.[120]

The Arab trade in slaves continued into the 20th century. Written travelogues and other historical works are replete with references to slaves owned by wealthy traders, nobility and heads of state in the Arabian Peninsula well into the 1920s. Slave owning and slave-like working conditions have been documented up to and including the present, in countries of the Middle East. Though the subject is considered taboo in the affected regions, a leading Saudi government cleric and author of the country's religious curriculum has called for the outright re-legalization of slavery[121][122].

Children as young as two years old are used for slavery as child camel jockeys across the Arab countries of the Middle East. Although strict laws have been introduced recently in Qatar and UAE, thanks to better awareness of the issue and lobbying by human rights organisations such as the Ansar Burney Trust, the use of children still continues in outlying areas and during secret night-time races.

Many of the Iraqi women fleeing the Iraq War are turning to prostitution, others are trafficked abroad, to countries like Syria, Jordan, Qatar, the United Arab Emirates, Turkey, and Iran.[123] In Syria alone, an estimated 50,000 Iraqi refugee girls and women, many of them widows, are forced into prostitution.[124] Cheap Iraqi prostitutes have helped to make Syria a popular destination for sex tourists. The clients come from wealthier countries in the Middle East - many are Saudi men.[125] High prices are offered for virgins.[126]

Slavery still continues today in a smaller form in the Arab states of the Persian Gulf, where women and children are trafficked from the post-Soviet states, Eastern Europe, Far East, Africa, South Asia and other parts of the Middle East.[127][128][129]
[edit] Afghanistan

    "The country generally between Caubul (Kabul) and the Oxus appears to be in a very lawless state; slavery is as rife as ever, and extends through Hazara, Badakshan, Wakhan, Sirikul (Sarikol), Kunjūt (Hunza), &c. A slave, if a strong man likely to stand work well, is, in Upper Badakshan, considered to be of the same value as one of the large dogs of the country, or of a horse, being about the equivalent of Rs 80. A slave girl is valued at from four horses or more, according to her looks &c.; men are, however, almost always exchanged for dogs. When I was in Little Tibet (Ladakh),a returned slave who had been in the Kashmir army took refuge in my camp; he said he was well enough treated as to food &c., but he could never get over having been exchanged for a dog, and constantly harped on the subject, the man who sold him evidently thinking the dog the better animal of the two. In Lower Badakshan, and more distant places, the price of slaves is much enhanced, and payment is made in coin."[130]

In response to the Hazara uprising of 1892, the Afghan Emir Abdur Rahman Khan declared a "Jihad" against the Shiites. His large army defeated the rebellion at its center, in Oruzgan, by 1892 and the local population was being massacred. According to S. A. Mousavi, "thousands of Hazara men, women, and children were sold as slaves in the markets of Kabul and Qandahar, while numerous towers of human heads were made from the defeated rebels as a warning to others who might challenge the rule of the Amir". Until the 20th century, some Hazaras were still kept as slaves by the Pashtuns; although Amanullah Khan banned slavery in Afghanistan during his reign,[131] the practice carried on unofficially for many more years.[132]
[edit] Africa
Main articles: African slave trade and Slavery in modern Africa
Two slightly differing Okpoho manillas as used to purchase slaves

In most African societies, there was very little difference between the free peasants and the feudal vassal peasants. Vassals of the Songhay Muslim Empire were used primarily in agriculture; they paid tribute to their masters in crop and service but they were slightly restricted in custom and convenience. These people were more an occupational caste, as their bondage was relative. In the Kanem Bornu Empire, vassals were three classes beneath the nobles. Marriage between captor and captive was far from rare, blurring the anticipated roles.[91]

French historian Fernand Braudel noted that slavery was endemic in Africa and part of the structure of everyday life. "Slavery came in different disguises in different societies: there were court slaves, slaves incorporated into princely armies, domestic and household slaves, slaves working on the land, in industry, as couriers and intermediaries, even as traders" (Braudel 1984 p. 435). During the 16th century, Europe began to outpace the Arab world in the export traffic, with its slave traffic from Africa to the Americas. The Dutch imported slaves from Asia into their colony in South Africa. In 1807 the United Kingdom, which held extensive, although mainly coastal colonial territories on the African continent (including southern Africa), made the international slave trade illegal throughout its empire. The end of the slave trade and the decline of slavery was imposed upon Africa by its European conquerors.

The nature of the slave societies differed greatly across the continent. There were large plantations worked by slaves in Egypt, the Sudan and Zanzibar, but this was not a typical use of slaves in Africa as a whole. In most African slave societies, slaves were protected and incorporated into the slave-owning family.[citation needed]
13th century Africa - Map of the main trade routes and states, kingdoms and empires

In Senegambia, between 1300 and 1900, close to one-third of the population was enslaved. In early Islamic states of the western Sudan, including Ghana (750-1076), Mali (1235–1645), Segou (1712–1861), and Songhai (1275-1591), about a third of the population were slaves. In Sierra Leone in the 19th century about half of the population consisted of slaves. In the 19th century at least half the population was enslaved among the Duala of the Cameroon, the Igbo and other peoples of the lower Niger, the Kongo, and the Kasanje kingdom and Chokwe of Angola. Among the Ashanti and Yoruba a third of the population consisted of slaves. The population of the Kanem was about a third-slave. It was perhaps 40% in Bornu (1396–1893). Between 1750 and 1900 from one- to two-thirds of the entire population of the Fulani jihad states consisted of slaves. The population of the Sokoto caliphate formed by Hausas in the northern Nigeria and Cameroon was half-slave in the 19th century. It is estimated that up to 90% of the population of Arab-Swahili Zanzibar was enslaved. Roughly half the population of Madagascar was enslaved.[133][134][135][136][137][138][139]

The Anti-Slavery Society estimated that there were 2,000,000 slaves in the early 1930s Ethiopia, out of an estimated population of between 8 and 16 million.[140] Slavery continued in Ethiopia until the brief Second Italo-Abyssinian War in October 1935, when it was abolished by order of the Italian occupying forces.[141] In response to pressure by Western Allies of World War II Ethiopia officially abolished slavery and serfdom after regaining its independence in 1942. On 26 August 1942 Haile Selassie issued a proclamation outlawing slavery.[142][143]

When British rule was first imposed on the Sokoto Caliphate and the surrounding areas in northern Nigeria at the turn of the 20th century, approximately 2 million to 2.5 million people there were slaves.[144] Slavery in northern Nigeria was finally outlawed in 1936.[145]

Elikia M’bokolo, April 1998, Le Monde diplomatique. Quote: "The African continent was bled of its human resources via all possible routes. Across the Sahara, through the Red Sea, from the Indian Ocean ports and across the Atlantic. At least ten centuries of slavery for the benefit of the Muslim countries (from the ninth to the nineteenth)." He continues: "Four million slaves exported via the Red Sea, another four million through the Swahili ports of the Indian Ocean, perhaps as many as nine million along the trans-Saharan caravan route, and eleven to twenty million (depending on the author) across the Atlantic Ocean"[146]
[edit] North Africa
[edit] Barbary pirates
See also: Arab slave trade

According to Robert Davis between 1 million and 1.25 million Europeans were captured by Barbary pirates and sold as slaves in North Africa and Ottoman Empire between the 16th and 19th centuries.[147] The coastal villages and towns of Italy, Portugal, Spain and Mediterranean islands were frequently attacked by them and long stretches of the Italian, Portuguese and Spanish coasts were almost completely abandoned by their inhabitants; after 1600 Barbary pirates occasionally entered the Atlantic and struck as far north as Iceland.[148]

In 1544, Hayreddin Barbarossa captured Ischia, taking 4,000 prisoners in the process, and deported to slavery some 9,000 inhabitants of Lipari, almost the entire population.[149] In 1551, Turgut Reis (known as Dragut in the West) enslaved the entire population of the Maltese island Gozo, between 5,000 and 6,000, sending them to Libya. When pirates sacked Vieste in southern Italy in 1554 they took 7,000 slaves. In 1555, Turgut Reis sailed to Corsica and ransacked Bastia, taking 6,000 prisoners. In 1558 Barbary corsairs captured the town of Ciutadella (Minorca), destroyed it, slaughtered the inhabitants and carried off 3,000 survivors to Istanbul as slaves.[150] In 1563 Turgut Reis landed at the shores of the province of Granada, Spain, and captured the coastal settlements in the area like Almuñécar, along with 4,000 prisoners. Barbary pirates frequently attacked the Balearic islands, resulting in many coastal watchtowers and fortified churches being erected. The threat was so severe that the island of Formentera became uninhabited.[151][152][153]

In Portugal for instance, the coastal city of Nazaré was raided several times during until the 16th century when the local fortress was built (according to Pedro Penteado and his book based in the historical ecclesiastic diaries of Nazaré). The city of Lisbon built the Torre de Belém to defend the capital against these pirates.

Between 1609 and 1616 England alone had a staggering 466 merchant ships lost to Barbary pirates. 160 British ships were captured by Algerians between 1677 and 1680.[154] Slave-taking persisted into the 19th century when Barbary pirates would capture ships and enslave the crew.[155] Even the United States was not immune. In 1783 the United States made peace with, and gained recognition from, the British monarchy, and in 1784 the first American ship was seized by pirates from Morocco. Payments in ransom and tribute to the Barbary states amounted to 20% of United States government annual revenues in 1800.[82] It was not until 1815 that naval victories in the Barbary Wars ended tribute payments by the U.S., although some European nations continued annual payments until the 1830s.[111]

Among the most important slave markets where Pirates operated in Mediterranean Europe were the ports of Majorca, Toulon, Marseille, Genoa, Pisa, Livorno and Malta. In Africa, the most important were the ports of Morocco, Tripoli, Algiers and Tunis.[156]
[edit] Sub-Saharan Africa
Main article: African slave trade
Slaves being transported in Africa, 19th century engraving.

David Livingstone wrote of the slave trade:

    "To overdraw its evils is a simple impossibility.... We passed a slave woman shot or stabbed through the body and lying on the path. [Onlookers] said an Arab who passed early that morning had done it in anger at losing the price he had given for her, because she was unable to walk any longer. We passed a woman tied by the neck to a tree and dead.... We came upon a man dead from starvation.... The strangest disease I have seen in this country seems really to be broken heartedness, and it attacks free men who have been captured and made slaves."

Livingstone estimated that 80,000 Africans died each year before ever reaching the slave markets of Zanzibar.[157][158][159][160] Zanzibar was once East Africa's main slave-trading port, and under Omani Arabs in the 19th century as many as 50,000 slaves were passing through the city each year.[161]

Prior to the 16th century, the bulk of slaves exported from Africa were shipped from East Africa to the Arabian peninsula. Zanzibar became a leading port in this trade. Arab slave traders differed from European ones in that they would often conduct raiding expeditions themselves, sometimes penetrating deep into the continent. They also differed in that their market greatly preferred the purchase of female slaves over male ones.

The increased presence of European rivals along the East coast led Arab traders to concentrate on the overland slave caravan routes across the Sahara from the Sahel to North Africa. The German explorer Gustav Nachtigal reported seeing slave caravans departing from Kukawa in Bornu bound for Tripoli and Egypt in 1870. The slave trade represented the major source of revenue for the state of Bornu as late as 1898. The eastern regions of the Central African Republic have never recovered demographically from the impact of nineteenth-century raids from the Sudan and still have a population density of less than 1 person/km.[162] During the 1870s, European initiatives against the slave trade caused an economic crisis in northern Sudan, precipitating the rise of Mahdist forces. Mahdi’s victory created an Islamic state, one that quickly reinstituted slavery.[163][164]

The Middle Passage, the crossing of the Atlantic to the Americas, endured by slaves laid out in rows in the holds of ships, was only one element of the well-known triangular trade engaged in by Portuguese, Dutch, French and British. Ships having landed slaves in Caribbean ports would take on sugar, indigo, raw cotton, and later coffee, and make for Liverpool, Nantes, Lisbon or Amsterdam. Ships leaving European ports for West Africa would carry printed cotton textiles, some originally from India, copper utensils and bangles, pewter plates and pots, iron bars more valued than gold, hats, trinkets, gunpowder and firearms and alcohol. Tropical shipworms were eliminated in the cold Atlantic waters, and at each unloading, a profit was made.

The Atlantic slave trade peaked in the late 18th century, when the largest number of slaves were captured on raiding expeditions into the interior of West Africa. These expeditions were typically carried out by African kingdoms, such as the Oyo empire (Yoruba), Kong Empire, Kingdom of Benin, Kingdom of Fouta Djallon, Kingdom of Fouta Tooro, Kingdom of Koya, Kingdom of Khasso, Kingdom of Kaabu, Fante Confederacy, Ashanti Confederacy, Aro Confederacy and the kingdom of Dahomey.[165][166] Europeans rarely entered the interior of Africa, due to fear of disease and moreover fierce African resistance. The slaves were brought to coastal outposts where they were traded for goods. The people captured on these expeditions were shipped by European traders to the colonies of the New World. As a result of the War of the Spanish Succession, the United Kingdom obtained the monopoly (asiento de negros) of transporting captive Africans to Spanish America. It is estimated that over the centuries, twelve to twenty million people were shipped as slaves from Africa by European traders, of whom some 15 percent died during the terrible voyage, many during the arduous journey through the Middle Passage. The great majority were shipped to the Americas, but some also went to Europe and Southern Africa.

Before the arrival of the Portuguese, slavery had already existed in Kingdom of Kongo. Despite its establishment within his kingdom, Afonso I of Kongo believed that the slave trade should be subject to Kongo law. When he suspected the Portuguese of receiving illegally enslaved persons to sell, he wrote letters to the King João III of Portugal in 1526 imploring him to put a stop to the practice.[167]

The kings of Dahomey sold their war captives into transatlantic slavery, who otherwise would have been killed in a ceremony known as the Annual Customs. As one of West Africa's principal slave states, Dahomey became extremely unpopular with neighbouring peoples.[168][169][170] Like the Bambara Empire to the east, the Khasso kingdoms depended heavily on the slave trade for their economy. A family's status was indicated by the number of slaves it owned, leading to wars for the sole purpose of taking more captives. This trade led the Khasso into increasing contact with the European settlements of Africa's west coast, particularly the French.[171] Benin grew increasingly rich during the 16th and 17th centuries on the slave trade with Europe; slaves from enemy states of the interior were sold, and carried to the Americas in Dutch and Portuguese ships. The Bight of Benin's shore soon came to be known as the "Slave Coast".[172]

In the 1840s, King Gezo of Dahomey said:[173]

    "The slave trade is the ruling principle of my people. It is the source and the glory of their wealth…the mother lulls the child to sleep with notes of triumph over an enemy reduced to slavery…"

200th anniversary of the British act of parliament abolishing slave trading, commemorated on a British two pound coin.

In 1807, the UK Parliament passed the Bill that abolished the trading of slaves. The King of Bonny (now in Nigeria) was horrified at the conclusion of the practice:[174]

    "We think this trade must go on. That is the verdict of our oracle and the priests. They say that your country, however great, can never stop a trade ordained by God himself."

Some historians conclude that the total loss in persons removed, those who died on the arduous march to coastal slave marts and those killed in slave raids, far exceeded the 65–75 million inhabitants remaining in Sub-Saharan Africa at the trade's end.[citation needed] Others believe that slavers had a vested interest in capturing rather than killing, and in keeping their captives alive; and that this coupled with the disproportionate removal of males and the introduction of new crops from the Americas (cassava, maize) would have limited general population decline to particular regions of western Africa around 1760–1810, and in Mozambique and neighbouring areas half a century later. There has also been speculation that within Africa, females were most often captured as brides, with their male protectors being a "bycatch" who would have been killed if there had not been an export market for them.

During the period from late 19th and early 20th centuries, demand for the labor-intensive harvesting of rubber drove frontier expansion and slavery. The personal monarchy of Belgian King Leopold II in the Congo Free State saw mass killings and slavery to extract rubber.[175]
[edit] Modern times

The trading of children has been reported in modern Nigeria and Benin. In parts of Ghana, a family may be punished for an offense by having to turn over a virgin female to serve as a sex slave within the offended family. In this instance, the woman does not gain the title or status of "wife". In parts of Ghana, Togo, and Benin, shrine slavery persists, despite being illegal in Ghana since 1998. In this system of ritual servitude, sometimes called trokosi (in Ghana) or voodoosi in Togo and Benin, young virgin girls are given as slaves to traditional shrines and are used sexually by the priests in addition to providing free labor for the shrine. Slavery in Sudan continues as part of an ongoing civil war. Evidence emerged in the late 1990s of systematic slavery in cacao plantations in West Africa; see the chocolate and slavery article.[173]
[edit] The Americas
[edit] Among indigenous peoples
Main articles: Aztec slavery, Repartimiento, Slavery in the Spanish New World colonies, and Slavery in Canada

In Pre-Columbian Mesoamerica the most common forms of slavery were those of prisoners-of-war and debtors. People unable to pay back a debt could be sentenced to work as a slave to the person owed until the debt was worked off. Warfare was important to the Maya society, because raids on surrounding areas provided the victims required for human sacrifice, as well as slaves for the construction of temples.[176] Most victims of human sacrifice were prisoners of war or slaves.[177] According to Aztec writings, as many as 84,000 people were sacrificed at a temple inauguration in 1487.[178] Slavery was not usually hereditary; children of slaves were born free. In the Inca Empire, workers were subject to a mita in lieu of taxes which they paid by working for the government. Each ayllu, or extended family, would decide which family member to send to do the work. It is unclear if this labor draft or corvée counts as slavery. The Spanish adopted this system, particularly for their silver mines in Bolivia.[179]

Other slave-owning societies and tribes of the New World were, for example, the Tehuelche of Patagonia, the Comanche of Texas, the Caribs of Dominica, the Tupinambá of Brazil, the fishing societies, such as the Yurok, that lived along the coast from what is now Alaska to California, the Pawnee and Klamath.[180] Many of the indigenous peoples of the Pacific Northwest Coast, such as the Haida and Tlingit, were traditionally known as fierce warriors and slave-traders, raiding as far as California. Slavery was hereditary, the slaves being prisoners of war. Among some Pacific Northwest tribes about a quarter of the population were slaves.[181][182] One slave narrative was composed by an Englishman, John R. Jewitt, who had been taken alive when his ship was captured in 1802; his memoir provides a detailed look at life as a slave, and asserts that a large number were held.
[edit] Brazil
Main articles: History of slavery in Brazil and Bandeirantes
Slavery in Brazil, Jean Baptiste Debret.
A Guaraní family captured by Indian slave hunters. By Jean Baptiste Debret

Slavery was a mainstay of the Brazilian colonial economy, especially in mining and sugar cane production. Brazil obtained 37% of all African slaves traded, and more than 3 million slaves were sent to this one country. Starting around 1550, the Portuguese began to trade African slaves to work the sugar plantations, once the native Tupi people deteriorated. Although Portuguese Prime Minister Marquês de Pombal abolished slavery in mainland Portugal on the 12 February 1761, slavery continued in her overseas colonies. Slavery was practiced among all classes. Slaves were owned by upper and middle classes, by the poor, and even by other slaves.[183]

From São Paulo, the Bandeirantes, adventurers mostly of mixed Portuguese and native ancestry, penetrated steadily westward in their search for Indian slaves. Along the Amazon river and its major tributaries, repeated slaving raids and punitive attacks left their mark. One French traveler in the 1740s described hundreds of miles of river banks with no sign of human life and once-thriving villages that were devastated and empty. In some areas of the Amazon Basin, and particularly among the Guarani of southern Brazil and Paraguay, the Jesuits had organized their Jesuit Reductions along military lines to fight the slavers. In the mid to late 19th century, many Amerindians were enslaved to work on rubber plantations.[184][185][186]
[edit] Resistance and abolition

Escaped slaves formed Maroon communities which played an important role in the histories of Brazil and other countries such as Suriname, Puerto Rico, Cuba, and Jamaica. In Brazil, the Maroon villages were called palenques or quilombos. Maroons survived by growing vegetables and hunting. They also raided plantations. At these attacks, the maroons would burn crops, steal livestock and tools, kill slavemasters, and invite other slaves to join their communities.

Jean-Baptiste Debret, a French painter who was active in Brazil in the first decades of the 19th Century, started out with painting portraits of members of the Brazilian Imperial family, but soon became concerned with the slavery of both blacks and indigenous inhabitants. His paintings on the subject (two appear on this page) helped bring attention to the subject in both Europe and Brazil itself.

The Clapham Sect, a group of evangelical reformers, campaigned during much of the 19th century for the United Kingdom to use its influence and power to stop the traffic of slaves to Brazil. Besides moral qualms, the low cost of slave-produced Brazilian sugar meant that British colonies in the West Indies were unable to match the market prices of Brazilian sugar, and each Briton was consuming 16 pounds (7 kg) of sugar a year by the 19th century. This combination led to intensive pressure from the British government for Brazil to end this practice, which it did by steps over several decades.

First, foreign slave trade was banned in 1850. Then, in 1871, the sons of the slaves were freed. In 1885, slaves aged over 60 years were freed. The Paraguayan War contributed to ending slavery, since many slaves enlisted in exchange for freedom. In Colonial Brazil, slavery was more a social than a racial condition. In fact, some of the greatest figures of the time, like the writer Machado de Assis and the engineer André Rebouças had black ancestry.

Brazil's 1877-78 Grande Seca (Great Drought) in the cotton-growing northeast led to major turmoil, starvation, poverty and internal migration. As wealthy plantation holders rushed to sell their slaves south, popular resistance and resentment grew, inspiring numerous emancipation societies. They succeeded in banning slavery altogether in the province of Ceará by 1884.[187] Slavery was legally ended nationwide on 13 May by the Lei Aurea ("Golden Law") of 1888. In fact, it was an institution in decadence at these times, as since the 1880s the country had begun to use European immigrant labor instead. Brazil was the last nation in the Western Hemisphere to abolish slavery.
[edit] Modern times

However, in 2004, the government acknowledged to the United Nations that at least 25,000 Brazilians work under conditions "analogous to slavery." The top anti-slavery official puts the number of modern slaves at 50,000.[188] More than 1,000 slave laborers were freed from a sugar cane plantation in 2007 by the Brazilian government, making it the largest anti-slavery raid in modern times in Brazil.[189]
[edit] Other South American countries
Funeral at slave plantation during Dutch colonial rule, Suriname. Colored lithograph printed circa 1840-1850, digitally restored.

During the period from late 19th and early 20th centuries, demand for the labor-intensive harvesting of rubber drove frontier expansion and slavery in Latin America and elsewhere. Indigenous people were enslaved as part of the rubber boom in Ecuador, Peru, Colombia, and Brazil.[190] In Central America, rubber tappers participated in the enslavement of the indigenous Guatuso-Maleku people for domestic service.[191]
[edit] British and French Caribbean
Main article: Slavery in the British and French Caribbean

Slavery was commonly used in the parts of the Caribbean controlled by France and the British Empire. The Lesser Antilles islands of Barbados, St. Kitts, Antigua, Martinique and Guadeloupe, which were the first important slave societies of the Caribbean, began the widespread use of African slaves by the end of the 17th century, as their economies converted from sugar production.[192] Among white Caribbeans there exists an underclass known as Redlegs; the descendants of English, Scottish and Irish indentured servants, and prisoners imported to the island.[193][194] The Calendar of State Papers, Colonial Series of 1701 records 25,000 slaves in Barbados, of which 21,700 were white.[195]

By the middle of the 18th century, British Jamaica and French Saint-Domingue had become the largest slave societies of the region, rivaling Brazil as a destination for enslaved Africans. Due to overwork and tropical diseases, the death rates for Caribbean slaves were greater than birth rates. The conditions led to increasing numbers of slave revolts, escaped slaves forming Maroon communities and fighting guerrilla wars against the plantation owners. Campaigns against slavery began during the period of the Enlightenment and grew to large proportions in Europe and United States during the 19th century (see Abolitionism).

To regularise slavery, in 1685 Louis XIV had enacted the code noir, which accorded certain human rights to slaves and responsibilities to the master, who was obliged to feed, clothe and provide for the general well-being of his slaves. Free blacks owned one-third of the plantation property and one-quarter of the slaves in Saint Domingue (later Haiti).[196] Slavery in the French Republic was abolished on 4 February 1794. When it became clear that Napoleon intended to re-establish slavery, Dessalines and Pétion switched sides, in October 1802. On 1 January 1804, Jean-Jacques Dessalines, the new leader under the dictatorial 1801 constitution, declared Haiti a free republic.[67] Thus Haiti became the second independent nation in the Western Hemisphere, after the United States, and the only successful slave rebellion in world history.[68]

Whitehall in England announced in 1833 that slaves in its territories would be totally freed by 1840. In the meantime, the government told slaves they had to remain on their plantations and would have the status of "apprentices" for the next six years. On 1 August 1834, an unarmed group of mainly elderly Negroes being addressed by the Governor at Government House about the new laws, began chanting: "Pas de six ans. Point de six ans" ("Not six years. No six years"), drowning out the voice of the Governor. Peaceful protests continued until a resolution to abolish apprenticeship was passed and de facto freedom was achieved. Full emancipation for all was legally granted ahead of schedule on 1 August 1838, making Trinidad the first British colony with slaves to completely abolish slavery.[197]

After Great Britain abolished slavery, it began to pressure other nations to do the same. France, too, abolished slavery. By then Saint-Domingue had already won its independence and formed the independent Republic of Haiti. French-controlled islands were then limited to a few smaller islands in the Lesser Antilles.
[edit] North America

Main Articles: Slavery in Colonial America, Slavery in Canada, History of slavery in the United States, Atlantic slave trade, Indian slavery, Slavery among the Cherokee, History of slavery in Kentucky, History of slavery in Missouri, History of slavery in Pennsylvania
[edit] Early events

The first slaves used by Europeans in what later became United States territory were among Lucas Vásquez de Ayllón's colonization attempt of North Carolina in 1526. The attempt was a failure, lasting only one year; the slaves revolted and fled into the wilderness to live among the Cofitachiqui people.[2]

The first historically significant slave in what would become the United States was Estevanico, a Moroccan slave and member of the Narváez expedition in 1528 and acted as a guide on Fray Marcos de Niza's expedition to find the Seven Cities of Gold in 1539.

In 1619 twenty Africans were brought by a Dutch soldier and sold to the English colony of Jamestown, Virginia as indentured servants. It is possible that Africans were brought to Virginia prior to this, both because neither John Rolfe our source on the 1619 shipment nor any contemporary of his ever says that this was the first contingent of Africans to come to Virginia and because the 1625 Virginia census lists one black as coming on a ship that appears to only have landed people in Virginia prior to 1619.[198] The transformation from indentured servitude to racial slavery happened gradually. It was not until 1661 that a reference to slavery entered into Virginia law, directed at Caucasian servants who ran away with a black servant. It was not until the Slave Codes of 1705 that the status of African Americans as slaves would be sealed. This status would last for another 160 years, until after the end of the American Civil War with the ratification of the 13th Amendment in December 1865.

Only a fraction of the enslaved Africans brought to the New World ended up in British North America-- perhaps 5%. The vast majority of slaves shipped across the Atlantic were sent to the Caribbean sugar colonies, Brazil, or Spanish America.

By the 1680s with the consolidation of England's Royal African Company, enslaved Africans were imported to English colonies in larger numbers, and the practice continued to be protected by the English Crown. Colonists began purchasing slaves in larger numbers.
[edit] Slavery in American colonial law

    * 1642: Massachusetts becomes the first colony to legalize slavery.
    * 1650: Connecticut legalizes slavery.
    * 1661: Virginia officially recognizes slavery by statute.
    * 1662: A Virginia statute declares that children born would have the same status as their mother.
    * 1663: Maryland legalizes slavery.
    * 1664: Slavery is legalized in New York and New Jersey.[199]

[edit] Development of slavery

The shift from indentured servants to African slaves was prompted by a dwindling class of former servants who had worked through the terms of their indentures and thus became competitors to their former masters. These newly freed servants were rarely able to support themselves comfortably, and the tobacco industry was increasingly dominated by large planters. This caused domestic unrest culminating in Bacon's Rebellion. Eventually, chattel slavery became the norm in regions dominated by plantations.

Many slaves in British North America were owned by plantation owners who lived in Britain. The British courts had made a series of contradictory rulings on the legality of slavery[200] which encouraged several thousand slaves to flee the newly-independent United States as refugees along with the retreating British in 1783. The British courts having ruled in 1772 that such slaves could not be forcibly returned to North America (see James Somersett and Somersett's Case for a review of the Somerset Decision), the British government resettled them as free men in Sierra Leone. See Black Loyalists.

Several slave rebellions took place during the 17th and 18th centuries.
[edit] Early United States law

Through the Northwest Ordinance of 1787 under the Congress of the Confederation, slavery was prohibited in the territories north west of the Ohio River. By 1804, abolitionists succeeded in passing legislation that would eventually (in conjunction with the 13th amendment) emancipate the slaves in every state north of the Ohio River and the Mason-Dixon Line. However, emancipation in the free states was so gradual that both New York and Pennsylvania listed slaves in their 1840 census returns, and a small number of black slaves were held in New Jersey in 1860.[201] The importation or export of slaves was banned on 1 January 1808;[202] but not the internal slave trade.

Despite the actions of abolitionists, free blacks were subject to racial segregation in the Northern states.[203] Slavey was legal in most of Canada until 1833, but after that it offered a haven for hundreds of runaway slaves. Refugees from slavery fled the South across the Ohio River to the North via the Underground Railroad. Midwestern state governments asserted States Rights arguments to refuse federal jurisdiction over fugitives. Some juries exercised their right of jury nullification and refused to convict those indicted under the Fugitive Slave Act of 1850.

The Dred Scott decision of 1857 asserted that one could take one's property anywhere, even if one's property was chattel and one crossed into a free territory. It also asserted that African Americans could not be citizens, as many Northern states granted blacks citizenship, who (in some states) could even vote. This was an example of Slave Power, the plantation aristocracy's attempt to control the North. While traditionally, this has been viewed as turning Northern public opinion against the South, it should be noted that pro-slavery forces made gains in the 1858 elections and it was the anti-slavery Republicans who were on the defensive on the issue. After the passage of the Kansas-Nebraska Act, armed conflict broke out in Kansas Territory, where the question of whether it would be admitted to the Union as a slave state or a free state had been left to the inhabitants. The radical abolitionist John Brown was active in the mayhem and killing in "Bleeding Kansas." The true turning point in public opinion is better fixed at the Lecompton Constitution fraud. Pro-slavery elements in Kansas had arrived first from Missouri and quickly organized a territorial government that excluded abolitionists. Through the machinery of the territory and violence, the pro-slavery faction attempted to force an unpopular pro-slavery constitution through the state. This infuriated Northern Democrats, who supported popular sovereignty, and was exacerbated by the Buchanan administration reneging on a promise to submit the constitution to a referendum - which it would surely fail. Anti-slavery legislators took office under the banner of the Republican Party.
[edit] Civil War
Peter, a slave from Baton Rouge, Louisiana, 1863. The scars are a result of a whipping by his overseer, who was subsequently discharged. It took two months to recover from the beating.

Approximately one Southern family in four held slaves prior to war. According to the 1860 United States Census, about 385,000 individuals[204] (i.e. 1.4% of White Americans in the country, or 4.8% of southern whites) owned one or more slaves.[205][206] and the slave population in the United States stood at four million.[207] 95% of blacks lived in the South, comprising one third of the population there as opposed to 1% of the population of the North. Consequently, fears of eventual emancipation were much greater in the South than in the North.[208]

In the election of 1860, the Republicans swept Abraham Lincoln into the Presidency (with only 39.8% of the popular vote) and legislators into Congress. Lincoln however, did not appear on the ballots in most southern states and his election split the nation along sectional lines. After decades of controlling the Federal Government, several of the southern states declared they had seceded from the U.S. (the Union) in an attempt to form the Confederate States of America.

Northern leaders like Lincoln viewed the prospect of a new Southern nation, with control over the Mississippi River and the West, as unacceptable. This led to the outbreak of the Civil War, which spelled the end for chattel slavery in America. However, in August 1862, Lincoln wrote to editor Horace Greeley that despite his own moral objection to slavery, the objective of the war was to save the Union and not either to save or to destroy slavery. He went on to say that if he could save the Union without freeing a single slave, or by freeing all the slaves, or by freeing only some of the slaves, he would do it. Lincoln's Emancipation Proclamation of 1863 was a powerful move that proclaimed freedom for slaves within the Confederacy as soon as the Union Army arrived; Lincoln had no power to free slaves in the border states or the rest of the Union, so he promoted the Thirteenth Amendment, which freed all the remaining slaves in December 1865. The proclamation made the abolition of slavery an official war goal and it was implemented as the Union captured territory from the Confederacy. Slaves in many parts of the south were freed by Union armies or when they simply left their former owners. Over 150,000 joined the Union Army and Navy as soldiers and sailors.

The remaining slaves within the United States remained enslaved until the final ratification of the Thirteenth Amendment to the Constitution on 6 December 1865 (with final recognition of the amendment on 18 December), eight months after the cessation of hostilities. Only in Kentucky did a significant slave population remain by that time, although there were some in West Virginia and Delaware.

After the failure of Reconstruction, freed slaves in the United States were treated as second class citizens. For decades after their emancipation, many former slaves living in the South sharecropped and had a low standard of living. In some states, it was only after the civil rights movement of the 1950s and 60s that blacks obtained legal protection from racial discrimination (see segregation).
[edit] Modern times

Although the thirteenth amendment is often understood as having made slavery illegal, Section I of the amendment actually states: “Neither slavery nor involuntary servitude, except as a punishment for crime whereof the party shall have been duly convicted, shall exist within the United States, or any place subject to their jurisdiction.” Thus slavery can exist constitutionally within the U.S., as punishment for a crime after the party has been convicted.[citation needed]

The United States Department of Labor occasionally prosecutes cases against people for false imprisonment and involuntary servitude. These cases often involve illegal immigrants who are forced to work as slaves in factories to pay off a debt claimed by the people who transported them into the United States. Other cases have involved domestic workers.

Long Islander Mahender Sabhnani, 52, an international perfume maker originally from India, was convicted by US Federal District Court Judge Arthur Spatt (in Central Islip N.Y.) of slavery of 2 Indonesian housekeepers in his $ 2 million Muttontown home, and sentenced on 27 June 2008 to 3 years and 4 months in prison with fine of $ 12,500. His wife, Varsha, was sentenced to 11 years in prison. A 12-count federal indictment included charges of forced labor, conspiracy, involuntary servitude and harboring aliens, specifically "beating slaves with brooms and umbrellas, slashed with knives, and forced to climb stairs and to take freezing showers for misdeeds that included sleeping late or stealing food from the trash because they were poorly fed."[209][210]
[edit] Asia
[edit] Indian subcontinent
Main article: Slavery in India

The Greek historian Arrian writes in his book Indica:

    "This also is remarkable in India, that all Indians are free, and no Indian at all is a slave. In this the Indians agree with the Lacedaemonians. Yet the Lacedaemonians have Helots for slaves, who perform the duties of slaves; but the Indians have no slaves at all, much less is any Indian a slave."

Though any formalised slave trade has not existed in South Asia, unfree labor has existed for centuries in the Medieval ages, in different forms. The most common forms have been kinds of bonded labor. During the epoch of the Mughals, debt bondage reached its peak, and it was common for money lenders to make slaves of peasants and others who failed to repay debts. Under these practices, more than one generation could be forced into unfree labor; for example, a son could be sold into bonded labor for life to pay off the debt, along with interest.

The early Arab invaders of Sind in the 700's, the armies of the Umayyad commander Muhammad bin Qasim, are reported to have enslaved tens of thousands of Indian prisoners, including both soldiers and civilians.[211][212] In the early 11th century Tarikh al-Yamini, the Arab historian Al-Utbi recorded that in 1001 the armies of Mahmud of Ghazna conquered Peshawar and Waihand, "in the midst of the land of Hindustan", and captured some 100,000 youths.[213][214] Later, following his twelfth expedition into India in 1018-19, Mahmud is reported to have returned to with such a large number of slaves that their value was reduced to only two to ten dirhams each. This unusually low price made, according to Al-Utbi, "merchants [come] from distant cities to purchase them, so that the countries of Central Asia, Iraq and Khurasan were swelled with them, and the fair and the dark, the rich and the poor, mingled in one common slavery". Elliot and Dowson refers to "five hundred thousand slaves, beautiful men and women.".[215][216][217] Later, during the Delhi Sultanate period (1206-1555), references to the abundant availability of low-priced Indian slaves abound. Levi attributes this primarily to the vast human resources of India, compared to its neighbours to the north and west (Mughal Indian population being approximately 12 to 20 times that of Turan and Iran at the end of 16th century) ..[218]

Arab slave traders also brought slaves as early as the first century AD from Africa. Most of the African slaves were brought however in the 17th century and were taken into Western India. The Siddi people are of mainly East African descent.

Much of the northern and central parts of the subcontinent was ruled by the so-called Slave Dynasty of Turkic origin from 1206-1290: Qutb-ud-din Aybak, a slave of Muhammad Ghori rose to power following his master's death. For almost a century, his descendants ruled presiding over the introduction of Tankas and building of Qutub Minar.

According to Sir Henry Bartle Frere (who sat on the Viceroy's Council), there were an estimated 8,000,000 or 9,000,000 slaves in India in 1841. In Malabar, about 15% of the population were slaves. Slavery was abolished in both Hindu and Muslim India by the Indian Slavery Act V. of 1843. Provisions of the Indian Penal Code of 1861 effectively abolished slavery in India by making the enslavement of human beings a criminal offense.[219][220][221][222]
[edit] Modern times

According to Human Rights Watch, there are currently more than 4100 bonded laborers in India,[223] who work as slaves to pay off debts; a majority of them are Dalits.[224] There are also an estimated 5 million bonded workers in Pakistan, even though the government has passed laws and set up funds to eradicate the practice and rehabilitate the labourers.[225] As many as 200,000 Nepali girls, many under 14, have been sold into sex slavery in India. Nepalese women and girls, especially virgins, are favored in India because of their fair skin and young looks.[226][227]
[edit] Nepal

Slavery was abolished in Nepal in 1924.[228] In 1997, a human rights agency reported that 40,000 Nepalese workers are subject to slavery and 200,000 kept in bonded labour.[229] Nepal's Maoist-led government has abolished the slavery-like Haliya system in 2008.[230]

As many as 200,000 Nepali girls, many under the age of 14, have been sold into sex slavery in India.[226] Nepalese women and girls, especially virgins, are favoured in India because of their fair skin and young looks.[227]
[edit] China

Slavery throughout pre-modern Chinese history has repeatedly come in and out of favor. Due to the enormous population and relatively high development of the region throughout most of its history, China has always had a large workforce.

Historically, Chinese families customarily had an average of four children or more. This custom was well suited to the agrarian societies of the period. In times of hardship such as widespread famine or severe financial difficulty, parents of poor families sold some of their children to wealthy homes, to be treated as future brides, servants or slaves. This depended on the the compassion and good grace of the master. However, more often it was teenagers or young adults who turned themselves in to become servants. They were not technically slaves since they received periodic payments, which they usually sent home to their families.

This passage gives some examples of slavery in the 19th century:

    "In the houses of wealthy citizens, it is not unusual to find twenty to thirty slaves attending upon a family. Even citizens in the humbler walks of life deem it necessary to have each a slave or two. The price of a slave varies, of course, according to age, health, strength, and general appearance. The average price is from fifty to one hundred dollars, but in time of war, or revolution, poor parents, on the verge of starvation, offer their sons and daughters for sale at remarkably low prices. I remember instances of parents, rendered destitute by the marauding bands who invested the two southern Kwangs in 1854-55, offering to sell their daughters in Canton for five dollars apiece. . . . The slavery to which these unfortunate persons are subject, is perpetual and hereditary, and they have no parental authority over their offspring. The great-grandsons of slaves, however, can, if they have sufficient means, purchase their freedom. . . . Masters seem to have the same uncontrolled power over their slaves that parents have over their children. Thus a master is not called to account for the death of a slave, although it is the result of punishment inflicted by him."[231]

    "In former times slaves were slain and offered in sacrifice to the spirit of the owner when dead, or by him to his ancestors: sometimes given as a substitute to suffer the death penalty incurred by his owner or in fulfillment of a vow. It used to be customary in Kuei-chou (and Szü-chuan too, I believe) to inter living slaves with their dead owners; the slaves were to keep a lamp burning in the tomb.... Slavery exists in China, especially in Canton and Peking.... It is a common thing for well-to-do people to present a couple of slave girls to a daughter as part of her marriage dowery [sic]. Nearly all prostitutes are slaves. It is, however, customary with respectable people to release their slave girls when marriageable. Some people sell their slaves girls to men wanting a wife for themselves or for a son of theirs. I have bought three different girls: two in Szü-chuan for a few taels each, less than fifteen dollars. One I released in Tientsin, another died in Hongkong; the other I gave in marriage to a faithful servant of mine. Some are worth much money at Shanghai."[232]

[edit] Slavery under Manchurian rule in the 17th century
Main article: Slavery in seventeenth-century China


Boo-i Aha (Manchu:booi niyalma) (Chinese translation:包衣阿哈) is a Manchu word literally translated as "household person" and sometimes rendered as "slaves". In his book China Marches West, Peter C. Perdue stated:"In 1624(After Nurhachi's invasion of Liaodong) "Chinese households....while those with less were made into slaves." The Manchu was establishing close personal and paternalist relationship between masters and their slaves, as Nurhachi said:" The Master should love the slaves and eat the same food as him". However, many Manchu slave masters treated their slaves very harshly, arranged numerous corvees (Chinese:徭役, 强迫的劳役), and sold and bought their slaves as if they were animals.[233]

Perdue further pointed out that boo-i aha "did not correspond exactly to the Chinese category of "bondservant-slave" (Chinese:奴僕), even though many western scholars would directly translate "boo-i" as "bondservant".[234]

Various classes of Booi

   1. booi niru a Manchu word (Chinese:包衣佐領), meaning Neiwufu Upper Three Banner's platoon leader of about 300 men .
   2. Booi guanlin a Manchu word (Chinese:包衣管領), meaning the manager of booi doing all the domestic duties of Neiwufu.
   3. Booi amban is also a Manchu word, meaning high official, (Chinese:包衣大臣).
   4. Estate bannerman (Chinese:庄头旗人) are those renegade Chinese who joined the Jurchen, or original civilians-soldiers working in the fields. These people were all turned into booi aha, or field slaves.

[edit] Modern times

All forms of slavery have been illegal in China since 1910,[235] although the practice still exists through illegal trafficking in some areas.[236]
[edit] Japan
Main article: Slavery in Japan

Slavery in Japan was, for most of its history, indigenous, since the export and import of slaves was restricted by Japan being a group of islands. However, Koreans were shipped to Japan as slaves during the Japanese invasions of Korea in the 16th century.[237][238] The export of a slave from Japan is recorded in 3rd century Chinese document, although the system involved is unclear. These slaves were called seiko (生口?), lit. "living mouth".

In the 8th century, a slave was called nuhi (奴婢?) and series of laws on slavery was issued. In an area of present-day Ibaraki Prefecture, out of a population of 190,000, around 2,000 were slaves; the proportion is believed to have been even higher in western Japan.

Slavery persisted into the Sengoku period (1467–1615), but the attitude that slavery was anachronistic had become widespread.[239] Oda Nobunaga is said to have had an African slave or former-slave in his retinue.[240][dubious – discuss]

In late 16th century Japan, slavery was officially banned; but forms of contract and indentured labor persisted alongside the period penal codes' forced labor. Somewhat later, the Edo period penal laws prescribed "non-free labor" for the immediate family of executed criminals in Article 17 of the Gotōke reijō (Tokugawa House Laws), but the practice never became common. The 1711 Gotōke reijō was compiled from over 600 statutes promulgated between 1597 and 1696.[241]
[edit] World War II

As the Empire of Japan annexed Asian countries, from the late 19th century onwards, archaic institutions including slavery were abolished in those countries. However, during the Second Sino-Japanese War and the Pacific War, the Japanese military used millions of civilians and prisoners of war as forced labor, on projects such as the Burma Railway.

According to a joint study by historians including Zhifen Ju, Mitsuyoshi Himeta, Toru Kubo and Mark Peattie, more than 10 million Chinese civilians were mobilized by the Kōa-in (Japanese Asia Development Board) for forced labour.[242] According to the Japanese military's own record, nearly 25% of 140,000 Allied POWs died while interned in Japanese prison camps where they were forced to work (U.S. POWs died at a rate of 37%).[243][244] More than 100,000 civilians and POWs died in the construction of the Burma-Siam Railway.[245] The U.S. Library of Congress estimates that in Java, between 4 and 10 million romusha (Japanese: "manual laborer"), were forced to work by the Japanese military.[246] About 270,000 of these Javanese laborers were sent to other Japanese-held areas in South East Asia. Only 52,000 were repatriated to Java, meaning that there was a death rate of 80%. (For further details, see Japanese war crimes.)[247]

Approximately 5,400,000 Koreans were conscripted into forced labor from 1939 to 1945. About 670,000 of them were taken to Japan, where about 60,000 died between 1939 and 1945 due mostly to exhaustion or poor working conditions. Many of those taken to Karafuto Prefecture (modern-day Sakhalin) were trapped there at the end of the war, stripped of their nationality and denied repatriation by Japan; they became known as the Sakhalin Koreans.[248] The total deaths of Korean forced laborers in Korea and Manchuria for those years is estimated to be between 270,000 and 810,000.[249]

As many as 200,000 women,[250] mostly from Korea and China, and some other countries such as the Philippines, Taiwan, Burma, the Dutch East Indies, Netherlands,[251] and Australia[252] were forced into sexual slavery during the World War II. (See Comfort women)
[edit] Korea

Indigenous slaves existed in Korea. Slavery was officially abolished with the Gabo Reform of 1894 but remained extant in reality until 1930. During the Joseon Dynasty (1392–1910), Korean was a hierarchical society. Some low class citizen called as Cheonmin. Low status was hereditary, as well as a form of legal punishment.[253] During poor harvests and famine, many peasants would voluntarily become low class in order to survive.[254][255][256][257]
[edit] Southeast Asia

There was a large slave class in Khmer Empire who built the enduring monuments in Angkor Wat and did most of the heavy work.[258] Slaves had been taken captive from the mountain tribes.[259] People unable to pay back a debt to the upper ruling class could be sentenced to work as a slave too.[260] Between the 17th and the early 20th centuries one-quarter to one-third of the population of some areas of Thailand and Burma were slaves.[261]

In Siam (Thailand), the war captives became the property of the king. During the reign of Rama III (1824–1851), there were an estimated 46,000 war slaves. Slaves from independent hill populations were "hunted incessantly and carried off as slaves by the Siamese, the Anamites, and the Cambodians" (Colquhoun 1885:53).[262] Slavery was not abolished in Siam until 1905.[263]

Yi people in Yunnan practiced a complicated form of slavery. People were split into the Black Yi (nobles, 7% of the population), White Yi (commoners), Ajia (33% of the Yi population) and the Xiaxi (10%). Ajia and Xiaxi were slave castes. The White Yi were not slaves but had no freedom of movement. The Black Yi were famous for their slave-raids on Han Chinese communities. After the 1959 some 700,000 slaves were freed.[264][265][266]

Slaves in Toraja society in Indonesia were family property. Sometimes Torajans decided to become slaves when they incurred a debt, pledging to work as payment. Slaves could be taken during wars, and slave trading was common. Torajan slaves were sold and shipped out to Java and Siam. Slaves could buy their freedom, but their children still inherited slave status. Slaves were prohibited from wearing bronze or gold, carving their houses, eating from the same dishes as their owners, or having sex with free women—a crime punishable by death. Slavery was abolished in 1909 by the Dutch East Indies government.[267][268]
[edit] Modern times

There are currently an estimated 300,000 women and children involved in the sex trade throughout Southeast Asia.[269] It is common that Thai women are lured to Japan and sold to Yakuza-controlled brothels where they are forced to work off their price.[270][271]

According to the International Labor Organization (ILO), an estimated 800,000 people are subject to forced labor in Myanmar.[272] In November 2006, the International Labor Organization announced it will be seeking "to prosecute members of the ruling Myanmar junta for crimes against humanity" over the continuous forced labor of its citizens by the military at the International Court of Justice.[273]
[edit] Central Asia and the Caucasus

Russian conquest of the Caucasus led to the abolition of slavery by the 1860s[274][275] and the conquest of the Central Asian Islamic khanates of Bukhara, Samarkand, and Khiva by the 1870s.[276] The Russian administration liberated the slaves of the Kazakhs in 1859.[277] A notorious slave market for captured Russian and Persian slaves was centred in the Khanate of Khiva from the 17th to the 19th century.[278] During the first half of the 19th century alone, some one million Persians, as well as an unknown number of Russians, were enslaved and transported to Central Asian khanates.[279][280] When the Russian troops took Khiva in 1898 there were 29,300 Persian slaves, captured by Turkoman raiders. According of Josef Wolff (Report of 1843-1845) the population of the Khanate of Bukhara was one million two hundred thousand, of whom 200,000 were Persian slaves.[281] At the beginning of the 21st century Chechens and Ingush kept Russian captives as slaves or in slave-like conditions in the mountains of the northern Caucasus.[282]
[edit] Oceania

In the first half of the nineteenth century, small-scale slave raids took place across Polynesia to supply labor and sex workers for the whaling and sealing trades, with examples from both the westerly and easterly extremes of the Polynesian triangle. By the 1860s this had grown to a larger scale operation with Peruvian slave raids in the South Sea Islands to collect labor for the guano industry.
[edit] Hawaii

Ancient Hawaii was a caste society. People were born into specific social classes. Kauwa were the outcast or slave class. They are believed to have been war captives, or the descendents of war captives. Marriage between higher castes and the kauwa was strictly forbidden. The kauwa worked for the chiefs and were often used as human sacrifices at the luakini heiau. (They were not the only sacrifices; law-breakers of all castes or defeated political opponents were also acceptable as victims.)[283]
[edit] New Zealand

In traditional Māori society of Aotearoa, prisoners of war became taurekareka, slaves, unless released, ransomed or tortured.[284] With some exceptions, the child of a slave remained a slave. As far as it is possible to tell, slavery seems to have increased in the early nineteenth century, as a result of increased numbers of prisoners being taken by Māori military leaders such as Hongi Hika and Te Rauparaha in the Musket Wars, the need for labor to supply whalers and traders with food, flax and timber in return for western goods, and the missionary condemnation of cannibalism. Slavery was outlawed when the British annexed New Zealand in 1840, immediately prior to the signing of the Treaty of Waitangi, although it did not end completely until government was effectively extended over the whole of the country with the defeat of the Kingi movement in the Wars of the mid 1860s.
[edit] Chatham Islands

One group of Polynesians who migrated to the Chatham Islands became the Moriori who developed a largely pacifist culture. It was originally speculated that they settled the Chathams direct from Polynesia, but it is now widely believed they were disaffected Māori who emigrated from the South Island of New Zealand.[285][286][287][288] Their pacifism left the Moriori unable to defend themselves when the islands were invaded by mainland Māori in the 1830s. Some 300 Moriori men, women and children were massacred and the remaining 1,200 to 1,300 survivors were enslaved.[289][290]
[edit] Rapa Nui / Easter Island

The isolated island of Rapa Nui/Easter Island was inhabited by the Rapanui, who suffered a series of slave raids from 1805 or earlier, culminating in a near genocidal experience in the 1860s. The 1805 raid was by American sealers and was one of a series that changed the attitude of the islanders to outside visitors, with reports in the 1820s and 1830s that all visitors received a hostile reception. In December 1862, Peruvian slave raiders took between 1,400 and 2,000 islanders back to Peru to work in the guano industry; this was about a third of the island's population and included much of the island's leadership, the last ariki-mau and possibly the last who could read Rongorongo. After intervention by the French ambassador in Lima, the last 15 survivors were returned to the island, but brought with them smallpox, which further devastated the island.
[edit] Abolitionist movements
Main article: Abolitionism
Proclamation of the abolition of slavery by Victor Hughes in the Guadeloupe, 1 November 1794

Slavery has existed, in one form or another, throughout the whole of human history. So, too, have movements to free large or distinct groups of slaves. Moses led Israelite slaves from ancient Egypt according to the Biblical Book of Exodus - possibly the first account of a movement to free slaves. {Exodus 5-20} However, abolitionism should be distinguished from efforts to help a particular group of slaves, or to restrict one practice, such as the slave trade.
[edit] Britain

In 1772, the Somersett Case (R. v. Knowles, ex parte Somersett)[291] of the English Court of King's Bench ruled that slavery was unlawful in England (although not elsewhere in the British Empire). A similar case, that of Joseph Knight, took place in Scotland five years later and ruled slavery to be contrary to the law of Scotland.

Following the work of campaigners in the United Kingdom, such as William Wilberforce and Thomas Clarkson, the Act for the Abolition of the Slave Trade was passed by Parliament on 25 March 1807, coming into effect the following year. The act imposed a fine of £100 for every slave found aboard a British ship. The intention was to outlaw entirely the Atlantic slave trade within the whole British Empire.

The significance of the abolition of the British slave trade lay in the number of people hitherto sold and carried by British slave vessels. Britain shipped 2,532,300 Africans across the Atlantic, equalling 41% of the total transport of 6,132,900 individuals. This made the British empire the biggest slave-trade contributor in the world due to the magnitude of the empire. A fact that made the abolition act all the more damaging to the global trade of slaves.[292]

The Slavery Abolition Act, passed on 23 August 1833, outlawed slavery itself in the British colonies. On 1 August 1834 all slaves in the British West Indies, were emancipated, but still indentured to their former owners in an apprenticeship system. The intention of, was to educate former slaves to a trade but instead allowed slave owners to maintain ownership illegally. The act was finally repealed in 1838.[293]

Britain abolished slavery in both Hindu and Muslim India by the Indian Slavery Act V. of 1843.[294]

Domestic slavery practised by the educated African coastal elites (as well as interior traditional rulers) in Sierra Leone was abolished in 1928. A study found practices of domestic slavery still widespread in rural areas in the 1970s.[295][296]
[edit] France

There were slaves in mainland France (especially in trade ports such as Nantes or Bordeaux).[297], but the institution was never officially authorized there. The legal case of Jean Boucaux in 1739 clarified the unclear legal position of possible slaves in France, and was followed by laws that established registers for slaves in mainland France, who were limited to a three-year stay, for visits or learning a trade. Unregistered "slaves" were regarded as free. However, slavery was of vital importance in France's Caribbean possessions, especially Saint-Domingue. In 1793, influenced by the French Declaration of the Rights of Man of August 1789 and alarmed as the massive slave revolt of August 1791 that had become the Haitian Revolution threatened to ally itself with the British, the French Revolutionary commissioners Sonthonax and Polverel declared general emancipation to reconcile them with France. In Paris, on 4 February 1794, Abbé Grégoire and the Convention ratified this action by officially abolishing slavery in all French territories outside mainland France, freeing all the slaves both for moral and security reasons.

Napoleon sent troops to the Caribbean in 1802 to try to re-establish slavery due to the economic stress France was suffering while fighting all over Europe. They succeeded in Guadeloupe, but the ex-slaves of Saint-Domingue defeated the French corps that was sent and declared independence. This colony became Haiti, the first black republic, on 1 January 1804, with at its head the leader of the revolt, Toussaint Louverture.[67] Slavery in the French colonies was finally abolished only in 1849.
[edit] United States

In 1688, four German Quakers in Germantown, a small village outside Philadelphia, wrote and presented a protest against the institution of slavery to their local Quaker Meeting. The Meeting did not know what to do and passed the petition up the chain of authority, where it continued to be ignored and was archived and forgotten for 150 years. In 1844 it was rediscovered and became a focus of the abolitionist movement. The 1688 Petition was the first American public document of its kind to protest slavery, and in addition was one of the first public documents to define universal human rights.

Slaves in the United States who escaped ownership would often make their way to Canada via the "Underground Railroad". The more famous of the African American abolitionists include former slaves Harriet Tubman, Sojourner Truth and Frederick Douglass. Many more people who opposed slavery and worked for abolition were northern whites, such as William Lloyd Garrison and John Brown. Slavery was legally abolished in 1865 by the Thirteenth Amendment to the United States Constitution.

The American Colonization Society, the primary vehicle for returning black Americans to greater freedom in Africa, established the colony of Liberia in 1821-22, on the premise former American slaves would have greater freedom and equality there.[298] The ACS assisted in the movement of thousands of African Americans to Liberia, with its founder Henry Clay stating; "unconquerable prejudice resulting from their color, they never could amalgamate with the free whites of this country. It was desirable, therefore, as it respected them, and the residue of the population of the country, to drain them off".[299]

While abolitionists agreed on the evils of slavery, there were differing opinions on what should happen after African Americans were freed. Some abolitionists, worried about the difficulties of integrating numerous uneducated people into a hostile environment, hoped to send freed people to Africa. By the time of Emancipation, most African-Americans were now native to the United States and did not want to leave. They believed that their labor had made the land theirs as well as that of the whites; trade unions feared competition in supplying an affordable labor force against former slaves. Most freed people stayed in the United States by choice.[citation needed]
[edit] Congress of Vienna

The Declaration of the Powers, on the Abolition of the Slave Trade, of 8 February 1815 (Which also formed ACT, No. XV. of the Final Act of the Congress of Vienna of the same year) included in its first sentence the concept of the "principles of humanity and universal morality" as justification for ending a trade that was "odious in its continuance".[300]
[edit] Twentieth century worldwide

The 1926 Slavery Convention, an initiative of the League of Nations, was a turning point in banning global slavery. Article 4 of the Universal Declaration of Human Rights, adopted in 1948 by the UN General Assembly, explicitly banned slavery. The United Nations 1956 Supplementary Convention on the Abolition of Slavery was convened to outlaw and ban slavery worldwide, including child slavery. In December 1966, the UN General Assembly adopted the International Covenant on Civil and Political Rights, which was developed from the Universal Declaration of Human Rights. Article 8 of this international treaty bans slavery. The treaty came into force in March 1976 after it had been ratified by 35 nations. As of November 2003, 104 nations had ratified the treaty. According to the British Anti-Slavery Society, "Although there is no longer any state which recognizes any claim by a person to a right of property over another, there are an estimated 27 million people throughout the world, mainly children, in conditions of slavery